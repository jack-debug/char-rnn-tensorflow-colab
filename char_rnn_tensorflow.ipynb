{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char-rnn-tensorflow",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOtmPXBkSFFTXOhI9TCfG/n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jack-debug/char-rnn-tensorflow-colab/blob/main/char_rnn_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kYjiqi0DbcK"
      },
      "source": [
        "# **char-rnn-tensorflow**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n4RggAJDVfm"
      },
      "source": [
        "First, lets install our required tensorflow version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0g2aoklDAk4",
        "outputId": "e5a65a66-3829-413c-d824-d845101ae27c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.15"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/b1/9c0d6640eab34fae38f4dae6b312894f8bc1025b0876b3eae1fe11745a7b/tensorflow_gpu-1.15.4-cp36-cp36m-manylinux2010_x86_64.whl (411.0MB)\n",
            "\u001b[K     |████████████████████████████████| 411.0MB 39kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.4) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.4) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.4) (1.1.2)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.4) (1.18.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.4) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.4) (3.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.4) (1.15.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.4) (1.33.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.4) (0.10.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.4) (0.35.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.4) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.4) (3.12.4)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (50.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (3.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.4) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (3.3.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=59628bde5cba41a44dd2b5f67f755267dfe21256d3a987403de9f06ba5682267\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, gast, keras-applications, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88oovhHyCaIW"
      },
      "source": [
        "Next, lets clone the repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2h8E-ZmCMeA",
        "outputId": "e18c3914-00d8-4ab9-e808-7130460486de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/sherjilozair/char-rnn-tensorflow.git\n",
        "%cd char-rnn-tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'char-rnn-tensorflow'...\n",
            "remote: Enumerating objects: 404, done.\u001b[K\n",
            "remote: Total 404 (delta 0), reused 0 (delta 0), pack-reused 404\n",
            "Receiving objects: 100% (404/404), 508.45 KiB | 20.34 MiB/s, done.\n",
            "Resolving deltas: 100% (238/238), done.\n",
            "/content/char-rnn-tensorflow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBPNhCHSWrpk"
      },
      "source": [
        "We will train on the tinyshakespeare dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q66uBlilCz81",
        "outputId": "b3fe4b24-a298-4117-e4dd-294f64a68d5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python train.py --rnn_size 512 --num_layers 4"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "17307/22300 (epoch 38), train_loss = 0.839, time/batch = 0.092\n",
            "17308/22300 (epoch 38), train_loss = 0.825, time/batch = 0.093\n",
            "17309/22300 (epoch 38), train_loss = 0.814, time/batch = 0.104\n",
            "17310/22300 (epoch 38), train_loss = 0.818, time/batch = 0.094\n",
            "17311/22300 (epoch 38), train_loss = 0.810, time/batch = 0.095\n",
            "17312/22300 (epoch 38), train_loss = 0.842, time/batch = 0.096\n",
            "17313/22300 (epoch 38), train_loss = 0.807, time/batch = 0.097\n",
            "17314/22300 (epoch 38), train_loss = 0.797, time/batch = 0.093\n",
            "17315/22300 (epoch 38), train_loss = 0.778, time/batch = 0.093\n",
            "17316/22300 (epoch 38), train_loss = 0.785, time/batch = 0.102\n",
            "17317/22300 (epoch 38), train_loss = 0.802, time/batch = 0.093\n",
            "17318/22300 (epoch 38), train_loss = 0.844, time/batch = 0.093\n",
            "17319/22300 (epoch 38), train_loss = 0.778, time/batch = 0.092\n",
            "17320/22300 (epoch 38), train_loss = 0.841, time/batch = 0.093\n",
            "17321/22300 (epoch 38), train_loss = 0.803, time/batch = 0.092\n",
            "17322/22300 (epoch 38), train_loss = 0.826, time/batch = 0.093\n",
            "17323/22300 (epoch 38), train_loss = 0.792, time/batch = 0.090\n",
            "17324/22300 (epoch 38), train_loss = 0.806, time/batch = 0.092\n",
            "17325/22300 (epoch 38), train_loss = 0.818, time/batch = 0.092\n",
            "17326/22300 (epoch 38), train_loss = 0.781, time/batch = 0.093\n",
            "17327/22300 (epoch 38), train_loss = 0.775, time/batch = 0.093\n",
            "17328/22300 (epoch 38), train_loss = 0.809, time/batch = 0.091\n",
            "17329/22300 (epoch 38), train_loss = 0.809, time/batch = 0.092\n",
            "17330/22300 (epoch 38), train_loss = 0.789, time/batch = 0.092\n",
            "17331/22300 (epoch 38), train_loss = 0.775, time/batch = 0.091\n",
            "17332/22300 (epoch 38), train_loss = 0.804, time/batch = 0.093\n",
            "17333/22300 (epoch 38), train_loss = 0.811, time/batch = 0.092\n",
            "17334/22300 (epoch 38), train_loss = 0.790, time/batch = 0.092\n",
            "17335/22300 (epoch 38), train_loss = 0.833, time/batch = 0.092\n",
            "17336/22300 (epoch 38), train_loss = 0.802, time/batch = 0.094\n",
            "17337/22300 (epoch 38), train_loss = 0.790, time/batch = 0.093\n",
            "17338/22300 (epoch 38), train_loss = 0.803, time/batch = 0.094\n",
            "17339/22300 (epoch 38), train_loss = 0.785, time/batch = 0.098\n",
            "17340/22300 (epoch 38), train_loss = 0.827, time/batch = 0.091\n",
            "17341/22300 (epoch 38), train_loss = 0.847, time/batch = 0.093\n",
            "17342/22300 (epoch 38), train_loss = 0.809, time/batch = 0.093\n",
            "17343/22300 (epoch 38), train_loss = 0.835, time/batch = 0.091\n",
            "17344/22300 (epoch 38), train_loss = 0.826, time/batch = 0.093\n",
            "17345/22300 (epoch 38), train_loss = 0.819, time/batch = 0.091\n",
            "17346/22300 (epoch 38), train_loss = 0.831, time/batch = 0.098\n",
            "17347/22300 (epoch 38), train_loss = 0.794, time/batch = 0.093\n",
            "17348/22300 (epoch 38), train_loss = 0.836, time/batch = 0.092\n",
            "17349/22300 (epoch 38), train_loss = 0.872, time/batch = 0.093\n",
            "17350/22300 (epoch 38), train_loss = 0.862, time/batch = 0.092\n",
            "17351/22300 (epoch 38), train_loss = 0.853, time/batch = 0.092\n",
            "17352/22300 (epoch 38), train_loss = 0.851, time/batch = 0.093\n",
            "17353/22300 (epoch 38), train_loss = 0.855, time/batch = 0.091\n",
            "17354/22300 (epoch 38), train_loss = 0.856, time/batch = 0.092\n",
            "17355/22300 (epoch 38), train_loss = 0.829, time/batch = 0.092\n",
            "17356/22300 (epoch 38), train_loss = 0.827, time/batch = 0.092\n",
            "17357/22300 (epoch 38), train_loss = 0.829, time/batch = 0.092\n",
            "17358/22300 (epoch 38), train_loss = 0.826, time/batch = 0.093\n",
            "17359/22300 (epoch 38), train_loss = 0.802, time/batch = 0.093\n",
            "17360/22300 (epoch 38), train_loss = 0.839, time/batch = 0.102\n",
            "17361/22300 (epoch 38), train_loss = 0.806, time/batch = 0.094\n",
            "17362/22300 (epoch 38), train_loss = 0.857, time/batch = 0.099\n",
            "17363/22300 (epoch 38), train_loss = 0.877, time/batch = 0.092\n",
            "17364/22300 (epoch 38), train_loss = 0.857, time/batch = 0.092\n",
            "17365/22300 (epoch 38), train_loss = 0.841, time/batch = 0.092\n",
            "17366/22300 (epoch 38), train_loss = 0.865, time/batch = 0.091\n",
            "17367/22300 (epoch 38), train_loss = 0.858, time/batch = 0.092\n",
            "17368/22300 (epoch 38), train_loss = 0.844, time/batch = 0.092\n",
            "17369/22300 (epoch 38), train_loss = 0.834, time/batch = 0.093\n",
            "17370/22300 (epoch 38), train_loss = 0.851, time/batch = 0.093\n",
            "17371/22300 (epoch 38), train_loss = 0.826, time/batch = 0.092\n",
            "17372/22300 (epoch 38), train_loss = 0.845, time/batch = 0.092\n",
            "17373/22300 (epoch 38), train_loss = 0.838, time/batch = 0.091\n",
            "17374/22300 (epoch 38), train_loss = 0.861, time/batch = 0.092\n",
            "17375/22300 (epoch 38), train_loss = 0.805, time/batch = 0.096\n",
            "17376/22300 (epoch 38), train_loss = 0.837, time/batch = 0.091\n",
            "17377/22300 (epoch 38), train_loss = 0.829, time/batch = 0.093\n",
            "17378/22300 (epoch 38), train_loss = 0.815, time/batch = 0.091\n",
            "17379/22300 (epoch 38), train_loss = 0.880, time/batch = 0.093\n",
            "17380/22300 (epoch 38), train_loss = 0.848, time/batch = 0.093\n",
            "17381/22300 (epoch 38), train_loss = 0.859, time/batch = 0.093\n",
            "17382/22300 (epoch 38), train_loss = 0.839, time/batch = 0.092\n",
            "17383/22300 (epoch 38), train_loss = 0.825, time/batch = 0.092\n",
            "17384/22300 (epoch 38), train_loss = 0.803, time/batch = 0.093\n",
            "17385/22300 (epoch 38), train_loss = 0.815, time/batch = 0.091\n",
            "17386/22300 (epoch 38), train_loss = 0.916, time/batch = 0.093\n",
            "17387/22300 (epoch 38), train_loss = 0.852, time/batch = 0.091\n",
            "17388/22300 (epoch 38), train_loss = 0.869, time/batch = 0.094\n",
            "17389/22300 (epoch 38), train_loss = 0.835, time/batch = 0.099\n",
            "17390/22300 (epoch 38), train_loss = 0.869, time/batch = 0.092\n",
            "17391/22300 (epoch 38), train_loss = 0.866, time/batch = 0.092\n",
            "17392/22300 (epoch 38), train_loss = 0.852, time/batch = 0.092\n",
            "17393/22300 (epoch 38), train_loss = 0.835, time/batch = 0.092\n",
            "17394/22300 (epoch 39), train_loss = 0.571, time/batch = 0.085\n",
            "17395/22300 (epoch 39), train_loss = 0.861, time/batch = 0.093\n",
            "17396/22300 (epoch 39), train_loss = 0.867, time/batch = 0.091\n",
            "17397/22300 (epoch 39), train_loss = 0.858, time/batch = 0.091\n",
            "17398/22300 (epoch 39), train_loss = 0.893, time/batch = 0.096\n",
            "17399/22300 (epoch 39), train_loss = 0.814, time/batch = 0.092\n",
            "17400/22300 (epoch 39), train_loss = 0.848, time/batch = 0.094\n",
            "17401/22300 (epoch 39), train_loss = 0.873, time/batch = 0.100\n",
            "17402/22300 (epoch 39), train_loss = 0.835, time/batch = 0.091\n",
            "17403/22300 (epoch 39), train_loss = 0.841, time/batch = 0.100\n",
            "17404/22300 (epoch 39), train_loss = 0.907, time/batch = 0.093\n",
            "17405/22300 (epoch 39), train_loss = 0.848, time/batch = 0.092\n",
            "17406/22300 (epoch 39), train_loss = 0.879, time/batch = 0.093\n",
            "17407/22300 (epoch 39), train_loss = 0.873, time/batch = 0.091\n",
            "17408/22300 (epoch 39), train_loss = 0.852, time/batch = 0.092\n",
            "17409/22300 (epoch 39), train_loss = 0.880, time/batch = 0.092\n",
            "17410/22300 (epoch 39), train_loss = 0.879, time/batch = 0.092\n",
            "17411/22300 (epoch 39), train_loss = 0.845, time/batch = 0.092\n",
            "17412/22300 (epoch 39), train_loss = 0.835, time/batch = 0.092\n",
            "17413/22300 (epoch 39), train_loss = 0.865, time/batch = 0.094\n",
            "17414/22300 (epoch 39), train_loss = 0.829, time/batch = 0.094\n",
            "17415/22300 (epoch 39), train_loss = 0.836, time/batch = 0.093\n",
            "17416/22300 (epoch 39), train_loss = 0.817, time/batch = 0.093\n",
            "17417/22300 (epoch 39), train_loss = 0.812, time/batch = 0.092\n",
            "17418/22300 (epoch 39), train_loss = 0.783, time/batch = 0.093\n",
            "17419/22300 (epoch 39), train_loss = 0.812, time/batch = 0.092\n",
            "17420/22300 (epoch 39), train_loss = 0.841, time/batch = 0.092\n",
            "17421/22300 (epoch 39), train_loss = 0.817, time/batch = 0.095\n",
            "17422/22300 (epoch 39), train_loss = 0.838, time/batch = 0.092\n",
            "17423/22300 (epoch 39), train_loss = 0.827, time/batch = 0.093\n",
            "17424/22300 (epoch 39), train_loss = 0.808, time/batch = 0.093\n",
            "17425/22300 (epoch 39), train_loss = 0.831, time/batch = 0.095\n",
            "17426/22300 (epoch 39), train_loss = 0.821, time/batch = 0.093\n",
            "17427/22300 (epoch 39), train_loss = 0.858, time/batch = 0.093\n",
            "17428/22300 (epoch 39), train_loss = 0.855, time/batch = 0.094\n",
            "17429/22300 (epoch 39), train_loss = 0.804, time/batch = 0.094\n",
            "17430/22300 (epoch 39), train_loss = 0.860, time/batch = 0.093\n",
            "17431/22300 (epoch 39), train_loss = 0.821, time/batch = 0.092\n",
            "17432/22300 (epoch 39), train_loss = 0.839, time/batch = 0.091\n",
            "17433/22300 (epoch 39), train_loss = 0.817, time/batch = 0.092\n",
            "17434/22300 (epoch 39), train_loss = 0.828, time/batch = 0.092\n",
            "17435/22300 (epoch 39), train_loss = 0.856, time/batch = 0.093\n",
            "17436/22300 (epoch 39), train_loss = 0.846, time/batch = 0.091\n",
            "17437/22300 (epoch 39), train_loss = 0.841, time/batch = 0.097\n",
            "17438/22300 (epoch 39), train_loss = 0.843, time/batch = 0.094\n",
            "17439/22300 (epoch 39), train_loss = 0.870, time/batch = 0.092\n",
            "17440/22300 (epoch 39), train_loss = 0.821, time/batch = 0.092\n",
            "17441/22300 (epoch 39), train_loss = 0.803, time/batch = 0.092\n",
            "17442/22300 (epoch 39), train_loss = 0.819, time/batch = 0.092\n",
            "17443/22300 (epoch 39), train_loss = 0.816, time/batch = 0.095\n",
            "17444/22300 (epoch 39), train_loss = 0.856, time/batch = 0.093\n",
            "17445/22300 (epoch 39), train_loss = 0.816, time/batch = 0.095\n",
            "17446/22300 (epoch 39), train_loss = 0.819, time/batch = 0.094\n",
            "17447/22300 (epoch 39), train_loss = 0.819, time/batch = 0.097\n",
            "17448/22300 (epoch 39), train_loss = 0.807, time/batch = 0.095\n",
            "17449/22300 (epoch 39), train_loss = 0.825, time/batch = 0.094\n",
            "17450/22300 (epoch 39), train_loss = 0.820, time/batch = 0.093\n",
            "17451/22300 (epoch 39), train_loss = 0.880, time/batch = 0.092\n",
            "17452/22300 (epoch 39), train_loss = 0.823, time/batch = 0.098\n",
            "17453/22300 (epoch 39), train_loss = 0.813, time/batch = 0.092\n",
            "17454/22300 (epoch 39), train_loss = 0.851, time/batch = 0.091\n",
            "17455/22300 (epoch 39), train_loss = 0.810, time/batch = 0.092\n",
            "17456/22300 (epoch 39), train_loss = 0.788, time/batch = 0.094\n",
            "17457/22300 (epoch 39), train_loss = 0.819, time/batch = 0.093\n",
            "17458/22300 (epoch 39), train_loss = 0.801, time/batch = 0.092\n",
            "17459/22300 (epoch 39), train_loss = 0.811, time/batch = 0.093\n",
            "17460/22300 (epoch 39), train_loss = 0.827, time/batch = 0.092\n",
            "17461/22300 (epoch 39), train_loss = 0.856, time/batch = 0.094\n",
            "17462/22300 (epoch 39), train_loss = 0.832, time/batch = 0.091\n",
            "17463/22300 (epoch 39), train_loss = 0.843, time/batch = 0.092\n",
            "17464/22300 (epoch 39), train_loss = 0.839, time/batch = 0.091\n",
            "17465/22300 (epoch 39), train_loss = 0.788, time/batch = 0.092\n",
            "17466/22300 (epoch 39), train_loss = 0.828, time/batch = 0.096\n",
            "17467/22300 (epoch 39), train_loss = 0.804, time/batch = 0.092\n",
            "17468/22300 (epoch 39), train_loss = 0.785, time/batch = 0.092\n",
            "17469/22300 (epoch 39), train_loss = 0.833, time/batch = 0.092\n",
            "17470/22300 (epoch 39), train_loss = 0.826, time/batch = 0.092\n",
            "17471/22300 (epoch 39), train_loss = 0.807, time/batch = 0.093\n",
            "17472/22300 (epoch 39), train_loss = 0.801, time/batch = 0.092\n",
            "17473/22300 (epoch 39), train_loss = 0.838, time/batch = 0.093\n",
            "17474/22300 (epoch 39), train_loss = 0.823, time/batch = 0.092\n",
            "17475/22300 (epoch 39), train_loss = 0.791, time/batch = 0.094\n",
            "17476/22300 (epoch 39), train_loss = 0.828, time/batch = 0.093\n",
            "17477/22300 (epoch 39), train_loss = 0.811, time/batch = 0.092\n",
            "17478/22300 (epoch 39), train_loss = 0.835, time/batch = 0.095\n",
            "17479/22300 (epoch 39), train_loss = 0.839, time/batch = 0.091\n",
            "17480/22300 (epoch 39), train_loss = 0.805, time/batch = 0.093\n",
            "17481/22300 (epoch 39), train_loss = 0.836, time/batch = 0.092\n",
            "17482/22300 (epoch 39), train_loss = 0.810, time/batch = 0.092\n",
            "17483/22300 (epoch 39), train_loss = 0.847, time/batch = 0.096\n",
            "17484/22300 (epoch 39), train_loss = 0.800, time/batch = 0.091\n",
            "17485/22300 (epoch 39), train_loss = 0.809, time/batch = 0.091\n",
            "17486/22300 (epoch 39), train_loss = 0.826, time/batch = 0.091\n",
            "17487/22300 (epoch 39), train_loss = 0.815, time/batch = 0.093\n",
            "17488/22300 (epoch 39), train_loss = 0.826, time/batch = 0.096\n",
            "17489/22300 (epoch 39), train_loss = 0.827, time/batch = 0.092\n",
            "17490/22300 (epoch 39), train_loss = 0.795, time/batch = 0.093\n",
            "17491/22300 (epoch 39), train_loss = 0.839, time/batch = 0.091\n",
            "17492/22300 (epoch 39), train_loss = 0.797, time/batch = 0.096\n",
            "17493/22300 (epoch 39), train_loss = 0.813, time/batch = 0.094\n",
            "17494/22300 (epoch 39), train_loss = 0.824, time/batch = 0.095\n",
            "17495/22300 (epoch 39), train_loss = 0.818, time/batch = 0.103\n",
            "17496/22300 (epoch 39), train_loss = 0.841, time/batch = 0.092\n",
            "17497/22300 (epoch 39), train_loss = 0.793, time/batch = 0.092\n",
            "17498/22300 (epoch 39), train_loss = 0.838, time/batch = 0.091\n",
            "17499/22300 (epoch 39), train_loss = 0.811, time/batch = 0.092\n",
            "17500/22300 (epoch 39), train_loss = 0.784, time/batch = 0.091\n",
            "17501/22300 (epoch 39), train_loss = 0.795, time/batch = 0.092\n",
            "17502/22300 (epoch 39), train_loss = 0.809, time/batch = 0.092\n",
            "17503/22300 (epoch 39), train_loss = 0.816, time/batch = 0.092\n",
            "17504/22300 (epoch 39), train_loss = 0.779, time/batch = 0.094\n",
            "17505/22300 (epoch 39), train_loss = 0.781, time/batch = 0.095\n",
            "17506/22300 (epoch 39), train_loss = 0.836, time/batch = 0.095\n",
            "17507/22300 (epoch 39), train_loss = 0.814, time/batch = 0.093\n",
            "17508/22300 (epoch 39), train_loss = 0.837, time/batch = 0.091\n",
            "17509/22300 (epoch 39), train_loss = 0.824, time/batch = 0.094\n",
            "17510/22300 (epoch 39), train_loss = 0.811, time/batch = 0.091\n",
            "17511/22300 (epoch 39), train_loss = 0.841, time/batch = 0.091\n",
            "17512/22300 (epoch 39), train_loss = 0.843, time/batch = 0.091\n",
            "17513/22300 (epoch 39), train_loss = 0.821, time/batch = 0.092\n",
            "17514/22300 (epoch 39), train_loss = 0.853, time/batch = 0.092\n",
            "17515/22300 (epoch 39), train_loss = 0.834, time/batch = 0.094\n",
            "17516/22300 (epoch 39), train_loss = 0.814, time/batch = 0.093\n",
            "17517/22300 (epoch 39), train_loss = 0.807, time/batch = 0.095\n",
            "17518/22300 (epoch 39), train_loss = 0.857, time/batch = 0.092\n",
            "17519/22300 (epoch 39), train_loss = 0.855, time/batch = 0.091\n",
            "17520/22300 (epoch 39), train_loss = 0.838, time/batch = 0.091\n",
            "17521/22300 (epoch 39), train_loss = 0.856, time/batch = 0.092\n",
            "17522/22300 (epoch 39), train_loss = 0.855, time/batch = 0.092\n",
            "17523/22300 (epoch 39), train_loss = 0.849, time/batch = 0.092\n",
            "17524/22300 (epoch 39), train_loss = 0.812, time/batch = 0.092\n",
            "17525/22300 (epoch 39), train_loss = 0.850, time/batch = 0.093\n",
            "17526/22300 (epoch 39), train_loss = 0.857, time/batch = 0.093\n",
            "17527/22300 (epoch 39), train_loss = 0.836, time/batch = 0.093\n",
            "17528/22300 (epoch 39), train_loss = 0.826, time/batch = 0.092\n",
            "17529/22300 (epoch 39), train_loss = 0.865, time/batch = 0.092\n",
            "17530/22300 (epoch 39), train_loss = 0.873, time/batch = 0.093\n",
            "17531/22300 (epoch 39), train_loss = 0.864, time/batch = 0.090\n",
            "17532/22300 (epoch 39), train_loss = 0.869, time/batch = 0.092\n",
            "17533/22300 (epoch 39), train_loss = 0.866, time/batch = 0.094\n",
            "17534/22300 (epoch 39), train_loss = 0.831, time/batch = 0.092\n",
            "17535/22300 (epoch 39), train_loss = 0.832, time/batch = 0.092\n",
            "17536/22300 (epoch 39), train_loss = 0.828, time/batch = 0.092\n",
            "17537/22300 (epoch 39), train_loss = 0.833, time/batch = 0.094\n",
            "17538/22300 (epoch 39), train_loss = 0.847, time/batch = 0.092\n",
            "17539/22300 (epoch 39), train_loss = 0.905, time/batch = 0.092\n",
            "17540/22300 (epoch 39), train_loss = 0.824, time/batch = 0.093\n",
            "17541/22300 (epoch 39), train_loss = 0.820, time/batch = 0.092\n",
            "17542/22300 (epoch 39), train_loss = 0.818, time/batch = 0.092\n",
            "17543/22300 (epoch 39), train_loss = 0.826, time/batch = 0.095\n",
            "17544/22300 (epoch 39), train_loss = 0.843, time/batch = 0.092\n",
            "17545/22300 (epoch 39), train_loss = 0.846, time/batch = 0.093\n",
            "17546/22300 (epoch 39), train_loss = 0.837, time/batch = 0.093\n",
            "17547/22300 (epoch 39), train_loss = 0.820, time/batch = 0.100\n",
            "17548/22300 (epoch 39), train_loss = 0.848, time/batch = 0.093\n",
            "17549/22300 (epoch 39), train_loss = 0.841, time/batch = 0.094\n",
            "17550/22300 (epoch 39), train_loss = 0.838, time/batch = 0.096\n",
            "17551/22300 (epoch 39), train_loss = 0.836, time/batch = 0.093\n",
            "17552/22300 (epoch 39), train_loss = 0.867, time/batch = 0.092\n",
            "17553/22300 (epoch 39), train_loss = 0.835, time/batch = 0.092\n",
            "17554/22300 (epoch 39), train_loss = 0.827, time/batch = 0.092\n",
            "17555/22300 (epoch 39), train_loss = 0.846, time/batch = 0.092\n",
            "17556/22300 (epoch 39), train_loss = 0.830, time/batch = 0.095\n",
            "17557/22300 (epoch 39), train_loss = 0.821, time/batch = 0.091\n",
            "17558/22300 (epoch 39), train_loss = 0.836, time/batch = 0.093\n",
            "17559/22300 (epoch 39), train_loss = 0.837, time/batch = 0.091\n",
            "17560/22300 (epoch 39), train_loss = 0.818, time/batch = 0.095\n",
            "17561/22300 (epoch 39), train_loss = 0.797, time/batch = 0.092\n",
            "17562/22300 (epoch 39), train_loss = 0.807, time/batch = 0.092\n",
            "17563/22300 (epoch 39), train_loss = 0.841, time/batch = 0.093\n",
            "17564/22300 (epoch 39), train_loss = 0.822, time/batch = 0.102\n",
            "17565/22300 (epoch 39), train_loss = 0.821, time/batch = 0.092\n",
            "17566/22300 (epoch 39), train_loss = 0.817, time/batch = 0.094\n",
            "17567/22300 (epoch 39), train_loss = 0.800, time/batch = 0.092\n",
            "17568/22300 (epoch 39), train_loss = 0.803, time/batch = 0.093\n",
            "17569/22300 (epoch 39), train_loss = 0.803, time/batch = 0.092\n",
            "17570/22300 (epoch 39), train_loss = 0.769, time/batch = 0.093\n",
            "17571/22300 (epoch 39), train_loss = 0.790, time/batch = 0.093\n",
            "17572/22300 (epoch 39), train_loss = 0.744, time/batch = 0.092\n",
            "17573/22300 (epoch 39), train_loss = 0.824, time/batch = 0.095\n",
            "17574/22300 (epoch 39), train_loss = 0.812, time/batch = 0.094\n",
            "17575/22300 (epoch 39), train_loss = 0.794, time/batch = 0.091\n",
            "17576/22300 (epoch 39), train_loss = 0.826, time/batch = 0.093\n",
            "17577/22300 (epoch 39), train_loss = 0.810, time/batch = 0.092\n",
            "17578/22300 (epoch 39), train_loss = 0.790, time/batch = 0.091\n",
            "17579/22300 (epoch 39), train_loss = 0.798, time/batch = 0.092\n",
            "17580/22300 (epoch 39), train_loss = 0.850, time/batch = 0.092\n",
            "17581/22300 (epoch 39), train_loss = 0.811, time/batch = 0.093\n",
            "17582/22300 (epoch 39), train_loss = 0.843, time/batch = 0.097\n",
            "17583/22300 (epoch 39), train_loss = 0.808, time/batch = 0.093\n",
            "17584/22300 (epoch 39), train_loss = 0.831, time/batch = 0.093\n",
            "17585/22300 (epoch 39), train_loss = 0.824, time/batch = 0.092\n",
            "17586/22300 (epoch 39), train_loss = 0.817, time/batch = 0.095\n",
            "17587/22300 (epoch 39), train_loss = 0.844, time/batch = 0.091\n",
            "17588/22300 (epoch 39), train_loss = 0.845, time/batch = 0.092\n",
            "17589/22300 (epoch 39), train_loss = 0.814, time/batch = 0.093\n",
            "17590/22300 (epoch 39), train_loss = 0.818, time/batch = 0.093\n",
            "17591/22300 (epoch 39), train_loss = 0.827, time/batch = 0.092\n",
            "17592/22300 (epoch 39), train_loss = 0.843, time/batch = 0.093\n",
            "17593/22300 (epoch 39), train_loss = 0.831, time/batch = 0.092\n",
            "17594/22300 (epoch 39), train_loss = 0.857, time/batch = 0.092\n",
            "17595/22300 (epoch 39), train_loss = 0.816, time/batch = 0.093\n",
            "17596/22300 (epoch 39), train_loss = 0.791, time/batch = 0.093\n",
            "17597/22300 (epoch 39), train_loss = 0.788, time/batch = 0.093\n",
            "17598/22300 (epoch 39), train_loss = 0.802, time/batch = 0.093\n",
            "17599/22300 (epoch 39), train_loss = 0.801, time/batch = 0.091\n",
            "17600/22300 (epoch 39), train_loss = 0.860, time/batch = 0.092\n",
            "17601/22300 (epoch 39), train_loss = 0.823, time/batch = 0.091\n",
            "17602/22300 (epoch 39), train_loss = 0.868, time/batch = 0.092\n",
            "17603/22300 (epoch 39), train_loss = 0.819, time/batch = 0.096\n",
            "17604/22300 (epoch 39), train_loss = 0.810, time/batch = 0.092\n",
            "17605/22300 (epoch 39), train_loss = 0.771, time/batch = 0.093\n",
            "17606/22300 (epoch 39), train_loss = 0.800, time/batch = 0.091\n",
            "17607/22300 (epoch 39), train_loss = 0.795, time/batch = 0.092\n",
            "17608/22300 (epoch 39), train_loss = 0.781, time/batch = 0.093\n",
            "17609/22300 (epoch 39), train_loss = 0.810, time/batch = 0.091\n",
            "17610/22300 (epoch 39), train_loss = 0.796, time/batch = 0.092\n",
            "17611/22300 (epoch 39), train_loss = 0.828, time/batch = 0.092\n",
            "17612/22300 (epoch 39), train_loss = 0.826, time/batch = 0.092\n",
            "17613/22300 (epoch 39), train_loss = 0.803, time/batch = 0.091\n",
            "17614/22300 (epoch 39), train_loss = 0.837, time/batch = 0.093\n",
            "17615/22300 (epoch 39), train_loss = 0.815, time/batch = 0.091\n",
            "17616/22300 (epoch 39), train_loss = 0.813, time/batch = 0.093\n",
            "17617/22300 (epoch 39), train_loss = 0.833, time/batch = 0.092\n",
            "17618/22300 (epoch 39), train_loss = 0.781, time/batch = 0.092\n",
            "17619/22300 (epoch 39), train_loss = 0.808, time/batch = 0.092\n",
            "17620/22300 (epoch 39), train_loss = 0.853, time/batch = 0.092\n",
            "17621/22300 (epoch 39), train_loss = 0.837, time/batch = 0.093\n",
            "17622/22300 (epoch 39), train_loss = 0.833, time/batch = 0.092\n",
            "17623/22300 (epoch 39), train_loss = 0.826, time/batch = 0.092\n",
            "17624/22300 (epoch 39), train_loss = 0.833, time/batch = 0.096\n",
            "17625/22300 (epoch 39), train_loss = 0.838, time/batch = 0.093\n",
            "17626/22300 (epoch 39), train_loss = 0.825, time/batch = 0.093\n",
            "17627/22300 (epoch 39), train_loss = 0.827, time/batch = 0.092\n",
            "17628/22300 (epoch 39), train_loss = 0.835, time/batch = 0.093\n",
            "17629/22300 (epoch 39), train_loss = 0.798, time/batch = 0.092\n",
            "17630/22300 (epoch 39), train_loss = 0.819, time/batch = 0.092\n",
            "17631/22300 (epoch 39), train_loss = 0.778, time/batch = 0.092\n",
            "17632/22300 (epoch 39), train_loss = 0.819, time/batch = 0.092\n",
            "17633/22300 (epoch 39), train_loss = 0.808, time/batch = 0.093\n",
            "17634/22300 (epoch 39), train_loss = 0.856, time/batch = 0.094\n",
            "17635/22300 (epoch 39), train_loss = 0.816, time/batch = 0.092\n",
            "17636/22300 (epoch 39), train_loss = 0.841, time/batch = 0.091\n",
            "17637/22300 (epoch 39), train_loss = 0.848, time/batch = 0.092\n",
            "17638/22300 (epoch 39), train_loss = 0.810, time/batch = 0.092\n",
            "17639/22300 (epoch 39), train_loss = 0.830, time/batch = 0.092\n",
            "17640/22300 (epoch 39), train_loss = 0.836, time/batch = 0.092\n",
            "17641/22300 (epoch 39), train_loss = 0.755, time/batch = 0.092\n",
            "17642/22300 (epoch 39), train_loss = 0.824, time/batch = 0.093\n",
            "17643/22300 (epoch 39), train_loss = 0.807, time/batch = 0.092\n",
            "17644/22300 (epoch 39), train_loss = 0.799, time/batch = 0.093\n",
            "17645/22300 (epoch 39), train_loss = 0.806, time/batch = 0.092\n",
            "17646/22300 (epoch 39), train_loss = 0.833, time/batch = 0.093\n",
            "17647/22300 (epoch 39), train_loss = 0.816, time/batch = 0.099\n",
            "17648/22300 (epoch 39), train_loss = 0.808, time/batch = 0.093\n",
            "17649/22300 (epoch 39), train_loss = 0.840, time/batch = 0.091\n",
            "17650/22300 (epoch 39), train_loss = 0.814, time/batch = 0.093\n",
            "17651/22300 (epoch 39), train_loss = 0.853, time/batch = 0.091\n",
            "17652/22300 (epoch 39), train_loss = 0.828, time/batch = 0.092\n",
            "17653/22300 (epoch 39), train_loss = 0.848, time/batch = 0.091\n",
            "17654/22300 (epoch 39), train_loss = 0.803, time/batch = 0.092\n",
            "17655/22300 (epoch 39), train_loss = 0.848, time/batch = 0.092\n",
            "17656/22300 (epoch 39), train_loss = 0.794, time/batch = 0.092\n",
            "17657/22300 (epoch 39), train_loss = 0.840, time/batch = 0.093\n",
            "17658/22300 (epoch 39), train_loss = 0.797, time/batch = 0.092\n",
            "17659/22300 (epoch 39), train_loss = 0.842, time/batch = 0.092\n",
            "17660/22300 (epoch 39), train_loss = 0.810, time/batch = 0.091\n",
            "17661/22300 (epoch 39), train_loss = 0.819, time/batch = 0.094\n",
            "17662/22300 (epoch 39), train_loss = 0.808, time/batch = 0.091\n",
            "17663/22300 (epoch 39), train_loss = 0.798, time/batch = 0.092\n",
            "17664/22300 (epoch 39), train_loss = 0.826, time/batch = 0.092\n",
            "17665/22300 (epoch 39), train_loss = 0.829, time/batch = 0.092\n",
            "17666/22300 (epoch 39), train_loss = 0.842, time/batch = 0.093\n",
            "17667/22300 (epoch 39), train_loss = 0.852, time/batch = 0.092\n",
            "17668/22300 (epoch 39), train_loss = 0.845, time/batch = 0.093\n",
            "17669/22300 (epoch 39), train_loss = 0.846, time/batch = 0.092\n",
            "17670/22300 (epoch 39), train_loss = 0.864, time/batch = 0.092\n",
            "17671/22300 (epoch 39), train_loss = 0.829, time/batch = 0.091\n",
            "17672/22300 (epoch 39), train_loss = 0.815, time/batch = 0.096\n",
            "17673/22300 (epoch 39), train_loss = 0.821, time/batch = 0.093\n",
            "17674/22300 (epoch 39), train_loss = 0.810, time/batch = 0.091\n",
            "17675/22300 (epoch 39), train_loss = 0.856, time/batch = 0.093\n",
            "17676/22300 (epoch 39), train_loss = 0.815, time/batch = 0.092\n",
            "17677/22300 (epoch 39), train_loss = 0.809, time/batch = 0.092\n",
            "17678/22300 (epoch 39), train_loss = 0.798, time/batch = 0.092\n",
            "17679/22300 (epoch 39), train_loss = 0.801, time/batch = 0.092\n",
            "17680/22300 (epoch 39), train_loss = 0.826, time/batch = 0.093\n",
            "17681/22300 (epoch 39), train_loss = 0.819, time/batch = 0.091\n",
            "17682/22300 (epoch 39), train_loss = 0.803, time/batch = 0.093\n",
            "17683/22300 (epoch 39), train_loss = 0.835, time/batch = 0.092\n",
            "17684/22300 (epoch 39), train_loss = 0.806, time/batch = 0.092\n",
            "17685/22300 (epoch 39), train_loss = 0.793, time/batch = 0.096\n",
            "17686/22300 (epoch 39), train_loss = 0.842, time/batch = 0.092\n",
            "17687/22300 (epoch 39), train_loss = 0.828, time/batch = 0.093\n",
            "17688/22300 (epoch 39), train_loss = 0.823, time/batch = 0.093\n",
            "17689/22300 (epoch 39), train_loss = 0.815, time/batch = 0.092\n",
            "17690/22300 (epoch 39), train_loss = 0.819, time/batch = 0.093\n",
            "17691/22300 (epoch 39), train_loss = 0.819, time/batch = 0.095\n",
            "17692/22300 (epoch 39), train_loss = 0.806, time/batch = 0.093\n",
            "17693/22300 (epoch 39), train_loss = 0.841, time/batch = 0.098\n",
            "17694/22300 (epoch 39), train_loss = 0.801, time/batch = 0.093\n",
            "17695/22300 (epoch 39), train_loss = 0.802, time/batch = 0.097\n",
            "17696/22300 (epoch 39), train_loss = 0.793, time/batch = 0.092\n",
            "17697/22300 (epoch 39), train_loss = 0.791, time/batch = 0.091\n",
            "17698/22300 (epoch 39), train_loss = 0.801, time/batch = 0.092\n",
            "17699/22300 (epoch 39), train_loss = 0.800, time/batch = 0.092\n",
            "17700/22300 (epoch 39), train_loss = 0.776, time/batch = 0.092\n",
            "17701/22300 (epoch 39), train_loss = 0.827, time/batch = 0.093\n",
            "17702/22300 (epoch 39), train_loss = 0.823, time/batch = 0.093\n",
            "17703/22300 (epoch 39), train_loss = 0.808, time/batch = 0.104\n",
            "17704/22300 (epoch 39), train_loss = 0.790, time/batch = 0.093\n",
            "17705/22300 (epoch 39), train_loss = 0.801, time/batch = 0.095\n",
            "17706/22300 (epoch 39), train_loss = 0.811, time/batch = 0.098\n",
            "17707/22300 (epoch 39), train_loss = 0.811, time/batch = 0.092\n",
            "17708/22300 (epoch 39), train_loss = 0.821, time/batch = 0.092\n",
            "17709/22300 (epoch 39), train_loss = 0.818, time/batch = 0.091\n",
            "17710/22300 (epoch 39), train_loss = 0.796, time/batch = 0.093\n",
            "17711/22300 (epoch 39), train_loss = 0.829, time/batch = 0.092\n",
            "17712/22300 (epoch 39), train_loss = 0.798, time/batch = 0.093\n",
            "17713/22300 (epoch 39), train_loss = 0.761, time/batch = 0.093\n",
            "17714/22300 (epoch 39), train_loss = 0.788, time/batch = 0.093\n",
            "17715/22300 (epoch 39), train_loss = 0.819, time/batch = 0.093\n",
            "17716/22300 (epoch 39), train_loss = 0.805, time/batch = 0.097\n",
            "17717/22300 (epoch 39), train_loss = 0.771, time/batch = 0.094\n",
            "17718/22300 (epoch 39), train_loss = 0.780, time/batch = 0.090\n",
            "17719/22300 (epoch 39), train_loss = 0.817, time/batch = 0.092\n",
            "17720/22300 (epoch 39), train_loss = 0.826, time/batch = 0.092\n",
            "17721/22300 (epoch 39), train_loss = 0.818, time/batch = 0.092\n",
            "17722/22300 (epoch 39), train_loss = 0.854, time/batch = 0.093\n",
            "17723/22300 (epoch 39), train_loss = 0.818, time/batch = 0.092\n",
            "17724/22300 (epoch 39), train_loss = 0.777, time/batch = 0.098\n",
            "17725/22300 (epoch 39), train_loss = 0.805, time/batch = 0.093\n",
            "17726/22300 (epoch 39), train_loss = 0.787, time/batch = 0.092\n",
            "17727/22300 (epoch 39), train_loss = 0.751, time/batch = 0.093\n",
            "17728/22300 (epoch 39), train_loss = 0.793, time/batch = 0.091\n",
            "17729/22300 (epoch 39), train_loss = 0.822, time/batch = 0.092\n",
            "17730/22300 (epoch 39), train_loss = 0.830, time/batch = 0.091\n",
            "17731/22300 (epoch 39), train_loss = 0.796, time/batch = 0.092\n",
            "17732/22300 (epoch 39), train_loss = 0.816, time/batch = 0.092\n",
            "17733/22300 (epoch 39), train_loss = 0.822, time/batch = 0.093\n",
            "17734/22300 (epoch 39), train_loss = 0.848, time/batch = 0.093\n",
            "17735/22300 (epoch 39), train_loss = 0.791, time/batch = 0.091\n",
            "17736/22300 (epoch 39), train_loss = 0.786, time/batch = 0.093\n",
            "17737/22300 (epoch 39), train_loss = 0.811, time/batch = 0.092\n",
            "17738/22300 (epoch 39), train_loss = 0.802, time/batch = 0.092\n",
            "17739/22300 (epoch 39), train_loss = 0.818, time/batch = 0.092\n",
            "17740/22300 (epoch 39), train_loss = 0.782, time/batch = 0.091\n",
            "17741/22300 (epoch 39), train_loss = 0.823, time/batch = 0.093\n",
            "17742/22300 (epoch 39), train_loss = 0.777, time/batch = 0.091\n",
            "17743/22300 (epoch 39), train_loss = 0.815, time/batch = 0.092\n",
            "17744/22300 (epoch 39), train_loss = 0.827, time/batch = 0.091\n",
            "17745/22300 (epoch 39), train_loss = 0.813, time/batch = 0.092\n",
            "17746/22300 (epoch 39), train_loss = 0.796, time/batch = 0.092\n",
            "17747/22300 (epoch 39), train_loss = 0.818, time/batch = 0.093\n",
            "17748/22300 (epoch 39), train_loss = 0.836, time/batch = 0.092\n",
            "17749/22300 (epoch 39), train_loss = 0.810, time/batch = 0.092\n",
            "17750/22300 (epoch 39), train_loss = 0.845, time/batch = 0.093\n",
            "17751/22300 (epoch 39), train_loss = 0.820, time/batch = 0.091\n",
            "17752/22300 (epoch 39), train_loss = 0.806, time/batch = 0.091\n",
            "17753/22300 (epoch 39), train_loss = 0.828, time/batch = 0.096\n",
            "17754/22300 (epoch 39), train_loss = 0.814, time/batch = 0.092\n",
            "17755/22300 (epoch 39), train_loss = 0.805, time/batch = 0.093\n",
            "17756/22300 (epoch 39), train_loss = 0.800, time/batch = 0.092\n",
            "17757/22300 (epoch 39), train_loss = 0.799, time/batch = 0.094\n",
            "17758/22300 (epoch 39), train_loss = 0.814, time/batch = 0.095\n",
            "17759/22300 (epoch 39), train_loss = 0.790, time/batch = 0.091\n",
            "17760/22300 (epoch 39), train_loss = 0.784, time/batch = 0.097\n",
            "17761/22300 (epoch 39), train_loss = 0.758, time/batch = 0.094\n",
            "17762/22300 (epoch 39), train_loss = 0.773, time/batch = 0.093\n",
            "17763/22300 (epoch 39), train_loss = 0.789, time/batch = 0.095\n",
            "17764/22300 (epoch 39), train_loss = 0.820, time/batch = 0.092\n",
            "17765/22300 (epoch 39), train_loss = 0.757, time/batch = 0.093\n",
            "17766/22300 (epoch 39), train_loss = 0.832, time/batch = 0.094\n",
            "17767/22300 (epoch 39), train_loss = 0.787, time/batch = 0.092\n",
            "17768/22300 (epoch 39), train_loss = 0.818, time/batch = 0.092\n",
            "17769/22300 (epoch 39), train_loss = 0.796, time/batch = 0.092\n",
            "17770/22300 (epoch 39), train_loss = 0.804, time/batch = 0.093\n",
            "17771/22300 (epoch 39), train_loss = 0.812, time/batch = 0.092\n",
            "17772/22300 (epoch 39), train_loss = 0.771, time/batch = 0.092\n",
            "17773/22300 (epoch 39), train_loss = 0.768, time/batch = 0.093\n",
            "17774/22300 (epoch 39), train_loss = 0.783, time/batch = 0.091\n",
            "17775/22300 (epoch 39), train_loss = 0.795, time/batch = 0.093\n",
            "17776/22300 (epoch 39), train_loss = 0.781, time/batch = 0.093\n",
            "17777/22300 (epoch 39), train_loss = 0.768, time/batch = 0.092\n",
            "17778/22300 (epoch 39), train_loss = 0.791, time/batch = 0.093\n",
            "17779/22300 (epoch 39), train_loss = 0.794, time/batch = 0.093\n",
            "17780/22300 (epoch 39), train_loss = 0.767, time/batch = 0.093\n",
            "17781/22300 (epoch 39), train_loss = 0.807, time/batch = 0.091\n",
            "17782/22300 (epoch 39), train_loss = 0.786, time/batch = 0.092\n",
            "17783/22300 (epoch 39), train_loss = 0.776, time/batch = 0.092\n",
            "17784/22300 (epoch 39), train_loss = 0.786, time/batch = 0.092\n",
            "17785/22300 (epoch 39), train_loss = 0.781, time/batch = 0.092\n",
            "17786/22300 (epoch 39), train_loss = 0.815, time/batch = 0.092\n",
            "17787/22300 (epoch 39), train_loss = 0.833, time/batch = 0.092\n",
            "17788/22300 (epoch 39), train_loss = 0.791, time/batch = 0.093\n",
            "17789/22300 (epoch 39), train_loss = 0.822, time/batch = 0.093\n",
            "17790/22300 (epoch 39), train_loss = 0.813, time/batch = 0.098\n",
            "17791/22300 (epoch 39), train_loss = 0.808, time/batch = 0.093\n",
            "17792/22300 (epoch 39), train_loss = 0.818, time/batch = 0.092\n",
            "17793/22300 (epoch 39), train_loss = 0.790, time/batch = 0.093\n",
            "17794/22300 (epoch 39), train_loss = 0.815, time/batch = 0.101\n",
            "17795/22300 (epoch 39), train_loss = 0.842, time/batch = 0.093\n",
            "17796/22300 (epoch 39), train_loss = 0.828, time/batch = 0.092\n",
            "17797/22300 (epoch 39), train_loss = 0.829, time/batch = 0.094\n",
            "17798/22300 (epoch 39), train_loss = 0.835, time/batch = 0.093\n",
            "17799/22300 (epoch 39), train_loss = 0.855, time/batch = 0.092\n",
            "17800/22300 (epoch 39), train_loss = 0.852, time/batch = 0.093\n",
            "17801/22300 (epoch 39), train_loss = 0.849, time/batch = 0.092\n",
            "17802/22300 (epoch 39), train_loss = 0.837, time/batch = 0.092\n",
            "17803/22300 (epoch 39), train_loss = 0.823, time/batch = 0.092\n",
            "17804/22300 (epoch 39), train_loss = 0.818, time/batch = 0.092\n",
            "17805/22300 (epoch 39), train_loss = 0.778, time/batch = 0.092\n",
            "17806/22300 (epoch 39), train_loss = 0.803, time/batch = 0.091\n",
            "17807/22300 (epoch 39), train_loss = 0.784, time/batch = 0.094\n",
            "17808/22300 (epoch 39), train_loss = 0.831, time/batch = 0.092\n",
            "17809/22300 (epoch 39), train_loss = 0.862, time/batch = 0.094\n",
            "17810/22300 (epoch 39), train_loss = 0.838, time/batch = 0.096\n",
            "17811/22300 (epoch 39), train_loss = 0.834, time/batch = 0.092\n",
            "17812/22300 (epoch 39), train_loss = 0.856, time/batch = 0.092\n",
            "17813/22300 (epoch 39), train_loss = 0.843, time/batch = 0.092\n",
            "17814/22300 (epoch 39), train_loss = 0.846, time/batch = 0.092\n",
            "17815/22300 (epoch 39), train_loss = 0.842, time/batch = 0.092\n",
            "17816/22300 (epoch 39), train_loss = 0.852, time/batch = 0.092\n",
            "17817/22300 (epoch 39), train_loss = 0.825, time/batch = 0.095\n",
            "17818/22300 (epoch 39), train_loss = 0.843, time/batch = 0.092\n",
            "17819/22300 (epoch 39), train_loss = 0.814, time/batch = 0.092\n",
            "17820/22300 (epoch 39), train_loss = 0.846, time/batch = 0.094\n",
            "17821/22300 (epoch 39), train_loss = 0.793, time/batch = 0.092\n",
            "17822/22300 (epoch 39), train_loss = 0.817, time/batch = 0.093\n",
            "17823/22300 (epoch 39), train_loss = 0.805, time/batch = 0.092\n",
            "17824/22300 (epoch 39), train_loss = 0.803, time/batch = 0.093\n",
            "17825/22300 (epoch 39), train_loss = 0.854, time/batch = 0.092\n",
            "17826/22300 (epoch 39), train_loss = 0.825, time/batch = 0.092\n",
            "17827/22300 (epoch 39), train_loss = 0.839, time/batch = 0.092\n",
            "17828/22300 (epoch 39), train_loss = 0.818, time/batch = 0.092\n",
            "17829/22300 (epoch 39), train_loss = 0.819, time/batch = 0.093\n",
            "17830/22300 (epoch 39), train_loss = 0.787, time/batch = 0.102\n",
            "17831/22300 (epoch 39), train_loss = 0.807, time/batch = 0.090\n",
            "17832/22300 (epoch 39), train_loss = 0.898, time/batch = 0.092\n",
            "17833/22300 (epoch 39), train_loss = 0.828, time/batch = 0.092\n",
            "17834/22300 (epoch 39), train_loss = 0.838, time/batch = 0.092\n",
            "17835/22300 (epoch 39), train_loss = 0.815, time/batch = 0.091\n",
            "17836/22300 (epoch 39), train_loss = 0.852, time/batch = 0.092\n",
            "17837/22300 (epoch 39), train_loss = 0.846, time/batch = 0.091\n",
            "17838/22300 (epoch 39), train_loss = 0.829, time/batch = 0.096\n",
            "17839/22300 (epoch 39), train_loss = 0.834, time/batch = 0.092\n",
            "17840/22300 (epoch 40), train_loss = 0.548, time/batch = 0.091\n",
            "17841/22300 (epoch 40), train_loss = 0.823, time/batch = 0.093\n",
            "17842/22300 (epoch 40), train_loss = 0.851, time/batch = 0.094\n",
            "17843/22300 (epoch 40), train_loss = 0.851, time/batch = 0.106\n",
            "17844/22300 (epoch 40), train_loss = 0.894, time/batch = 0.092\n",
            "17845/22300 (epoch 40), train_loss = 0.815, time/batch = 0.106\n",
            "17846/22300 (epoch 40), train_loss = 0.846, time/batch = 0.098\n",
            "17847/22300 (epoch 40), train_loss = 0.852, time/batch = 0.092\n",
            "17848/22300 (epoch 40), train_loss = 0.814, time/batch = 0.092\n",
            "17849/22300 (epoch 40), train_loss = 0.815, time/batch = 0.095\n",
            "17850/22300 (epoch 40), train_loss = 0.884, time/batch = 0.091\n",
            "17851/22300 (epoch 40), train_loss = 0.830, time/batch = 0.090\n",
            "17852/22300 (epoch 40), train_loss = 0.868, time/batch = 0.093\n",
            "17853/22300 (epoch 40), train_loss = 0.870, time/batch = 0.092\n",
            "17854/22300 (epoch 40), train_loss = 0.863, time/batch = 0.091\n",
            "17855/22300 (epoch 40), train_loss = 0.875, time/batch = 0.092\n",
            "17856/22300 (epoch 40), train_loss = 0.869, time/batch = 0.091\n",
            "17857/22300 (epoch 40), train_loss = 0.828, time/batch = 0.092\n",
            "17858/22300 (epoch 40), train_loss = 0.820, time/batch = 0.092\n",
            "17859/22300 (epoch 40), train_loss = 0.852, time/batch = 0.093\n",
            "17860/22300 (epoch 40), train_loss = 0.815, time/batch = 0.093\n",
            "17861/22300 (epoch 40), train_loss = 0.816, time/batch = 0.091\n",
            "17862/22300 (epoch 40), train_loss = 0.802, time/batch = 0.092\n",
            "17863/22300 (epoch 40), train_loss = 0.791, time/batch = 0.092\n",
            "17864/22300 (epoch 40), train_loss = 0.765, time/batch = 0.093\n",
            "17865/22300 (epoch 40), train_loss = 0.798, time/batch = 0.091\n",
            "17866/22300 (epoch 40), train_loss = 0.820, time/batch = 0.093\n",
            "17867/22300 (epoch 40), train_loss = 0.805, time/batch = 0.093\n",
            "17868/22300 (epoch 40), train_loss = 0.825, time/batch = 0.092\n",
            "17869/22300 (epoch 40), train_loss = 0.811, time/batch = 0.092\n",
            "17870/22300 (epoch 40), train_loss = 0.799, time/batch = 0.099\n",
            "17871/22300 (epoch 40), train_loss = 0.819, time/batch = 0.092\n",
            "17872/22300 (epoch 40), train_loss = 0.809, time/batch = 0.092\n",
            "17873/22300 (epoch 40), train_loss = 0.840, time/batch = 0.092\n",
            "17874/22300 (epoch 40), train_loss = 0.837, time/batch = 0.093\n",
            "17875/22300 (epoch 40), train_loss = 0.804, time/batch = 0.091\n",
            "17876/22300 (epoch 40), train_loss = 0.853, time/batch = 0.093\n",
            "17877/22300 (epoch 40), train_loss = 0.810, time/batch = 0.092\n",
            "17878/22300 (epoch 40), train_loss = 0.824, time/batch = 0.093\n",
            "17879/22300 (epoch 40), train_loss = 0.798, time/batch = 0.092\n",
            "17880/22300 (epoch 40), train_loss = 0.805, time/batch = 0.092\n",
            "17881/22300 (epoch 40), train_loss = 0.831, time/batch = 0.094\n",
            "17882/22300 (epoch 40), train_loss = 0.829, time/batch = 0.091\n",
            "17883/22300 (epoch 40), train_loss = 0.830, time/batch = 0.091\n",
            "17884/22300 (epoch 40), train_loss = 0.821, time/batch = 0.092\n",
            "17885/22300 (epoch 40), train_loss = 0.843, time/batch = 0.092\n",
            "17886/22300 (epoch 40), train_loss = 0.802, time/batch = 0.092\n",
            "17887/22300 (epoch 40), train_loss = 0.788, time/batch = 0.093\n",
            "17888/22300 (epoch 40), train_loss = 0.810, time/batch = 0.092\n",
            "17889/22300 (epoch 40), train_loss = 0.817, time/batch = 0.091\n",
            "17890/22300 (epoch 40), train_loss = 0.846, time/batch = 0.093\n",
            "17891/22300 (epoch 40), train_loss = 0.812, time/batch = 0.092\n",
            "17892/22300 (epoch 40), train_loss = 0.803, time/batch = 0.092\n",
            "17893/22300 (epoch 40), train_loss = 0.801, time/batch = 0.094\n",
            "17894/22300 (epoch 40), train_loss = 0.786, time/batch = 0.092\n",
            "17895/22300 (epoch 40), train_loss = 0.796, time/batch = 0.097\n",
            "17896/22300 (epoch 40), train_loss = 0.792, time/batch = 0.092\n",
            "17897/22300 (epoch 40), train_loss = 0.868, time/batch = 0.092\n",
            "17898/22300 (epoch 40), train_loss = 0.805, time/batch = 0.093\n",
            "17899/22300 (epoch 40), train_loss = 0.800, time/batch = 0.091\n",
            "17900/22300 (epoch 40), train_loss = 0.838, time/batch = 0.092\n",
            "17901/22300 (epoch 40), train_loss = 0.805, time/batch = 0.091\n",
            "17902/22300 (epoch 40), train_loss = 0.775, time/batch = 0.094\n",
            "17903/22300 (epoch 40), train_loss = 0.809, time/batch = 0.097\n",
            "17904/22300 (epoch 40), train_loss = 0.788, time/batch = 0.091\n",
            "17905/22300 (epoch 40), train_loss = 0.798, time/batch = 0.093\n",
            "17906/22300 (epoch 40), train_loss = 0.805, time/batch = 0.091\n",
            "17907/22300 (epoch 40), train_loss = 0.846, time/batch = 0.092\n",
            "17908/22300 (epoch 40), train_loss = 0.813, time/batch = 0.092\n",
            "17909/22300 (epoch 40), train_loss = 0.826, time/batch = 0.092\n",
            "17910/22300 (epoch 40), train_loss = 0.827, time/batch = 0.093\n",
            "17911/22300 (epoch 40), train_loss = 0.775, time/batch = 0.092\n",
            "17912/22300 (epoch 40), train_loss = 0.816, time/batch = 0.093\n",
            "17913/22300 (epoch 40), train_loss = 0.786, time/batch = 0.091\n",
            "17914/22300 (epoch 40), train_loss = 0.778, time/batch = 0.094\n",
            "17915/22300 (epoch 40), train_loss = 0.823, time/batch = 0.093\n",
            "17916/22300 (epoch 40), train_loss = 0.813, time/batch = 0.092\n",
            "17917/22300 (epoch 40), train_loss = 0.803, time/batch = 0.092\n",
            "17918/22300 (epoch 40), train_loss = 0.795, time/batch = 0.092\n",
            "17919/22300 (epoch 40), train_loss = 0.827, time/batch = 0.093\n",
            "17920/22300 (epoch 40), train_loss = 0.808, time/batch = 0.091\n",
            "17921/22300 (epoch 40), train_loss = 0.780, time/batch = 0.092\n",
            "17922/22300 (epoch 40), train_loss = 0.806, time/batch = 0.092\n",
            "17923/22300 (epoch 40), train_loss = 0.786, time/batch = 0.092\n",
            "17924/22300 (epoch 40), train_loss = 0.816, time/batch = 0.093\n",
            "17925/22300 (epoch 40), train_loss = 0.819, time/batch = 0.091\n",
            "17926/22300 (epoch 40), train_loss = 0.788, time/batch = 0.095\n",
            "17927/22300 (epoch 40), train_loss = 0.827, time/batch = 0.090\n",
            "17928/22300 (epoch 40), train_loss = 0.791, time/batch = 0.091\n",
            "17929/22300 (epoch 40), train_loss = 0.830, time/batch = 0.091\n",
            "17930/22300 (epoch 40), train_loss = 0.794, time/batch = 0.092\n",
            "17931/22300 (epoch 40), train_loss = 0.801, time/batch = 0.091\n",
            "17932/22300 (epoch 40), train_loss = 0.819, time/batch = 0.092\n",
            "17933/22300 (epoch 40), train_loss = 0.809, time/batch = 0.092\n",
            "17934/22300 (epoch 40), train_loss = 0.813, time/batch = 0.092\n",
            "17935/22300 (epoch 40), train_loss = 0.806, time/batch = 0.093\n",
            "17936/22300 (epoch 40), train_loss = 0.774, time/batch = 0.093\n",
            "17937/22300 (epoch 40), train_loss = 0.818, time/batch = 0.091\n",
            "17938/22300 (epoch 40), train_loss = 0.777, time/batch = 0.092\n",
            "17939/22300 (epoch 40), train_loss = 0.783, time/batch = 0.092\n",
            "17940/22300 (epoch 40), train_loss = 0.796, time/batch = 0.092\n",
            "17941/22300 (epoch 40), train_loss = 0.797, time/batch = 0.092\n",
            "17942/22300 (epoch 40), train_loss = 0.823, time/batch = 0.092\n",
            "17943/22300 (epoch 40), train_loss = 0.776, time/batch = 0.092\n",
            "17944/22300 (epoch 40), train_loss = 0.818, time/batch = 0.092\n",
            "17945/22300 (epoch 40), train_loss = 0.785, time/batch = 0.092\n",
            "17946/22300 (epoch 40), train_loss = 0.768, time/batch = 0.092\n",
            "17947/22300 (epoch 40), train_loss = 0.776, time/batch = 0.092\n",
            "17948/22300 (epoch 40), train_loss = 0.789, time/batch = 0.092\n",
            "17949/22300 (epoch 40), train_loss = 0.787, time/batch = 0.092\n",
            "17950/22300 (epoch 40), train_loss = 0.749, time/batch = 0.092\n",
            "17951/22300 (epoch 40), train_loss = 0.751, time/batch = 0.092\n",
            "17952/22300 (epoch 40), train_loss = 0.803, time/batch = 0.092\n",
            "17953/22300 (epoch 40), train_loss = 0.780, time/batch = 0.092\n",
            "17954/22300 (epoch 40), train_loss = 0.804, time/batch = 0.092\n",
            "17955/22300 (epoch 40), train_loss = 0.814, time/batch = 0.093\n",
            "17956/22300 (epoch 40), train_loss = 0.801, time/batch = 0.092\n",
            "17957/22300 (epoch 40), train_loss = 0.830, time/batch = 0.092\n",
            "17958/22300 (epoch 40), train_loss = 0.828, time/batch = 0.093\n",
            "17959/22300 (epoch 40), train_loss = 0.805, time/batch = 0.091\n",
            "17960/22300 (epoch 40), train_loss = 0.843, time/batch = 0.092\n",
            "17961/22300 (epoch 40), train_loss = 0.814, time/batch = 0.092\n",
            "17962/22300 (epoch 40), train_loss = 0.799, time/batch = 0.094\n",
            "17963/22300 (epoch 40), train_loss = 0.791, time/batch = 0.091\n",
            "17964/22300 (epoch 40), train_loss = 0.836, time/batch = 0.093\n",
            "17965/22300 (epoch 40), train_loss = 0.823, time/batch = 0.092\n",
            "17966/22300 (epoch 40), train_loss = 0.804, time/batch = 0.092\n",
            "17967/22300 (epoch 40), train_loss = 0.819, time/batch = 0.093\n",
            "17968/22300 (epoch 40), train_loss = 0.825, time/batch = 0.092\n",
            "17969/22300 (epoch 40), train_loss = 0.817, time/batch = 0.092\n",
            "17970/22300 (epoch 40), train_loss = 0.797, time/batch = 0.092\n",
            "17971/22300 (epoch 40), train_loss = 0.837, time/batch = 0.092\n",
            "17972/22300 (epoch 40), train_loss = 0.840, time/batch = 0.091\n",
            "17973/22300 (epoch 40), train_loss = 0.840, time/batch = 0.092\n",
            "17974/22300 (epoch 40), train_loss = 0.812, time/batch = 0.092\n",
            "17975/22300 (epoch 40), train_loss = 0.855, time/batch = 0.092\n",
            "17976/22300 (epoch 40), train_loss = 0.858, time/batch = 0.092\n",
            "17977/22300 (epoch 40), train_loss = 0.848, time/batch = 0.092\n",
            "17978/22300 (epoch 40), train_loss = 0.851, time/batch = 0.093\n",
            "17979/22300 (epoch 40), train_loss = 0.859, time/batch = 0.092\n",
            "17980/22300 (epoch 40), train_loss = 0.817, time/batch = 0.092\n",
            "17981/22300 (epoch 40), train_loss = 0.802, time/batch = 0.092\n",
            "17982/22300 (epoch 40), train_loss = 0.803, time/batch = 0.092\n",
            "17983/22300 (epoch 40), train_loss = 0.818, time/batch = 0.093\n",
            "17984/22300 (epoch 40), train_loss = 0.825, time/batch = 0.092\n",
            "17985/22300 (epoch 40), train_loss = 0.896, time/batch = 0.092\n",
            "17986/22300 (epoch 40), train_loss = 0.816, time/batch = 0.091\n",
            "17987/22300 (epoch 40), train_loss = 0.819, time/batch = 0.096\n",
            "17988/22300 (epoch 40), train_loss = 0.810, time/batch = 0.092\n",
            "17989/22300 (epoch 40), train_loss = 0.811, time/batch = 0.092\n",
            "17990/22300 (epoch 40), train_loss = 0.833, time/batch = 0.092\n",
            "17991/22300 (epoch 40), train_loss = 0.832, time/batch = 0.095\n",
            "17992/22300 (epoch 40), train_loss = 0.816, time/batch = 0.091\n",
            "17993/22300 (epoch 40), train_loss = 0.804, time/batch = 0.092\n",
            "17994/22300 (epoch 40), train_loss = 0.822, time/batch = 0.092\n",
            "17995/22300 (epoch 40), train_loss = 0.817, time/batch = 0.093\n",
            "17996/22300 (epoch 40), train_loss = 0.820, time/batch = 0.092\n",
            "17997/22300 (epoch 40), train_loss = 0.814, time/batch = 0.092\n",
            "17998/22300 (epoch 40), train_loss = 0.852, time/batch = 0.092\n",
            "17999/22300 (epoch 40), train_loss = 0.832, time/batch = 0.092\n",
            "18000/22300 (epoch 40), train_loss = 0.816, time/batch = 0.092\n",
            "model saved to save/model.ckpt\n",
            "18001/22300 (epoch 40), train_loss = 0.832, time/batch = 0.085\n",
            "18002/22300 (epoch 40), train_loss = 0.821, time/batch = 0.092\n",
            "18003/22300 (epoch 40), train_loss = 0.806, time/batch = 0.092\n",
            "18004/22300 (epoch 40), train_loss = 0.817, time/batch = 0.091\n",
            "18005/22300 (epoch 40), train_loss = 0.816, time/batch = 0.092\n",
            "18006/22300 (epoch 40), train_loss = 0.790, time/batch = 0.091\n",
            "18007/22300 (epoch 40), train_loss = 0.782, time/batch = 0.091\n",
            "18008/22300 (epoch 40), train_loss = 0.786, time/batch = 0.091\n",
            "18009/22300 (epoch 40), train_loss = 0.823, time/batch = 0.092\n",
            "18010/22300 (epoch 40), train_loss = 0.805, time/batch = 0.097\n",
            "18011/22300 (epoch 40), train_loss = 0.802, time/batch = 0.093\n",
            "18012/22300 (epoch 40), train_loss = 0.792, time/batch = 0.091\n",
            "18013/22300 (epoch 40), train_loss = 0.781, time/batch = 0.091\n",
            "18014/22300 (epoch 40), train_loss = 0.786, time/batch = 0.091\n",
            "18015/22300 (epoch 40), train_loss = 0.782, time/batch = 0.097\n",
            "18016/22300 (epoch 40), train_loss = 0.753, time/batch = 0.091\n",
            "18017/22300 (epoch 40), train_loss = 0.785, time/batch = 0.092\n",
            "18018/22300 (epoch 40), train_loss = 0.729, time/batch = 0.091\n",
            "18019/22300 (epoch 40), train_loss = 0.810, time/batch = 0.091\n",
            "18020/22300 (epoch 40), train_loss = 0.784, time/batch = 0.092\n",
            "18021/22300 (epoch 40), train_loss = 0.776, time/batch = 0.094\n",
            "18022/22300 (epoch 40), train_loss = 0.806, time/batch = 0.092\n",
            "18023/22300 (epoch 40), train_loss = 0.788, time/batch = 0.093\n",
            "18024/22300 (epoch 40), train_loss = 0.770, time/batch = 0.095\n",
            "18025/22300 (epoch 40), train_loss = 0.766, time/batch = 0.093\n",
            "18026/22300 (epoch 40), train_loss = 0.834, time/batch = 0.094\n",
            "18027/22300 (epoch 40), train_loss = 0.797, time/batch = 0.092\n",
            "18028/22300 (epoch 40), train_loss = 0.832, time/batch = 0.091\n",
            "18029/22300 (epoch 40), train_loss = 0.809, time/batch = 0.091\n",
            "18030/22300 (epoch 40), train_loss = 0.815, time/batch = 0.093\n",
            "18031/22300 (epoch 40), train_loss = 0.806, time/batch = 0.093\n",
            "18032/22300 (epoch 40), train_loss = 0.801, time/batch = 0.092\n",
            "18033/22300 (epoch 40), train_loss = 0.824, time/batch = 0.094\n",
            "18034/22300 (epoch 40), train_loss = 0.826, time/batch = 0.092\n",
            "18035/22300 (epoch 40), train_loss = 0.791, time/batch = 0.092\n",
            "18036/22300 (epoch 40), train_loss = 0.796, time/batch = 0.094\n",
            "18037/22300 (epoch 40), train_loss = 0.793, time/batch = 0.094\n",
            "18038/22300 (epoch 40), train_loss = 0.829, time/batch = 0.092\n",
            "18039/22300 (epoch 40), train_loss = 0.800, time/batch = 0.091\n",
            "18040/22300 (epoch 40), train_loss = 0.833, time/batch = 0.091\n",
            "18041/22300 (epoch 40), train_loss = 0.798, time/batch = 0.092\n",
            "18042/22300 (epoch 40), train_loss = 0.784, time/batch = 0.092\n",
            "18043/22300 (epoch 40), train_loss = 0.776, time/batch = 0.093\n",
            "18044/22300 (epoch 40), train_loss = 0.787, time/batch = 0.092\n",
            "18045/22300 (epoch 40), train_loss = 0.787, time/batch = 0.093\n",
            "18046/22300 (epoch 40), train_loss = 0.837, time/batch = 0.091\n",
            "18047/22300 (epoch 40), train_loss = 0.791, time/batch = 0.092\n",
            "18048/22300 (epoch 40), train_loss = 0.834, time/batch = 0.091\n",
            "18049/22300 (epoch 40), train_loss = 0.790, time/batch = 0.092\n",
            "18050/22300 (epoch 40), train_loss = 0.790, time/batch = 0.092\n",
            "18051/22300 (epoch 40), train_loss = 0.770, time/batch = 0.091\n",
            "18052/22300 (epoch 40), train_loss = 0.799, time/batch = 0.101\n",
            "18053/22300 (epoch 40), train_loss = 0.789, time/batch = 0.094\n",
            "18054/22300 (epoch 40), train_loss = 0.771, time/batch = 0.092\n",
            "18055/22300 (epoch 40), train_loss = 0.792, time/batch = 0.092\n",
            "18056/22300 (epoch 40), train_loss = 0.778, time/batch = 0.091\n",
            "18057/22300 (epoch 40), train_loss = 0.802, time/batch = 0.093\n",
            "18058/22300 (epoch 40), train_loss = 0.799, time/batch = 0.091\n",
            "18059/22300 (epoch 40), train_loss = 0.786, time/batch = 0.091\n",
            "18060/22300 (epoch 40), train_loss = 0.829, time/batch = 0.093\n",
            "18061/22300 (epoch 40), train_loss = 0.794, time/batch = 0.092\n",
            "18062/22300 (epoch 40), train_loss = 0.803, time/batch = 0.093\n",
            "18063/22300 (epoch 40), train_loss = 0.831, time/batch = 0.091\n",
            "18064/22300 (epoch 40), train_loss = 0.771, time/batch = 0.092\n",
            "18065/22300 (epoch 40), train_loss = 0.791, time/batch = 0.092\n",
            "18066/22300 (epoch 40), train_loss = 0.825, time/batch = 0.093\n",
            "18067/22300 (epoch 40), train_loss = 0.809, time/batch = 0.092\n",
            "18068/22300 (epoch 40), train_loss = 0.806, time/batch = 0.092\n",
            "18069/22300 (epoch 40), train_loss = 0.806, time/batch = 0.093\n",
            "18070/22300 (epoch 40), train_loss = 0.814, time/batch = 0.093\n",
            "18071/22300 (epoch 40), train_loss = 0.818, time/batch = 0.093\n",
            "18072/22300 (epoch 40), train_loss = 0.813, time/batch = 0.092\n",
            "18073/22300 (epoch 40), train_loss = 0.816, time/batch = 0.093\n",
            "18074/22300 (epoch 40), train_loss = 0.837, time/batch = 0.093\n",
            "18075/22300 (epoch 40), train_loss = 0.797, time/batch = 0.092\n",
            "18076/22300 (epoch 40), train_loss = 0.817, time/batch = 0.094\n",
            "18077/22300 (epoch 40), train_loss = 0.757, time/batch = 0.092\n",
            "18078/22300 (epoch 40), train_loss = 0.791, time/batch = 0.096\n",
            "18079/22300 (epoch 40), train_loss = 0.792, time/batch = 0.092\n",
            "18080/22300 (epoch 40), train_loss = 0.835, time/batch = 0.092\n",
            "18081/22300 (epoch 40), train_loss = 0.798, time/batch = 0.092\n",
            "18082/22300 (epoch 40), train_loss = 0.826, time/batch = 0.092\n",
            "18083/22300 (epoch 40), train_loss = 0.830, time/batch = 0.093\n",
            "18084/22300 (epoch 40), train_loss = 0.796, time/batch = 0.091\n",
            "18085/22300 (epoch 40), train_loss = 0.830, time/batch = 0.093\n",
            "18086/22300 (epoch 40), train_loss = 0.822, time/batch = 0.092\n",
            "18087/22300 (epoch 40), train_loss = 0.753, time/batch = 0.098\n",
            "18088/22300 (epoch 40), train_loss = 0.811, time/batch = 0.090\n",
            "18089/22300 (epoch 40), train_loss = 0.789, time/batch = 0.091\n",
            "18090/22300 (epoch 40), train_loss = 0.788, time/batch = 0.092\n",
            "18091/22300 (epoch 40), train_loss = 0.802, time/batch = 0.092\n",
            "18092/22300 (epoch 40), train_loss = 0.821, time/batch = 0.093\n",
            "18093/22300 (epoch 40), train_loss = 0.799, time/batch = 0.092\n",
            "18094/22300 (epoch 40), train_loss = 0.798, time/batch = 0.091\n",
            "18095/22300 (epoch 40), train_loss = 0.818, time/batch = 0.093\n",
            "18096/22300 (epoch 40), train_loss = 0.805, time/batch = 0.092\n",
            "18097/22300 (epoch 40), train_loss = 0.849, time/batch = 0.093\n",
            "18098/22300 (epoch 40), train_loss = 0.811, time/batch = 0.092\n",
            "18099/22300 (epoch 40), train_loss = 0.838, time/batch = 0.094\n",
            "18100/22300 (epoch 40), train_loss = 0.795, time/batch = 0.091\n",
            "18101/22300 (epoch 40), train_loss = 0.854, time/batch = 0.092\n",
            "18102/22300 (epoch 40), train_loss = 0.794, time/batch = 0.093\n",
            "18103/22300 (epoch 40), train_loss = 0.833, time/batch = 0.092\n",
            "18104/22300 (epoch 40), train_loss = 0.793, time/batch = 0.092\n",
            "18105/22300 (epoch 40), train_loss = 0.838, time/batch = 0.092\n",
            "18106/22300 (epoch 40), train_loss = 0.800, time/batch = 0.093\n",
            "18107/22300 (epoch 40), train_loss = 0.813, time/batch = 0.092\n",
            "18108/22300 (epoch 40), train_loss = 0.797, time/batch = 0.107\n",
            "18109/22300 (epoch 40), train_loss = 0.800, time/batch = 0.088\n",
            "18110/22300 (epoch 40), train_loss = 0.823, time/batch = 0.093\n",
            "18111/22300 (epoch 40), train_loss = 0.824, time/batch = 0.090\n",
            "18112/22300 (epoch 40), train_loss = 0.835, time/batch = 0.093\n",
            "18113/22300 (epoch 40), train_loss = 0.845, time/batch = 0.096\n",
            "18114/22300 (epoch 40), train_loss = 0.827, time/batch = 0.097\n",
            "18115/22300 (epoch 40), train_loss = 0.844, time/batch = 0.092\n",
            "18116/22300 (epoch 40), train_loss = 0.862, time/batch = 0.091\n",
            "18117/22300 (epoch 40), train_loss = 0.825, time/batch = 0.092\n",
            "18118/22300 (epoch 40), train_loss = 0.805, time/batch = 0.094\n",
            "18119/22300 (epoch 40), train_loss = 0.816, time/batch = 0.092\n",
            "18120/22300 (epoch 40), train_loss = 0.805, time/batch = 0.093\n",
            "18121/22300 (epoch 40), train_loss = 0.848, time/batch = 0.091\n",
            "18122/22300 (epoch 40), train_loss = 0.807, time/batch = 0.093\n",
            "18123/22300 (epoch 40), train_loss = 0.803, time/batch = 0.093\n",
            "18124/22300 (epoch 40), train_loss = 0.796, time/batch = 0.097\n",
            "18125/22300 (epoch 40), train_loss = 0.794, time/batch = 0.092\n",
            "18126/22300 (epoch 40), train_loss = 0.817, time/batch = 0.091\n",
            "18127/22300 (epoch 40), train_loss = 0.809, time/batch = 0.092\n",
            "18128/22300 (epoch 40), train_loss = 0.797, time/batch = 0.092\n",
            "18129/22300 (epoch 40), train_loss = 0.812, time/batch = 0.092\n",
            "18130/22300 (epoch 40), train_loss = 0.789, time/batch = 0.093\n",
            "18131/22300 (epoch 40), train_loss = 0.781, time/batch = 0.092\n",
            "18132/22300 (epoch 40), train_loss = 0.841, time/batch = 0.093\n",
            "18133/22300 (epoch 40), train_loss = 0.827, time/batch = 0.092\n",
            "18134/22300 (epoch 40), train_loss = 0.818, time/batch = 0.095\n",
            "18135/22300 (epoch 40), train_loss = 0.802, time/batch = 0.094\n",
            "18136/22300 (epoch 40), train_loss = 0.810, time/batch = 0.092\n",
            "18137/22300 (epoch 40), train_loss = 0.806, time/batch = 0.092\n",
            "18138/22300 (epoch 40), train_loss = 0.792, time/batch = 0.092\n",
            "18139/22300 (epoch 40), train_loss = 0.828, time/batch = 0.092\n",
            "18140/22300 (epoch 40), train_loss = 0.788, time/batch = 0.092\n",
            "18141/22300 (epoch 40), train_loss = 0.800, time/batch = 0.092\n",
            "18142/22300 (epoch 40), train_loss = 0.785, time/batch = 0.092\n",
            "18143/22300 (epoch 40), train_loss = 0.786, time/batch = 0.092\n",
            "18144/22300 (epoch 40), train_loss = 0.804, time/batch = 0.093\n",
            "18145/22300 (epoch 40), train_loss = 0.791, time/batch = 0.093\n",
            "18146/22300 (epoch 40), train_loss = 0.766, time/batch = 0.092\n",
            "18147/22300 (epoch 40), train_loss = 0.809, time/batch = 0.091\n",
            "18148/22300 (epoch 40), train_loss = 0.810, time/batch = 0.092\n",
            "18149/22300 (epoch 40), train_loss = 0.793, time/batch = 0.093\n",
            "18150/22300 (epoch 40), train_loss = 0.771, time/batch = 0.091\n",
            "18151/22300 (epoch 40), train_loss = 0.782, time/batch = 0.093\n",
            "18152/22300 (epoch 40), train_loss = 0.788, time/batch = 0.092\n",
            "18153/22300 (epoch 40), train_loss = 0.794, time/batch = 0.093\n",
            "18154/22300 (epoch 40), train_loss = 0.798, time/batch = 0.091\n",
            "18155/22300 (epoch 40), train_loss = 0.809, time/batch = 0.093\n",
            "18156/22300 (epoch 40), train_loss = 0.802, time/batch = 0.092\n",
            "18157/22300 (epoch 40), train_loss = 0.827, time/batch = 0.093\n",
            "18158/22300 (epoch 40), train_loss = 0.788, time/batch = 0.093\n",
            "18159/22300 (epoch 40), train_loss = 0.761, time/batch = 0.091\n",
            "18160/22300 (epoch 40), train_loss = 0.775, time/batch = 0.092\n",
            "18161/22300 (epoch 40), train_loss = 0.816, time/batch = 0.091\n",
            "18162/22300 (epoch 40), train_loss = 0.793, time/batch = 0.092\n",
            "18163/22300 (epoch 40), train_loss = 0.763, time/batch = 0.091\n",
            "18164/22300 (epoch 40), train_loss = 0.765, time/batch = 0.094\n",
            "18165/22300 (epoch 40), train_loss = 0.799, time/batch = 0.093\n",
            "18166/22300 (epoch 40), train_loss = 0.814, time/batch = 0.092\n",
            "18167/22300 (epoch 40), train_loss = 0.811, time/batch = 0.092\n",
            "18168/22300 (epoch 40), train_loss = 0.853, time/batch = 0.092\n",
            "18169/22300 (epoch 40), train_loss = 0.806, time/batch = 0.092\n",
            "18170/22300 (epoch 40), train_loss = 0.769, time/batch = 0.092\n",
            "18171/22300 (epoch 40), train_loss = 0.791, time/batch = 0.091\n",
            "18172/22300 (epoch 40), train_loss = 0.780, time/batch = 0.093\n",
            "18173/22300 (epoch 40), train_loss = 0.742, time/batch = 0.093\n",
            "18174/22300 (epoch 40), train_loss = 0.777, time/batch = 0.093\n",
            "18175/22300 (epoch 40), train_loss = 0.803, time/batch = 0.092\n",
            "18176/22300 (epoch 40), train_loss = 0.808, time/batch = 0.093\n",
            "18177/22300 (epoch 40), train_loss = 0.772, time/batch = 0.092\n",
            "18178/22300 (epoch 40), train_loss = 0.789, time/batch = 0.092\n",
            "18179/22300 (epoch 40), train_loss = 0.801, time/batch = 0.093\n",
            "18180/22300 (epoch 40), train_loss = 0.836, time/batch = 0.105\n",
            "18181/22300 (epoch 40), train_loss = 0.776, time/batch = 0.095\n",
            "18182/22300 (epoch 40), train_loss = 0.776, time/batch = 0.094\n",
            "18183/22300 (epoch 40), train_loss = 0.813, time/batch = 0.102\n",
            "18184/22300 (epoch 40), train_loss = 0.802, time/batch = 0.093\n",
            "18185/22300 (epoch 40), train_loss = 0.811, time/batch = 0.091\n",
            "18186/22300 (epoch 40), train_loss = 0.776, time/batch = 0.094\n",
            "18187/22300 (epoch 40), train_loss = 0.821, time/batch = 0.093\n",
            "18188/22300 (epoch 40), train_loss = 0.761, time/batch = 0.092\n",
            "18189/22300 (epoch 40), train_loss = 0.799, time/batch = 0.094\n",
            "18190/22300 (epoch 40), train_loss = 0.804, time/batch = 0.092\n",
            "18191/22300 (epoch 40), train_loss = 0.796, time/batch = 0.092\n",
            "18192/22300 (epoch 40), train_loss = 0.787, time/batch = 0.096\n",
            "18193/22300 (epoch 40), train_loss = 0.806, time/batch = 0.093\n",
            "18194/22300 (epoch 40), train_loss = 0.823, time/batch = 0.092\n",
            "18195/22300 (epoch 40), train_loss = 0.797, time/batch = 0.091\n",
            "18196/22300 (epoch 40), train_loss = 0.837, time/batch = 0.092\n",
            "18197/22300 (epoch 40), train_loss = 0.817, time/batch = 0.091\n",
            "18198/22300 (epoch 40), train_loss = 0.792, time/batch = 0.093\n",
            "18199/22300 (epoch 40), train_loss = 0.809, time/batch = 0.094\n",
            "18200/22300 (epoch 40), train_loss = 0.806, time/batch = 0.092\n",
            "18201/22300 (epoch 40), train_loss = 0.800, time/batch = 0.093\n",
            "18202/22300 (epoch 40), train_loss = 0.795, time/batch = 0.092\n",
            "18203/22300 (epoch 40), train_loss = 0.799, time/batch = 0.092\n",
            "18204/22300 (epoch 40), train_loss = 0.812, time/batch = 0.091\n",
            "18205/22300 (epoch 40), train_loss = 0.782, time/batch = 0.091\n",
            "18206/22300 (epoch 40), train_loss = 0.773, time/batch = 0.091\n",
            "18207/22300 (epoch 40), train_loss = 0.742, time/batch = 0.092\n",
            "18208/22300 (epoch 40), train_loss = 0.766, time/batch = 0.092\n",
            "18209/22300 (epoch 40), train_loss = 0.776, time/batch = 0.093\n",
            "18210/22300 (epoch 40), train_loss = 0.803, time/batch = 0.094\n",
            "18211/22300 (epoch 40), train_loss = 0.755, time/batch = 0.095\n",
            "18212/22300 (epoch 40), train_loss = 0.821, time/batch = 0.092\n",
            "18213/22300 (epoch 40), train_loss = 0.777, time/batch = 0.093\n",
            "18214/22300 (epoch 40), train_loss = 0.802, time/batch = 0.092\n",
            "18215/22300 (epoch 40), train_loss = 0.782, time/batch = 0.093\n",
            "18216/22300 (epoch 40), train_loss = 0.803, time/batch = 0.091\n",
            "18217/22300 (epoch 40), train_loss = 0.809, time/batch = 0.095\n",
            "18218/22300 (epoch 40), train_loss = 0.769, time/batch = 0.092\n",
            "18219/22300 (epoch 40), train_loss = 0.763, time/batch = 0.092\n",
            "18220/22300 (epoch 40), train_loss = 0.775, time/batch = 0.094\n",
            "18221/22300 (epoch 40), train_loss = 0.785, time/batch = 0.092\n",
            "18222/22300 (epoch 40), train_loss = 0.766, time/batch = 0.093\n",
            "18223/22300 (epoch 40), train_loss = 0.746, time/batch = 0.093\n",
            "18224/22300 (epoch 40), train_loss = 0.782, time/batch = 0.092\n",
            "18225/22300 (epoch 40), train_loss = 0.783, time/batch = 0.093\n",
            "18226/22300 (epoch 40), train_loss = 0.757, time/batch = 0.091\n",
            "18227/22300 (epoch 40), train_loss = 0.787, time/batch = 0.093\n",
            "18228/22300 (epoch 40), train_loss = 0.766, time/batch = 0.091\n",
            "18229/22300 (epoch 40), train_loss = 0.757, time/batch = 0.092\n",
            "18230/22300 (epoch 40), train_loss = 0.767, time/batch = 0.094\n",
            "18231/22300 (epoch 40), train_loss = 0.770, time/batch = 0.092\n",
            "18232/22300 (epoch 40), train_loss = 0.804, time/batch = 0.093\n",
            "18233/22300 (epoch 40), train_loss = 0.822, time/batch = 0.100\n",
            "18234/22300 (epoch 40), train_loss = 0.789, time/batch = 0.092\n",
            "18235/22300 (epoch 40), train_loss = 0.804, time/batch = 0.092\n",
            "18236/22300 (epoch 40), train_loss = 0.790, time/batch = 0.093\n",
            "18237/22300 (epoch 40), train_loss = 0.784, time/batch = 0.092\n",
            "18238/22300 (epoch 40), train_loss = 0.800, time/batch = 0.093\n",
            "18239/22300 (epoch 40), train_loss = 0.773, time/batch = 0.091\n",
            "18240/22300 (epoch 40), train_loss = 0.803, time/batch = 0.093\n",
            "18241/22300 (epoch 40), train_loss = 0.834, time/batch = 0.092\n",
            "18242/22300 (epoch 40), train_loss = 0.819, time/batch = 0.093\n",
            "18243/22300 (epoch 40), train_loss = 0.802, time/batch = 0.093\n",
            "18244/22300 (epoch 40), train_loss = 0.815, time/batch = 0.092\n",
            "18245/22300 (epoch 40), train_loss = 0.828, time/batch = 0.096\n",
            "18246/22300 (epoch 40), train_loss = 0.810, time/batch = 0.093\n",
            "18247/22300 (epoch 40), train_loss = 0.829, time/batch = 0.092\n",
            "18248/22300 (epoch 40), train_loss = 0.835, time/batch = 0.094\n",
            "18249/22300 (epoch 40), train_loss = 0.826, time/batch = 0.094\n",
            "18250/22300 (epoch 40), train_loss = 0.822, time/batch = 0.096\n",
            "18251/22300 (epoch 40), train_loss = 0.781, time/batch = 0.103\n",
            "18252/22300 (epoch 40), train_loss = 0.791, time/batch = 0.092\n",
            "18253/22300 (epoch 40), train_loss = 0.762, time/batch = 0.092\n",
            "18254/22300 (epoch 40), train_loss = 0.797, time/batch = 0.093\n",
            "18255/22300 (epoch 40), train_loss = 0.824, time/batch = 0.092\n",
            "18256/22300 (epoch 40), train_loss = 0.804, time/batch = 0.091\n",
            "18257/22300 (epoch 40), train_loss = 0.805, time/batch = 0.091\n",
            "18258/22300 (epoch 40), train_loss = 0.829, time/batch = 0.094\n",
            "18259/22300 (epoch 40), train_loss = 0.819, time/batch = 0.093\n",
            "18260/22300 (epoch 40), train_loss = 0.825, time/batch = 0.092\n",
            "18261/22300 (epoch 40), train_loss = 0.820, time/batch = 0.096\n",
            "18262/22300 (epoch 40), train_loss = 0.835, time/batch = 0.092\n",
            "18263/22300 (epoch 40), train_loss = 0.820, time/batch = 0.092\n",
            "18264/22300 (epoch 40), train_loss = 0.840, time/batch = 0.092\n",
            "18265/22300 (epoch 40), train_loss = 0.805, time/batch = 0.091\n",
            "18266/22300 (epoch 40), train_loss = 0.854, time/batch = 0.093\n",
            "18267/22300 (epoch 40), train_loss = 0.775, time/batch = 0.092\n",
            "18268/22300 (epoch 40), train_loss = 0.797, time/batch = 0.092\n",
            "18269/22300 (epoch 40), train_loss = 0.779, time/batch = 0.092\n",
            "18270/22300 (epoch 40), train_loss = 0.783, time/batch = 0.093\n",
            "18271/22300 (epoch 40), train_loss = 0.828, time/batch = 0.095\n",
            "18272/22300 (epoch 40), train_loss = 0.817, time/batch = 0.094\n",
            "18273/22300 (epoch 40), train_loss = 0.834, time/batch = 0.095\n",
            "18274/22300 (epoch 40), train_loss = 0.813, time/batch = 0.092\n",
            "18275/22300 (epoch 40), train_loss = 0.807, time/batch = 0.091\n",
            "18276/22300 (epoch 40), train_loss = 0.790, time/batch = 0.092\n",
            "18277/22300 (epoch 40), train_loss = 0.802, time/batch = 0.092\n",
            "18278/22300 (epoch 40), train_loss = 0.882, time/batch = 0.093\n",
            "18279/22300 (epoch 40), train_loss = 0.810, time/batch = 0.092\n",
            "18280/22300 (epoch 40), train_loss = 0.823, time/batch = 0.093\n",
            "18281/22300 (epoch 40), train_loss = 0.798, time/batch = 0.093\n",
            "18282/22300 (epoch 40), train_loss = 0.820, time/batch = 0.093\n",
            "18283/22300 (epoch 40), train_loss = 0.827, time/batch = 0.098\n",
            "18284/22300 (epoch 40), train_loss = 0.807, time/batch = 0.092\n",
            "18285/22300 (epoch 40), train_loss = 0.825, time/batch = 0.094\n",
            "18286/22300 (epoch 41), train_loss = 0.533, time/batch = 0.087\n",
            "18287/22300 (epoch 41), train_loss = 0.820, time/batch = 0.093\n",
            "18288/22300 (epoch 41), train_loss = 0.841, time/batch = 0.091\n",
            "18289/22300 (epoch 41), train_loss = 0.839, time/batch = 0.093\n",
            "18290/22300 (epoch 41), train_loss = 0.879, time/batch = 0.093\n",
            "18291/22300 (epoch 41), train_loss = 0.806, time/batch = 0.091\n",
            "18292/22300 (epoch 41), train_loss = 0.834, time/batch = 0.092\n",
            "18293/22300 (epoch 41), train_loss = 0.843, time/batch = 0.092\n",
            "18294/22300 (epoch 41), train_loss = 0.809, time/batch = 0.092\n",
            "18295/22300 (epoch 41), train_loss = 0.810, time/batch = 0.099\n",
            "18296/22300 (epoch 41), train_loss = 0.878, time/batch = 0.093\n",
            "18297/22300 (epoch 41), train_loss = 0.813, time/batch = 0.093\n",
            "18298/22300 (epoch 41), train_loss = 0.862, time/batch = 0.091\n",
            "18299/22300 (epoch 41), train_loss = 0.857, time/batch = 0.093\n",
            "18300/22300 (epoch 41), train_loss = 0.852, time/batch = 0.091\n",
            "18301/22300 (epoch 41), train_loss = 0.860, time/batch = 0.091\n",
            "18302/22300 (epoch 41), train_loss = 0.858, time/batch = 0.091\n",
            "18303/22300 (epoch 41), train_loss = 0.816, time/batch = 0.091\n",
            "18304/22300 (epoch 41), train_loss = 0.815, time/batch = 0.092\n",
            "18305/22300 (epoch 41), train_loss = 0.844, time/batch = 0.094\n",
            "18306/22300 (epoch 41), train_loss = 0.815, time/batch = 0.095\n",
            "18307/22300 (epoch 41), train_loss = 0.806, time/batch = 0.094\n",
            "18308/22300 (epoch 41), train_loss = 0.788, time/batch = 0.092\n",
            "18309/22300 (epoch 41), train_loss = 0.784, time/batch = 0.093\n",
            "18310/22300 (epoch 41), train_loss = 0.760, time/batch = 0.090\n",
            "18311/22300 (epoch 41), train_loss = 0.796, time/batch = 0.091\n",
            "18312/22300 (epoch 41), train_loss = 0.803, time/batch = 0.091\n",
            "18313/22300 (epoch 41), train_loss = 0.789, time/batch = 0.091\n",
            "18314/22300 (epoch 41), train_loss = 0.811, time/batch = 0.091\n",
            "18315/22300 (epoch 41), train_loss = 0.799, time/batch = 0.091\n",
            "18316/22300 (epoch 41), train_loss = 0.788, time/batch = 0.093\n",
            "18317/22300 (epoch 41), train_loss = 0.812, time/batch = 0.094\n",
            "18318/22300 (epoch 41), train_loss = 0.796, time/batch = 0.093\n",
            "18319/22300 (epoch 41), train_loss = 0.815, time/batch = 0.092\n",
            "18320/22300 (epoch 41), train_loss = 0.823, time/batch = 0.092\n",
            "18321/22300 (epoch 41), train_loss = 0.784, time/batch = 0.092\n",
            "18322/22300 (epoch 41), train_loss = 0.845, time/batch = 0.092\n",
            "18323/22300 (epoch 41), train_loss = 0.796, time/batch = 0.092\n",
            "18324/22300 (epoch 41), train_loss = 0.815, time/batch = 0.091\n",
            "18325/22300 (epoch 41), train_loss = 0.778, time/batch = 0.091\n",
            "18326/22300 (epoch 41), train_loss = 0.797, time/batch = 0.093\n",
            "18327/22300 (epoch 41), train_loss = 0.819, time/batch = 0.098\n",
            "18328/22300 (epoch 41), train_loss = 0.816, time/batch = 0.090\n",
            "18329/22300 (epoch 41), train_loss = 0.809, time/batch = 0.092\n",
            "18330/22300 (epoch 41), train_loss = 0.796, time/batch = 0.092\n",
            "18331/22300 (epoch 41), train_loss = 0.831, time/batch = 0.092\n",
            "18332/22300 (epoch 41), train_loss = 0.795, time/batch = 0.093\n",
            "18333/22300 (epoch 41), train_loss = 0.785, time/batch = 0.091\n",
            "18334/22300 (epoch 41), train_loss = 0.798, time/batch = 0.092\n",
            "18335/22300 (epoch 41), train_loss = 0.794, time/batch = 0.091\n",
            "18336/22300 (epoch 41), train_loss = 0.837, time/batch = 0.092\n",
            "18337/22300 (epoch 41), train_loss = 0.795, time/batch = 0.093\n",
            "18338/22300 (epoch 41), train_loss = 0.790, time/batch = 0.093\n",
            "18339/22300 (epoch 41), train_loss = 0.790, time/batch = 0.094\n",
            "18340/22300 (epoch 41), train_loss = 0.777, time/batch = 0.092\n",
            "18341/22300 (epoch 41), train_loss = 0.785, time/batch = 0.092\n",
            "18342/22300 (epoch 41), train_loss = 0.781, time/batch = 0.092\n",
            "18343/22300 (epoch 41), train_loss = 0.838, time/batch = 0.092\n",
            "18344/22300 (epoch 41), train_loss = 0.793, time/batch = 0.092\n",
            "18345/22300 (epoch 41), train_loss = 0.786, time/batch = 0.092\n",
            "18346/22300 (epoch 41), train_loss = 0.823, time/batch = 0.092\n",
            "18347/22300 (epoch 41), train_loss = 0.784, time/batch = 0.095\n",
            "18348/22300 (epoch 41), train_loss = 0.764, time/batch = 0.093\n",
            "18349/22300 (epoch 41), train_loss = 0.805, time/batch = 0.091\n",
            "18350/22300 (epoch 41), train_loss = 0.784, time/batch = 0.095\n",
            "18351/22300 (epoch 41), train_loss = 0.798, time/batch = 0.094\n",
            "18352/22300 (epoch 41), train_loss = 0.790, time/batch = 0.091\n",
            "18353/22300 (epoch 41), train_loss = 0.842, time/batch = 0.094\n",
            "18354/22300 (epoch 41), train_loss = 0.808, time/batch = 0.091\n",
            "18355/22300 (epoch 41), train_loss = 0.810, time/batch = 0.093\n",
            "18356/22300 (epoch 41), train_loss = 0.810, time/batch = 0.096\n",
            "18357/22300 (epoch 41), train_loss = 0.760, time/batch = 0.091\n",
            "18358/22300 (epoch 41), train_loss = 0.794, time/batch = 0.093\n",
            "18359/22300 (epoch 41), train_loss = 0.770, time/batch = 0.091\n",
            "18360/22300 (epoch 41), train_loss = 0.777, time/batch = 0.094\n",
            "18361/22300 (epoch 41), train_loss = 0.818, time/batch = 0.093\n",
            "18362/22300 (epoch 41), train_loss = 0.810, time/batch = 0.092\n",
            "18363/22300 (epoch 41), train_loss = 0.795, time/batch = 0.093\n",
            "18364/22300 (epoch 41), train_loss = 0.790, time/batch = 0.092\n",
            "18365/22300 (epoch 41), train_loss = 0.815, time/batch = 0.092\n",
            "18366/22300 (epoch 41), train_loss = 0.794, time/batch = 0.092\n",
            "18367/22300 (epoch 41), train_loss = 0.770, time/batch = 0.093\n",
            "18368/22300 (epoch 41), train_loss = 0.791, time/batch = 0.093\n",
            "18369/22300 (epoch 41), train_loss = 0.772, time/batch = 0.092\n",
            "18370/22300 (epoch 41), train_loss = 0.804, time/batch = 0.093\n",
            "18371/22300 (epoch 41), train_loss = 0.804, time/batch = 0.092\n",
            "18372/22300 (epoch 41), train_loss = 0.782, time/batch = 0.093\n",
            "18373/22300 (epoch 41), train_loss = 0.810, time/batch = 0.097\n",
            "18374/22300 (epoch 41), train_loss = 0.768, time/batch = 0.091\n",
            "18375/22300 (epoch 41), train_loss = 0.808, time/batch = 0.092\n",
            "18376/22300 (epoch 41), train_loss = 0.777, time/batch = 0.092\n",
            "18377/22300 (epoch 41), train_loss = 0.782, time/batch = 0.092\n",
            "18378/22300 (epoch 41), train_loss = 0.812, time/batch = 0.092\n",
            "18379/22300 (epoch 41), train_loss = 0.802, time/batch = 0.093\n",
            "18380/22300 (epoch 41), train_loss = 0.807, time/batch = 0.093\n",
            "18381/22300 (epoch 41), train_loss = 0.804, time/batch = 0.092\n",
            "18382/22300 (epoch 41), train_loss = 0.758, time/batch = 0.093\n",
            "18383/22300 (epoch 41), train_loss = 0.797, time/batch = 0.093\n",
            "18384/22300 (epoch 41), train_loss = 0.765, time/batch = 0.093\n",
            "18385/22300 (epoch 41), train_loss = 0.766, time/batch = 0.092\n",
            "18386/22300 (epoch 41), train_loss = 0.779, time/batch = 0.102\n",
            "18387/22300 (epoch 41), train_loss = 0.780, time/batch = 0.092\n",
            "18388/22300 (epoch 41), train_loss = 0.796, time/batch = 0.092\n",
            "18389/22300 (epoch 41), train_loss = 0.764, time/batch = 0.092\n",
            "18390/22300 (epoch 41), train_loss = 0.799, time/batch = 0.093\n",
            "18391/22300 (epoch 41), train_loss = 0.765, time/batch = 0.092\n",
            "18392/22300 (epoch 41), train_loss = 0.758, time/batch = 0.093\n",
            "18393/22300 (epoch 41), train_loss = 0.767, time/batch = 0.093\n",
            "18394/22300 (epoch 41), train_loss = 0.773, time/batch = 0.091\n",
            "18395/22300 (epoch 41), train_loss = 0.772, time/batch = 0.094\n",
            "18396/22300 (epoch 41), train_loss = 0.744, time/batch = 0.092\n",
            "18397/22300 (epoch 41), train_loss = 0.737, time/batch = 0.092\n",
            "18398/22300 (epoch 41), train_loss = 0.784, time/batch = 0.092\n",
            "18399/22300 (epoch 41), train_loss = 0.753, time/batch = 0.091\n",
            "18400/22300 (epoch 41), train_loss = 0.785, time/batch = 0.093\n",
            "18401/22300 (epoch 41), train_loss = 0.785, time/batch = 0.092\n",
            "18402/22300 (epoch 41), train_loss = 0.778, time/batch = 0.093\n",
            "18403/22300 (epoch 41), train_loss = 0.809, time/batch = 0.092\n",
            "18404/22300 (epoch 41), train_loss = 0.833, time/batch = 0.092\n",
            "18405/22300 (epoch 41), train_loss = 0.798, time/batch = 0.093\n",
            "18406/22300 (epoch 41), train_loss = 0.833, time/batch = 0.092\n",
            "18407/22300 (epoch 41), train_loss = 0.809, time/batch = 0.093\n",
            "18408/22300 (epoch 41), train_loss = 0.788, time/batch = 0.091\n",
            "18409/22300 (epoch 41), train_loss = 0.781, time/batch = 0.092\n",
            "18410/22300 (epoch 41), train_loss = 0.815, time/batch = 0.092\n",
            "18411/22300 (epoch 41), train_loss = 0.805, time/batch = 0.093\n",
            "18412/22300 (epoch 41), train_loss = 0.793, time/batch = 0.095\n",
            "18413/22300 (epoch 41), train_loss = 0.812, time/batch = 0.097\n",
            "18414/22300 (epoch 41), train_loss = 0.808, time/batch = 0.093\n",
            "18415/22300 (epoch 41), train_loss = 0.795, time/batch = 0.094\n",
            "18416/22300 (epoch 41), train_loss = 0.779, time/batch = 0.097\n",
            "18417/22300 (epoch 41), train_loss = 0.810, time/batch = 0.092\n",
            "18418/22300 (epoch 41), train_loss = 0.819, time/batch = 0.098\n",
            "18419/22300 (epoch 41), train_loss = 0.814, time/batch = 0.092\n",
            "18420/22300 (epoch 41), train_loss = 0.795, time/batch = 0.091\n",
            "18421/22300 (epoch 41), train_loss = 0.835, time/batch = 0.093\n",
            "18422/22300 (epoch 41), train_loss = 0.844, time/batch = 0.093\n",
            "18423/22300 (epoch 41), train_loss = 0.840, time/batch = 0.093\n",
            "18424/22300 (epoch 41), train_loss = 0.841, time/batch = 0.091\n",
            "18425/22300 (epoch 41), train_loss = 0.849, time/batch = 0.094\n",
            "18426/22300 (epoch 41), train_loss = 0.813, time/batch = 0.091\n",
            "18427/22300 (epoch 41), train_loss = 0.809, time/batch = 0.093\n",
            "18428/22300 (epoch 41), train_loss = 0.789, time/batch = 0.093\n",
            "18429/22300 (epoch 41), train_loss = 0.806, time/batch = 0.091\n",
            "18430/22300 (epoch 41), train_loss = 0.811, time/batch = 0.092\n",
            "18431/22300 (epoch 41), train_loss = 0.872, time/batch = 0.092\n",
            "18432/22300 (epoch 41), train_loss = 0.813, time/batch = 0.092\n",
            "18433/22300 (epoch 41), train_loss = 0.810, time/batch = 0.092\n",
            "18434/22300 (epoch 41), train_loss = 0.816, time/batch = 0.092\n",
            "18435/22300 (epoch 41), train_loss = 0.816, time/batch = 0.093\n",
            "18436/22300 (epoch 41), train_loss = 0.836, time/batch = 0.092\n",
            "18437/22300 (epoch 41), train_loss = 0.824, time/batch = 0.094\n",
            "18438/22300 (epoch 41), train_loss = 0.811, time/batch = 0.092\n",
            "18439/22300 (epoch 41), train_loss = 0.790, time/batch = 0.094\n",
            "18440/22300 (epoch 41), train_loss = 0.806, time/batch = 0.093\n",
            "18441/22300 (epoch 41), train_loss = 0.803, time/batch = 0.091\n",
            "18442/22300 (epoch 41), train_loss = 0.804, time/batch = 0.093\n",
            "18443/22300 (epoch 41), train_loss = 0.793, time/batch = 0.092\n",
            "18444/22300 (epoch 41), train_loss = 0.836, time/batch = 0.093\n",
            "18445/22300 (epoch 41), train_loss = 0.808, time/batch = 0.092\n",
            "18446/22300 (epoch 41), train_loss = 0.792, time/batch = 0.092\n",
            "18447/22300 (epoch 41), train_loss = 0.813, time/batch = 0.094\n",
            "18448/22300 (epoch 41), train_loss = 0.817, time/batch = 0.093\n",
            "18449/22300 (epoch 41), train_loss = 0.798, time/batch = 0.093\n",
            "18450/22300 (epoch 41), train_loss = 0.808, time/batch = 0.092\n",
            "18451/22300 (epoch 41), train_loss = 0.808, time/batch = 0.092\n",
            "18452/22300 (epoch 41), train_loss = 0.785, time/batch = 0.092\n",
            "18453/22300 (epoch 41), train_loss = 0.767, time/batch = 0.092\n",
            "18454/22300 (epoch 41), train_loss = 0.769, time/batch = 0.093\n",
            "18455/22300 (epoch 41), train_loss = 0.808, time/batch = 0.092\n",
            "18456/22300 (epoch 41), train_loss = 0.791, time/batch = 0.092\n",
            "18457/22300 (epoch 41), train_loss = 0.784, time/batch = 0.092\n",
            "18458/22300 (epoch 41), train_loss = 0.775, time/batch = 0.093\n",
            "18459/22300 (epoch 41), train_loss = 0.759, time/batch = 0.093\n",
            "18460/22300 (epoch 41), train_loss = 0.763, time/batch = 0.091\n",
            "18461/22300 (epoch 41), train_loss = 0.754, time/batch = 0.093\n",
            "18462/22300 (epoch 41), train_loss = 0.730, time/batch = 0.092\n",
            "18463/22300 (epoch 41), train_loss = 0.769, time/batch = 0.091\n",
            "18464/22300 (epoch 41), train_loss = 0.714, time/batch = 0.092\n",
            "18465/22300 (epoch 41), train_loss = 0.800, time/batch = 0.092\n",
            "18466/22300 (epoch 41), train_loss = 0.780, time/batch = 0.092\n",
            "18467/22300 (epoch 41), train_loss = 0.763, time/batch = 0.093\n",
            "18468/22300 (epoch 41), train_loss = 0.796, time/batch = 0.093\n",
            "18469/22300 (epoch 41), train_loss = 0.769, time/batch = 0.092\n",
            "18470/22300 (epoch 41), train_loss = 0.750, time/batch = 0.093\n",
            "18471/22300 (epoch 41), train_loss = 0.744, time/batch = 0.091\n",
            "18472/22300 (epoch 41), train_loss = 0.814, time/batch = 0.092\n",
            "18473/22300 (epoch 41), train_loss = 0.772, time/batch = 0.092\n",
            "18474/22300 (epoch 41), train_loss = 0.816, time/batch = 0.095\n",
            "18475/22300 (epoch 41), train_loss = 0.794, time/batch = 0.092\n",
            "18476/22300 (epoch 41), train_loss = 0.810, time/batch = 0.091\n",
            "18477/22300 (epoch 41), train_loss = 0.805, time/batch = 0.092\n",
            "18478/22300 (epoch 41), train_loss = 0.793, time/batch = 0.091\n",
            "18479/22300 (epoch 41), train_loss = 0.814, time/batch = 0.093\n",
            "18480/22300 (epoch 41), train_loss = 0.807, time/batch = 0.093\n",
            "18481/22300 (epoch 41), train_loss = 0.780, time/batch = 0.092\n",
            "18482/22300 (epoch 41), train_loss = 0.776, time/batch = 0.094\n",
            "18483/22300 (epoch 41), train_loss = 0.784, time/batch = 0.092\n",
            "18484/22300 (epoch 41), train_loss = 0.810, time/batch = 0.094\n",
            "18485/22300 (epoch 41), train_loss = 0.787, time/batch = 0.094\n",
            "18486/22300 (epoch 41), train_loss = 0.825, time/batch = 0.092\n",
            "18487/22300 (epoch 41), train_loss = 0.788, time/batch = 0.093\n",
            "18488/22300 (epoch 41), train_loss = 0.775, time/batch = 0.095\n",
            "18489/22300 (epoch 41), train_loss = 0.768, time/batch = 0.096\n",
            "18490/22300 (epoch 41), train_loss = 0.772, time/batch = 0.093\n",
            "18491/22300 (epoch 41), train_loss = 0.776, time/batch = 0.092\n",
            "18492/22300 (epoch 41), train_loss = 0.827, time/batch = 0.092\n",
            "18493/22300 (epoch 41), train_loss = 0.784, time/batch = 0.092\n",
            "18494/22300 (epoch 41), train_loss = 0.817, time/batch = 0.092\n",
            "18495/22300 (epoch 41), train_loss = 0.772, time/batch = 0.092\n",
            "18496/22300 (epoch 41), train_loss = 0.769, time/batch = 0.093\n",
            "18497/22300 (epoch 41), train_loss = 0.757, time/batch = 0.093\n",
            "18498/22300 (epoch 41), train_loss = 0.784, time/batch = 0.093\n",
            "18499/22300 (epoch 41), train_loss = 0.783, time/batch = 0.096\n",
            "18500/22300 (epoch 41), train_loss = 0.756, time/batch = 0.099\n",
            "18501/22300 (epoch 41), train_loss = 0.779, time/batch = 0.092\n",
            "18502/22300 (epoch 41), train_loss = 0.769, time/batch = 0.093\n",
            "18503/22300 (epoch 41), train_loss = 0.795, time/batch = 0.094\n",
            "18504/22300 (epoch 41), train_loss = 0.780, time/batch = 0.093\n",
            "18505/22300 (epoch 41), train_loss = 0.784, time/batch = 0.098\n",
            "18506/22300 (epoch 41), train_loss = 0.807, time/batch = 0.093\n",
            "18507/22300 (epoch 41), train_loss = 0.782, time/batch = 0.092\n",
            "18508/22300 (epoch 41), train_loss = 0.785, time/batch = 0.096\n",
            "18509/22300 (epoch 41), train_loss = 0.826, time/batch = 0.093\n",
            "18510/22300 (epoch 41), train_loss = 0.773, time/batch = 0.093\n",
            "18511/22300 (epoch 41), train_loss = 0.789, time/batch = 0.092\n",
            "18512/22300 (epoch 41), train_loss = 0.821, time/batch = 0.092\n",
            "18513/22300 (epoch 41), train_loss = 0.794, time/batch = 0.091\n",
            "18514/22300 (epoch 41), train_loss = 0.797, time/batch = 0.093\n",
            "18515/22300 (epoch 41), train_loss = 0.793, time/batch = 0.092\n",
            "18516/22300 (epoch 41), train_loss = 0.790, time/batch = 0.092\n",
            "18517/22300 (epoch 41), train_loss = 0.803, time/batch = 0.092\n",
            "18518/22300 (epoch 41), train_loss = 0.783, time/batch = 0.092\n",
            "18519/22300 (epoch 41), train_loss = 0.798, time/batch = 0.093\n",
            "18520/22300 (epoch 41), train_loss = 0.817, time/batch = 0.092\n",
            "18521/22300 (epoch 41), train_loss = 0.790, time/batch = 0.092\n",
            "18522/22300 (epoch 41), train_loss = 0.813, time/batch = 0.095\n",
            "18523/22300 (epoch 41), train_loss = 0.759, time/batch = 0.099\n",
            "18524/22300 (epoch 41), train_loss = 0.795, time/batch = 0.093\n",
            "18525/22300 (epoch 41), train_loss = 0.789, time/batch = 0.093\n",
            "18526/22300 (epoch 41), train_loss = 0.824, time/batch = 0.093\n",
            "18527/22300 (epoch 41), train_loss = 0.785, time/batch = 0.100\n",
            "18528/22300 (epoch 41), train_loss = 0.793, time/batch = 0.091\n",
            "18529/22300 (epoch 41), train_loss = 0.810, time/batch = 0.092\n",
            "18530/22300 (epoch 41), train_loss = 0.777, time/batch = 0.093\n",
            "18531/22300 (epoch 41), train_loss = 0.813, time/batch = 0.093\n",
            "18532/22300 (epoch 41), train_loss = 0.813, time/batch = 0.093\n",
            "18533/22300 (epoch 41), train_loss = 0.742, time/batch = 0.094\n",
            "18534/22300 (epoch 41), train_loss = 0.810, time/batch = 0.104\n",
            "18535/22300 (epoch 41), train_loss = 0.791, time/batch = 0.100\n",
            "18536/22300 (epoch 41), train_loss = 0.798, time/batch = 0.095\n",
            "18537/22300 (epoch 41), train_loss = 0.783, time/batch = 0.093\n",
            "18538/22300 (epoch 41), train_loss = 0.805, time/batch = 0.097\n",
            "18539/22300 (epoch 41), train_loss = 0.791, time/batch = 0.090\n",
            "18540/22300 (epoch 41), train_loss = 0.782, time/batch = 0.093\n",
            "18541/22300 (epoch 41), train_loss = 0.813, time/batch = 0.093\n",
            "18542/22300 (epoch 41), train_loss = 0.795, time/batch = 0.092\n",
            "18543/22300 (epoch 41), train_loss = 0.840, time/batch = 0.094\n",
            "18544/22300 (epoch 41), train_loss = 0.800, time/batch = 0.093\n",
            "18545/22300 (epoch 41), train_loss = 0.831, time/batch = 0.093\n",
            "18546/22300 (epoch 41), train_loss = 0.806, time/batch = 0.092\n",
            "18547/22300 (epoch 41), train_loss = 0.864, time/batch = 0.096\n",
            "18548/22300 (epoch 41), train_loss = 0.800, time/batch = 0.093\n",
            "18549/22300 (epoch 41), train_loss = 0.830, time/batch = 0.091\n",
            "18550/22300 (epoch 41), train_loss = 0.793, time/batch = 0.092\n",
            "18551/22300 (epoch 41), train_loss = 0.822, time/batch = 0.091\n",
            "18552/22300 (epoch 41), train_loss = 0.793, time/batch = 0.095\n",
            "18553/22300 (epoch 41), train_loss = 0.816, time/batch = 0.093\n",
            "18554/22300 (epoch 41), train_loss = 0.790, time/batch = 0.093\n",
            "18555/22300 (epoch 41), train_loss = 0.799, time/batch = 0.093\n",
            "18556/22300 (epoch 41), train_loss = 0.816, time/batch = 0.093\n",
            "18557/22300 (epoch 41), train_loss = 0.816, time/batch = 0.092\n",
            "18558/22300 (epoch 41), train_loss = 0.828, time/batch = 0.092\n",
            "18559/22300 (epoch 41), train_loss = 0.834, time/batch = 0.091\n",
            "18560/22300 (epoch 41), train_loss = 0.817, time/batch = 0.093\n",
            "18561/22300 (epoch 41), train_loss = 0.818, time/batch = 0.093\n",
            "18562/22300 (epoch 41), train_loss = 0.849, time/batch = 0.094\n",
            "18563/22300 (epoch 41), train_loss = 0.815, time/batch = 0.093\n",
            "18564/22300 (epoch 41), train_loss = 0.799, time/batch = 0.092\n",
            "18565/22300 (epoch 41), train_loss = 0.811, time/batch = 0.093\n",
            "18566/22300 (epoch 41), train_loss = 0.804, time/batch = 0.093\n",
            "18567/22300 (epoch 41), train_loss = 0.850, time/batch = 0.092\n",
            "18568/22300 (epoch 41), train_loss = 0.803, time/batch = 0.093\n",
            "18569/22300 (epoch 41), train_loss = 0.790, time/batch = 0.092\n",
            "18570/22300 (epoch 41), train_loss = 0.790, time/batch = 0.093\n",
            "18571/22300 (epoch 41), train_loss = 0.794, time/batch = 0.091\n",
            "18572/22300 (epoch 41), train_loss = 0.810, time/batch = 0.093\n",
            "18573/22300 (epoch 41), train_loss = 0.810, time/batch = 0.091\n",
            "18574/22300 (epoch 41), train_loss = 0.796, time/batch = 0.096\n",
            "18575/22300 (epoch 41), train_loss = 0.802, time/batch = 0.094\n",
            "18576/22300 (epoch 41), train_loss = 0.787, time/batch = 0.092\n",
            "18577/22300 (epoch 41), train_loss = 0.776, time/batch = 0.104\n",
            "18578/22300 (epoch 41), train_loss = 0.821, time/batch = 0.092\n",
            "18579/22300 (epoch 41), train_loss = 0.815, time/batch = 0.092\n",
            "18580/22300 (epoch 41), train_loss = 0.810, time/batch = 0.092\n",
            "18581/22300 (epoch 41), train_loss = 0.806, time/batch = 0.093\n",
            "18582/22300 (epoch 41), train_loss = 0.807, time/batch = 0.092\n",
            "18583/22300 (epoch 41), train_loss = 0.796, time/batch = 0.092\n",
            "18584/22300 (epoch 41), train_loss = 0.785, time/batch = 0.092\n",
            "18585/22300 (epoch 41), train_loss = 0.814, time/batch = 0.093\n",
            "18586/22300 (epoch 41), train_loss = 0.782, time/batch = 0.097\n",
            "18587/22300 (epoch 41), train_loss = 0.790, time/batch = 0.092\n",
            "18588/22300 (epoch 41), train_loss = 0.768, time/batch = 0.092\n",
            "18589/22300 (epoch 41), train_loss = 0.785, time/batch = 0.092\n",
            "18590/22300 (epoch 41), train_loss = 0.796, time/batch = 0.092\n",
            "18591/22300 (epoch 41), train_loss = 0.783, time/batch = 0.092\n",
            "18592/22300 (epoch 41), train_loss = 0.763, time/batch = 0.093\n",
            "18593/22300 (epoch 41), train_loss = 0.804, time/batch = 0.093\n",
            "18594/22300 (epoch 41), train_loss = 0.801, time/batch = 0.092\n",
            "18595/22300 (epoch 41), train_loss = 0.786, time/batch = 0.092\n",
            "18596/22300 (epoch 41), train_loss = 0.765, time/batch = 0.092\n",
            "18597/22300 (epoch 41), train_loss = 0.765, time/batch = 0.093\n",
            "18598/22300 (epoch 41), train_loss = 0.772, time/batch = 0.093\n",
            "18599/22300 (epoch 41), train_loss = 0.782, time/batch = 0.092\n",
            "18600/22300 (epoch 41), train_loss = 0.783, time/batch = 0.093\n",
            "18601/22300 (epoch 41), train_loss = 0.793, time/batch = 0.092\n",
            "18602/22300 (epoch 41), train_loss = 0.784, time/batch = 0.097\n",
            "18603/22300 (epoch 41), train_loss = 0.814, time/batch = 0.093\n",
            "18604/22300 (epoch 41), train_loss = 0.776, time/batch = 0.092\n",
            "18605/22300 (epoch 41), train_loss = 0.752, time/batch = 0.092\n",
            "18606/22300 (epoch 41), train_loss = 0.769, time/batch = 0.092\n",
            "18607/22300 (epoch 41), train_loss = 0.816, time/batch = 0.093\n",
            "18608/22300 (epoch 41), train_loss = 0.790, time/batch = 0.093\n",
            "18609/22300 (epoch 41), train_loss = 0.746, time/batch = 0.092\n",
            "18610/22300 (epoch 41), train_loss = 0.755, time/batch = 0.093\n",
            "18611/22300 (epoch 41), train_loss = 0.786, time/batch = 0.091\n",
            "18612/22300 (epoch 41), train_loss = 0.800, time/batch = 0.094\n",
            "18613/22300 (epoch 41), train_loss = 0.789, time/batch = 0.092\n",
            "18614/22300 (epoch 41), train_loss = 0.835, time/batch = 0.092\n",
            "18615/22300 (epoch 41), train_loss = 0.790, time/batch = 0.093\n",
            "18616/22300 (epoch 41), train_loss = 0.760, time/batch = 0.092\n",
            "18617/22300 (epoch 41), train_loss = 0.782, time/batch = 0.094\n",
            "18618/22300 (epoch 41), train_loss = 0.779, time/batch = 0.091\n",
            "18619/22300 (epoch 41), train_loss = 0.742, time/batch = 0.092\n",
            "18620/22300 (epoch 41), train_loss = 0.775, time/batch = 0.093\n",
            "18621/22300 (epoch 41), train_loss = 0.791, time/batch = 0.092\n",
            "18622/22300 (epoch 41), train_loss = 0.796, time/batch = 0.093\n",
            "18623/22300 (epoch 41), train_loss = 0.755, time/batch = 0.092\n",
            "18624/22300 (epoch 41), train_loss = 0.766, time/batch = 0.093\n",
            "18625/22300 (epoch 41), train_loss = 0.780, time/batch = 0.099\n",
            "18626/22300 (epoch 41), train_loss = 0.814, time/batch = 0.089\n",
            "18627/22300 (epoch 41), train_loss = 0.763, time/batch = 0.093\n",
            "18628/22300 (epoch 41), train_loss = 0.771, time/batch = 0.094\n",
            "18629/22300 (epoch 41), train_loss = 0.805, time/batch = 0.093\n",
            "18630/22300 (epoch 41), train_loss = 0.796, time/batch = 0.094\n",
            "18631/22300 (epoch 41), train_loss = 0.805, time/batch = 0.092\n",
            "18632/22300 (epoch 41), train_loss = 0.787, time/batch = 0.094\n",
            "18633/22300 (epoch 41), train_loss = 0.827, time/batch = 0.100\n",
            "18634/22300 (epoch 41), train_loss = 0.753, time/batch = 0.095\n",
            "18635/22300 (epoch 41), train_loss = 0.797, time/batch = 0.094\n",
            "18636/22300 (epoch 41), train_loss = 0.790, time/batch = 0.093\n",
            "18637/22300 (epoch 41), train_loss = 0.778, time/batch = 0.092\n",
            "18638/22300 (epoch 41), train_loss = 0.770, time/batch = 0.092\n",
            "18639/22300 (epoch 41), train_loss = 0.787, time/batch = 0.092\n",
            "18640/22300 (epoch 41), train_loss = 0.813, time/batch = 0.092\n",
            "18641/22300 (epoch 41), train_loss = 0.778, time/batch = 0.093\n",
            "18642/22300 (epoch 41), train_loss = 0.835, time/batch = 0.093\n",
            "18643/22300 (epoch 41), train_loss = 0.818, time/batch = 0.094\n",
            "18644/22300 (epoch 41), train_loss = 0.785, time/batch = 0.100\n",
            "18645/22300 (epoch 41), train_loss = 0.803, time/batch = 0.092\n",
            "18646/22300 (epoch 41), train_loss = 0.798, time/batch = 0.092\n",
            "18647/22300 (epoch 41), train_loss = 0.796, time/batch = 0.091\n",
            "18648/22300 (epoch 41), train_loss = 0.789, time/batch = 0.092\n",
            "18649/22300 (epoch 41), train_loss = 0.794, time/batch = 0.092\n",
            "18650/22300 (epoch 41), train_loss = 0.807, time/batch = 0.092\n",
            "18651/22300 (epoch 41), train_loss = 0.779, time/batch = 0.094\n",
            "18652/22300 (epoch 41), train_loss = 0.766, time/batch = 0.095\n",
            "18653/22300 (epoch 41), train_loss = 0.739, time/batch = 0.093\n",
            "18654/22300 (epoch 41), train_loss = 0.765, time/batch = 0.095\n",
            "18655/22300 (epoch 41), train_loss = 0.772, time/batch = 0.094\n",
            "18656/22300 (epoch 41), train_loss = 0.800, time/batch = 0.092\n",
            "18657/22300 (epoch 41), train_loss = 0.739, time/batch = 0.092\n",
            "18658/22300 (epoch 41), train_loss = 0.803, time/batch = 0.092\n",
            "18659/22300 (epoch 41), train_loss = 0.756, time/batch = 0.092\n",
            "18660/22300 (epoch 41), train_loss = 0.788, time/batch = 0.092\n",
            "18661/22300 (epoch 41), train_loss = 0.774, time/batch = 0.092\n",
            "18662/22300 (epoch 41), train_loss = 0.783, time/batch = 0.091\n",
            "18663/22300 (epoch 41), train_loss = 0.812, time/batch = 0.093\n",
            "18664/22300 (epoch 41), train_loss = 0.770, time/batch = 0.092\n",
            "18665/22300 (epoch 41), train_loss = 0.756, time/batch = 0.093\n",
            "18666/22300 (epoch 41), train_loss = 0.767, time/batch = 0.094\n",
            "18667/22300 (epoch 41), train_loss = 0.778, time/batch = 0.092\n",
            "18668/22300 (epoch 41), train_loss = 0.770, time/batch = 0.100\n",
            "18669/22300 (epoch 41), train_loss = 0.742, time/batch = 0.090\n",
            "18670/22300 (epoch 41), train_loss = 0.770, time/batch = 0.092\n",
            "18671/22300 (epoch 41), train_loss = 0.773, time/batch = 0.092\n",
            "18672/22300 (epoch 41), train_loss = 0.749, time/batch = 0.091\n",
            "18673/22300 (epoch 41), train_loss = 0.779, time/batch = 0.093\n",
            "18674/22300 (epoch 41), train_loss = 0.750, time/batch = 0.092\n",
            "18675/22300 (epoch 41), train_loss = 0.745, time/batch = 0.093\n",
            "18676/22300 (epoch 41), train_loss = 0.752, time/batch = 0.093\n",
            "18677/22300 (epoch 41), train_loss = 0.746, time/batch = 0.092\n",
            "18678/22300 (epoch 41), train_loss = 0.775, time/batch = 0.094\n",
            "18679/22300 (epoch 41), train_loss = 0.813, time/batch = 0.092\n",
            "18680/22300 (epoch 41), train_loss = 0.777, time/batch = 0.092\n",
            "18681/22300 (epoch 41), train_loss = 0.795, time/batch = 0.093\n",
            "18682/22300 (epoch 41), train_loss = 0.779, time/batch = 0.093\n",
            "18683/22300 (epoch 41), train_loss = 0.764, time/batch = 0.093\n",
            "18684/22300 (epoch 41), train_loss = 0.777, time/batch = 0.092\n",
            "18685/22300 (epoch 41), train_loss = 0.744, time/batch = 0.095\n",
            "18686/22300 (epoch 41), train_loss = 0.784, time/batch = 0.093\n",
            "18687/22300 (epoch 41), train_loss = 0.819, time/batch = 0.092\n",
            "18688/22300 (epoch 41), train_loss = 0.809, time/batch = 0.093\n",
            "18689/22300 (epoch 41), train_loss = 0.790, time/batch = 0.093\n",
            "18690/22300 (epoch 41), train_loss = 0.809, time/batch = 0.092\n",
            "18691/22300 (epoch 41), train_loss = 0.818, time/batch = 0.093\n",
            "18692/22300 (epoch 41), train_loss = 0.795, time/batch = 0.092\n",
            "18693/22300 (epoch 41), train_loss = 0.820, time/batch = 0.092\n",
            "18694/22300 (epoch 41), train_loss = 0.820, time/batch = 0.092\n",
            "18695/22300 (epoch 41), train_loss = 0.820, time/batch = 0.093\n",
            "18696/22300 (epoch 41), train_loss = 0.818, time/batch = 0.093\n",
            "18697/22300 (epoch 41), train_loss = 0.780, time/batch = 0.092\n",
            "18698/22300 (epoch 41), train_loss = 0.794, time/batch = 0.093\n",
            "18699/22300 (epoch 41), train_loss = 0.750, time/batch = 0.093\n",
            "18700/22300 (epoch 41), train_loss = 0.788, time/batch = 0.094\n",
            "18701/22300 (epoch 41), train_loss = 0.811, time/batch = 0.092\n",
            "18702/22300 (epoch 41), train_loss = 0.781, time/batch = 0.091\n",
            "18703/22300 (epoch 41), train_loss = 0.791, time/batch = 0.091\n",
            "18704/22300 (epoch 41), train_loss = 0.803, time/batch = 0.093\n",
            "18705/22300 (epoch 41), train_loss = 0.800, time/batch = 0.092\n",
            "18706/22300 (epoch 41), train_loss = 0.805, time/batch = 0.093\n",
            "18707/22300 (epoch 41), train_loss = 0.807, time/batch = 0.094\n",
            "18708/22300 (epoch 41), train_loss = 0.805, time/batch = 0.092\n",
            "18709/22300 (epoch 41), train_loss = 0.805, time/batch = 0.092\n",
            "18710/22300 (epoch 41), train_loss = 0.819, time/batch = 0.100\n",
            "18711/22300 (epoch 41), train_loss = 0.796, time/batch = 0.091\n",
            "18712/22300 (epoch 41), train_loss = 0.855, time/batch = 0.102\n",
            "18713/22300 (epoch 41), train_loss = 0.778, time/batch = 0.092\n",
            "18714/22300 (epoch 41), train_loss = 0.790, time/batch = 0.092\n",
            "18715/22300 (epoch 41), train_loss = 0.767, time/batch = 0.092\n",
            "18716/22300 (epoch 41), train_loss = 0.765, time/batch = 0.093\n",
            "18717/22300 (epoch 41), train_loss = 0.817, time/batch = 0.092\n",
            "18718/22300 (epoch 41), train_loss = 0.796, time/batch = 0.092\n",
            "18719/22300 (epoch 41), train_loss = 0.814, time/batch = 0.092\n",
            "18720/22300 (epoch 41), train_loss = 0.777, time/batch = 0.094\n",
            "18721/22300 (epoch 41), train_loss = 0.775, time/batch = 0.093\n",
            "18722/22300 (epoch 41), train_loss = 0.769, time/batch = 0.093\n",
            "18723/22300 (epoch 41), train_loss = 0.789, time/batch = 0.092\n",
            "18724/22300 (epoch 41), train_loss = 0.874, time/batch = 0.092\n",
            "18725/22300 (epoch 41), train_loss = 0.800, time/batch = 0.092\n",
            "18726/22300 (epoch 41), train_loss = 0.815, time/batch = 0.093\n",
            "18727/22300 (epoch 41), train_loss = 0.793, time/batch = 0.092\n",
            "18728/22300 (epoch 41), train_loss = 0.806, time/batch = 0.092\n",
            "18729/22300 (epoch 41), train_loss = 0.807, time/batch = 0.091\n",
            "18730/22300 (epoch 41), train_loss = 0.777, time/batch = 0.093\n",
            "18731/22300 (epoch 41), train_loss = 0.796, time/batch = 0.092\n",
            "18732/22300 (epoch 42), train_loss = 0.508, time/batch = 0.085\n",
            "18733/22300 (epoch 42), train_loss = 0.789, time/batch = 0.094\n",
            "18734/22300 (epoch 42), train_loss = 0.818, time/batch = 0.091\n",
            "18735/22300 (epoch 42), train_loss = 0.823, time/batch = 0.092\n",
            "18736/22300 (epoch 42), train_loss = 0.858, time/batch = 0.092\n",
            "18737/22300 (epoch 42), train_loss = 0.792, time/batch = 0.091\n",
            "18738/22300 (epoch 42), train_loss = 0.822, time/batch = 0.091\n",
            "18739/22300 (epoch 42), train_loss = 0.822, time/batch = 0.091\n",
            "18740/22300 (epoch 42), train_loss = 0.794, time/batch = 0.091\n",
            "18741/22300 (epoch 42), train_loss = 0.801, time/batch = 0.098\n",
            "18742/22300 (epoch 42), train_loss = 0.854, time/batch = 0.101\n",
            "18743/22300 (epoch 42), train_loss = 0.801, time/batch = 0.093\n",
            "18744/22300 (epoch 42), train_loss = 0.841, time/batch = 0.091\n",
            "18745/22300 (epoch 42), train_loss = 0.848, time/batch = 0.093\n",
            "18746/22300 (epoch 42), train_loss = 0.834, time/batch = 0.093\n",
            "18747/22300 (epoch 42), train_loss = 0.856, time/batch = 0.092\n",
            "18748/22300 (epoch 42), train_loss = 0.856, time/batch = 0.092\n",
            "18749/22300 (epoch 42), train_loss = 0.806, time/batch = 0.091\n",
            "18750/22300 (epoch 42), train_loss = 0.808, time/batch = 0.096\n",
            "18751/22300 (epoch 42), train_loss = 0.843, time/batch = 0.096\n",
            "18752/22300 (epoch 42), train_loss = 0.802, time/batch = 0.094\n",
            "18753/22300 (epoch 42), train_loss = 0.790, time/batch = 0.092\n",
            "18754/22300 (epoch 42), train_loss = 0.777, time/batch = 0.093\n",
            "18755/22300 (epoch 42), train_loss = 0.768, time/batch = 0.092\n",
            "18756/22300 (epoch 42), train_loss = 0.757, time/batch = 0.093\n",
            "18757/22300 (epoch 42), train_loss = 0.790, time/batch = 0.092\n",
            "18758/22300 (epoch 42), train_loss = 0.802, time/batch = 0.094\n",
            "18759/22300 (epoch 42), train_loss = 0.794, time/batch = 0.091\n",
            "18760/22300 (epoch 42), train_loss = 0.814, time/batch = 0.094\n",
            "18761/22300 (epoch 42), train_loss = 0.798, time/batch = 0.093\n",
            "18762/22300 (epoch 42), train_loss = 0.785, time/batch = 0.093\n",
            "18763/22300 (epoch 42), train_loss = 0.797, time/batch = 0.094\n",
            "18764/22300 (epoch 42), train_loss = 0.785, time/batch = 0.093\n",
            "18765/22300 (epoch 42), train_loss = 0.811, time/batch = 0.092\n",
            "18766/22300 (epoch 42), train_loss = 0.806, time/batch = 0.093\n",
            "18767/22300 (epoch 42), train_loss = 0.771, time/batch = 0.094\n",
            "18768/22300 (epoch 42), train_loss = 0.830, time/batch = 0.095\n",
            "18769/22300 (epoch 42), train_loss = 0.795, time/batch = 0.094\n",
            "18770/22300 (epoch 42), train_loss = 0.814, time/batch = 0.092\n",
            "18771/22300 (epoch 42), train_loss = 0.775, time/batch = 0.092\n",
            "18772/22300 (epoch 42), train_loss = 0.792, time/batch = 0.092\n",
            "18773/22300 (epoch 42), train_loss = 0.807, time/batch = 0.097\n",
            "18774/22300 (epoch 42), train_loss = 0.812, time/batch = 0.093\n",
            "18775/22300 (epoch 42), train_loss = 0.800, time/batch = 0.091\n",
            "18776/22300 (epoch 42), train_loss = 0.791, time/batch = 0.093\n",
            "18777/22300 (epoch 42), train_loss = 0.824, time/batch = 0.092\n",
            "18778/22300 (epoch 42), train_loss = 0.785, time/batch = 0.093\n",
            "18779/22300 (epoch 42), train_loss = 0.765, time/batch = 0.093\n",
            "18780/22300 (epoch 42), train_loss = 0.783, time/batch = 0.092\n",
            "18781/22300 (epoch 42), train_loss = 0.780, time/batch = 0.095\n",
            "18782/22300 (epoch 42), train_loss = 0.812, time/batch = 0.096\n",
            "18783/22300 (epoch 42), train_loss = 0.776, time/batch = 0.091\n",
            "18784/22300 (epoch 42), train_loss = 0.791, time/batch = 0.093\n",
            "18785/22300 (epoch 42), train_loss = 0.788, time/batch = 0.092\n",
            "18786/22300 (epoch 42), train_loss = 0.778, time/batch = 0.093\n",
            "18787/22300 (epoch 42), train_loss = 0.781, time/batch = 0.092\n",
            "18788/22300 (epoch 42), train_loss = 0.779, time/batch = 0.092\n",
            "18789/22300 (epoch 42), train_loss = 0.829, time/batch = 0.093\n",
            "18790/22300 (epoch 42), train_loss = 0.787, time/batch = 0.092\n",
            "18791/22300 (epoch 42), train_loss = 0.779, time/batch = 0.092\n",
            "18792/22300 (epoch 42), train_loss = 0.817, time/batch = 0.092\n",
            "18793/22300 (epoch 42), train_loss = 0.774, time/batch = 0.092\n",
            "18794/22300 (epoch 42), train_loss = 0.750, time/batch = 0.092\n",
            "18795/22300 (epoch 42), train_loss = 0.802, time/batch = 0.092\n",
            "18796/22300 (epoch 42), train_loss = 0.787, time/batch = 0.092\n",
            "18797/22300 (epoch 42), train_loss = 0.792, time/batch = 0.098\n",
            "18798/22300 (epoch 42), train_loss = 0.786, time/batch = 0.093\n",
            "18799/22300 (epoch 42), train_loss = 0.830, time/batch = 0.095\n",
            "18800/22300 (epoch 42), train_loss = 0.808, time/batch = 0.093\n",
            "18801/22300 (epoch 42), train_loss = 0.801, time/batch = 0.092\n",
            "18802/22300 (epoch 42), train_loss = 0.791, time/batch = 0.092\n",
            "18803/22300 (epoch 42), train_loss = 0.750, time/batch = 0.093\n",
            "18804/22300 (epoch 42), train_loss = 0.783, time/batch = 0.093\n",
            "18805/22300 (epoch 42), train_loss = 0.761, time/batch = 0.092\n",
            "18806/22300 (epoch 42), train_loss = 0.764, time/batch = 0.095\n",
            "18807/22300 (epoch 42), train_loss = 0.811, time/batch = 0.091\n",
            "18808/22300 (epoch 42), train_loss = 0.804, time/batch = 0.093\n",
            "18809/22300 (epoch 42), train_loss = 0.794, time/batch = 0.093\n",
            "18810/22300 (epoch 42), train_loss = 0.797, time/batch = 0.093\n",
            "18811/22300 (epoch 42), train_loss = 0.821, time/batch = 0.092\n",
            "18812/22300 (epoch 42), train_loss = 0.803, time/batch = 0.091\n",
            "18813/22300 (epoch 42), train_loss = 0.764, time/batch = 0.092\n",
            "18814/22300 (epoch 42), train_loss = 0.781, time/batch = 0.092\n",
            "18815/22300 (epoch 42), train_loss = 0.766, time/batch = 0.092\n",
            "18816/22300 (epoch 42), train_loss = 0.790, time/batch = 0.092\n",
            "18817/22300 (epoch 42), train_loss = 0.791, time/batch = 0.093\n",
            "18818/22300 (epoch 42), train_loss = 0.769, time/batch = 0.093\n",
            "18819/22300 (epoch 42), train_loss = 0.805, time/batch = 0.091\n",
            "18820/22300 (epoch 42), train_loss = 0.771, time/batch = 0.093\n",
            "18821/22300 (epoch 42), train_loss = 0.805, time/batch = 0.092\n",
            "18822/22300 (epoch 42), train_loss = 0.764, time/batch = 0.092\n",
            "18823/22300 (epoch 42), train_loss = 0.771, time/batch = 0.093\n",
            "18824/22300 (epoch 42), train_loss = 0.795, time/batch = 0.092\n",
            "18825/22300 (epoch 42), train_loss = 0.789, time/batch = 0.094\n",
            "18826/22300 (epoch 42), train_loss = 0.794, time/batch = 0.092\n",
            "18827/22300 (epoch 42), train_loss = 0.798, time/batch = 0.093\n",
            "18828/22300 (epoch 42), train_loss = 0.752, time/batch = 0.094\n",
            "18829/22300 (epoch 42), train_loss = 0.797, time/batch = 0.094\n",
            "18830/22300 (epoch 42), train_loss = 0.756, time/batch = 0.093\n",
            "18831/22300 (epoch 42), train_loss = 0.761, time/batch = 0.092\n",
            "18832/22300 (epoch 42), train_loss = 0.775, time/batch = 0.092\n",
            "18833/22300 (epoch 42), train_loss = 0.763, time/batch = 0.094\n",
            "18834/22300 (epoch 42), train_loss = 0.785, time/batch = 0.089\n",
            "18835/22300 (epoch 42), train_loss = 0.753, time/batch = 0.093\n",
            "18836/22300 (epoch 42), train_loss = 0.783, time/batch = 0.091\n",
            "18837/22300 (epoch 42), train_loss = 0.740, time/batch = 0.092\n",
            "18838/22300 (epoch 42), train_loss = 0.737, time/batch = 0.093\n",
            "18839/22300 (epoch 42), train_loss = 0.755, time/batch = 0.092\n",
            "18840/22300 (epoch 42), train_loss = 0.758, time/batch = 0.093\n",
            "18841/22300 (epoch 42), train_loss = 0.751, time/batch = 0.092\n",
            "18842/22300 (epoch 42), train_loss = 0.731, time/batch = 0.092\n",
            "18843/22300 (epoch 42), train_loss = 0.726, time/batch = 0.093\n",
            "18844/22300 (epoch 42), train_loss = 0.772, time/batch = 0.092\n",
            "18845/22300 (epoch 42), train_loss = 0.740, time/batch = 0.093\n",
            "18846/22300 (epoch 42), train_loss = 0.771, time/batch = 0.091\n",
            "18847/22300 (epoch 42), train_loss = 0.758, time/batch = 0.092\n",
            "18848/22300 (epoch 42), train_loss = 0.758, time/batch = 0.092\n",
            "18849/22300 (epoch 42), train_loss = 0.788, time/batch = 0.093\n",
            "18850/22300 (epoch 42), train_loss = 0.806, time/batch = 0.092\n",
            "18851/22300 (epoch 42), train_loss = 0.778, time/batch = 0.092\n",
            "18852/22300 (epoch 42), train_loss = 0.814, time/batch = 0.093\n",
            "18853/22300 (epoch 42), train_loss = 0.794, time/batch = 0.093\n",
            "18854/22300 (epoch 42), train_loss = 0.767, time/batch = 0.093\n",
            "18855/22300 (epoch 42), train_loss = 0.775, time/batch = 0.092\n",
            "18856/22300 (epoch 42), train_loss = 0.804, time/batch = 0.092\n",
            "18857/22300 (epoch 42), train_loss = 0.801, time/batch = 0.091\n",
            "18858/22300 (epoch 42), train_loss = 0.781, time/batch = 0.092\n",
            "18859/22300 (epoch 42), train_loss = 0.802, time/batch = 0.093\n",
            "18860/22300 (epoch 42), train_loss = 0.800, time/batch = 0.094\n",
            "18861/22300 (epoch 42), train_loss = 0.783, time/batch = 0.092\n",
            "18862/22300 (epoch 42), train_loss = 0.763, time/batch = 0.092\n",
            "18863/22300 (epoch 42), train_loss = 0.793, time/batch = 0.092\n",
            "18864/22300 (epoch 42), train_loss = 0.798, time/batch = 0.092\n",
            "18865/22300 (epoch 42), train_loss = 0.801, time/batch = 0.093\n",
            "18866/22300 (epoch 42), train_loss = 0.774, time/batch = 0.092\n",
            "18867/22300 (epoch 42), train_loss = 0.810, time/batch = 0.092\n",
            "18868/22300 (epoch 42), train_loss = 0.825, time/batch = 0.092\n",
            "18869/22300 (epoch 42), train_loss = 0.820, time/batch = 0.092\n",
            "18870/22300 (epoch 42), train_loss = 0.819, time/batch = 0.092\n",
            "18871/22300 (epoch 42), train_loss = 0.831, time/batch = 0.092\n",
            "18872/22300 (epoch 42), train_loss = 0.802, time/batch = 0.092\n",
            "18873/22300 (epoch 42), train_loss = 0.812, time/batch = 0.093\n",
            "18874/22300 (epoch 42), train_loss = 0.789, time/batch = 0.092\n",
            "18875/22300 (epoch 42), train_loss = 0.796, time/batch = 0.096\n",
            "18876/22300 (epoch 42), train_loss = 0.796, time/batch = 0.093\n",
            "18877/22300 (epoch 42), train_loss = 0.856, time/batch = 0.092\n",
            "18878/22300 (epoch 42), train_loss = 0.797, time/batch = 0.092\n",
            "18879/22300 (epoch 42), train_loss = 0.788, time/batch = 0.093\n",
            "18880/22300 (epoch 42), train_loss = 0.803, time/batch = 0.092\n",
            "18881/22300 (epoch 42), train_loss = 0.806, time/batch = 0.092\n",
            "18882/22300 (epoch 42), train_loss = 0.827, time/batch = 0.092\n",
            "18883/22300 (epoch 42), train_loss = 0.817, time/batch = 0.096\n",
            "18884/22300 (epoch 42), train_loss = 0.806, time/batch = 0.091\n",
            "18885/22300 (epoch 42), train_loss = 0.783, time/batch = 0.092\n",
            "18886/22300 (epoch 42), train_loss = 0.809, time/batch = 0.092\n",
            "18887/22300 (epoch 42), train_loss = 0.800, time/batch = 0.095\n",
            "18888/22300 (epoch 42), train_loss = 0.788, time/batch = 0.093\n",
            "18889/22300 (epoch 42), train_loss = 0.778, time/batch = 0.091\n",
            "18890/22300 (epoch 42), train_loss = 0.824, time/batch = 0.092\n",
            "18891/22300 (epoch 42), train_loss = 0.796, time/batch = 0.092\n",
            "18892/22300 (epoch 42), train_loss = 0.782, time/batch = 0.094\n",
            "18893/22300 (epoch 42), train_loss = 0.802, time/batch = 0.093\n",
            "18894/22300 (epoch 42), train_loss = 0.801, time/batch = 0.092\n",
            "18895/22300 (epoch 42), train_loss = 0.789, time/batch = 0.093\n",
            "18896/22300 (epoch 42), train_loss = 0.805, time/batch = 0.093\n",
            "18897/22300 (epoch 42), train_loss = 0.808, time/batch = 0.093\n",
            "18898/22300 (epoch 42), train_loss = 0.779, time/batch = 0.093\n",
            "18899/22300 (epoch 42), train_loss = 0.773, time/batch = 0.092\n",
            "18900/22300 (epoch 42), train_loss = 0.761, time/batch = 0.094\n",
            "18901/22300 (epoch 42), train_loss = 0.789, time/batch = 0.092\n",
            "18902/22300 (epoch 42), train_loss = 0.780, time/batch = 0.092\n",
            "18903/22300 (epoch 42), train_loss = 0.771, time/batch = 0.093\n",
            "18904/22300 (epoch 42), train_loss = 0.767, time/batch = 0.093\n",
            "18905/22300 (epoch 42), train_loss = 0.754, time/batch = 0.093\n",
            "18906/22300 (epoch 42), train_loss = 0.758, time/batch = 0.091\n",
            "18907/22300 (epoch 42), train_loss = 0.750, time/batch = 0.092\n",
            "18908/22300 (epoch 42), train_loss = 0.721, time/batch = 0.092\n",
            "18909/22300 (epoch 42), train_loss = 0.760, time/batch = 0.096\n",
            "18910/22300 (epoch 42), train_loss = 0.705, time/batch = 0.092\n",
            "18911/22300 (epoch 42), train_loss = 0.784, time/batch = 0.092\n",
            "18912/22300 (epoch 42), train_loss = 0.768, time/batch = 0.098\n",
            "18913/22300 (epoch 42), train_loss = 0.751, time/batch = 0.093\n",
            "18914/22300 (epoch 42), train_loss = 0.793, time/batch = 0.092\n",
            "18915/22300 (epoch 42), train_loss = 0.757, time/batch = 0.094\n",
            "18916/22300 (epoch 42), train_loss = 0.741, time/batch = 0.092\n",
            "18917/22300 (epoch 42), train_loss = 0.738, time/batch = 0.092\n",
            "18918/22300 (epoch 42), train_loss = 0.793, time/batch = 0.092\n",
            "18919/22300 (epoch 42), train_loss = 0.757, time/batch = 0.093\n",
            "18920/22300 (epoch 42), train_loss = 0.801, time/batch = 0.094\n",
            "18921/22300 (epoch 42), train_loss = 0.774, time/batch = 0.093\n",
            "18922/22300 (epoch 42), train_loss = 0.799, time/batch = 0.093\n",
            "18923/22300 (epoch 42), train_loss = 0.796, time/batch = 0.092\n",
            "18924/22300 (epoch 42), train_loss = 0.791, time/batch = 0.099\n",
            "18925/22300 (epoch 42), train_loss = 0.807, time/batch = 0.093\n",
            "18926/22300 (epoch 42), train_loss = 0.805, time/batch = 0.091\n",
            "18927/22300 (epoch 42), train_loss = 0.773, time/batch = 0.093\n",
            "18928/22300 (epoch 42), train_loss = 0.775, time/batch = 0.093\n",
            "18929/22300 (epoch 42), train_loss = 0.772, time/batch = 0.093\n",
            "18930/22300 (epoch 42), train_loss = 0.802, time/batch = 0.093\n",
            "18931/22300 (epoch 42), train_loss = 0.770, time/batch = 0.092\n",
            "18932/22300 (epoch 42), train_loss = 0.811, time/batch = 0.093\n",
            "18933/22300 (epoch 42), train_loss = 0.779, time/batch = 0.094\n",
            "18934/22300 (epoch 42), train_loss = 0.761, time/batch = 0.092\n",
            "18935/22300 (epoch 42), train_loss = 0.755, time/batch = 0.093\n",
            "18936/22300 (epoch 42), train_loss = 0.769, time/batch = 0.091\n",
            "18937/22300 (epoch 42), train_loss = 0.771, time/batch = 0.091\n",
            "18938/22300 (epoch 42), train_loss = 0.832, time/batch = 0.092\n",
            "18939/22300 (epoch 42), train_loss = 0.775, time/batch = 0.093\n",
            "18940/22300 (epoch 42), train_loss = 0.808, time/batch = 0.093\n",
            "18941/22300 (epoch 42), train_loss = 0.767, time/batch = 0.092\n",
            "18942/22300 (epoch 42), train_loss = 0.756, time/batch = 0.094\n",
            "18943/22300 (epoch 42), train_loss = 0.751, time/batch = 0.092\n",
            "18944/22300 (epoch 42), train_loss = 0.770, time/batch = 0.093\n",
            "18945/22300 (epoch 42), train_loss = 0.766, time/batch = 0.093\n",
            "18946/22300 (epoch 42), train_loss = 0.746, time/batch = 0.091\n",
            "18947/22300 (epoch 42), train_loss = 0.758, time/batch = 0.092\n",
            "18948/22300 (epoch 42), train_loss = 0.761, time/batch = 0.092\n",
            "18949/22300 (epoch 42), train_loss = 0.775, time/batch = 0.093\n",
            "18950/22300 (epoch 42), train_loss = 0.782, time/batch = 0.092\n",
            "18951/22300 (epoch 42), train_loss = 0.774, time/batch = 0.092\n",
            "18952/22300 (epoch 42), train_loss = 0.804, time/batch = 0.093\n",
            "18953/22300 (epoch 42), train_loss = 0.778, time/batch = 0.093\n",
            "18954/22300 (epoch 42), train_loss = 0.778, time/batch = 0.094\n",
            "18955/22300 (epoch 42), train_loss = 0.806, time/batch = 0.091\n",
            "18956/22300 (epoch 42), train_loss = 0.756, time/batch = 0.093\n",
            "18957/22300 (epoch 42), train_loss = 0.773, time/batch = 0.092\n",
            "18958/22300 (epoch 42), train_loss = 0.807, time/batch = 0.092\n",
            "18959/22300 (epoch 42), train_loss = 0.788, time/batch = 0.093\n",
            "18960/22300 (epoch 42), train_loss = 0.802, time/batch = 0.092\n",
            "18961/22300 (epoch 42), train_loss = 0.795, time/batch = 0.092\n",
            "18962/22300 (epoch 42), train_loss = 0.797, time/batch = 0.093\n",
            "18963/22300 (epoch 42), train_loss = 0.791, time/batch = 0.094\n",
            "18964/22300 (epoch 42), train_loss = 0.770, time/batch = 0.093\n",
            "18965/22300 (epoch 42), train_loss = 0.783, time/batch = 0.092\n",
            "18966/22300 (epoch 42), train_loss = 0.808, time/batch = 0.093\n",
            "18967/22300 (epoch 42), train_loss = 0.779, time/batch = 0.092\n",
            "18968/22300 (epoch 42), train_loss = 0.786, time/batch = 0.093\n",
            "18969/22300 (epoch 42), train_loss = 0.752, time/batch = 0.091\n",
            "18970/22300 (epoch 42), train_loss = 0.790, time/batch = 0.092\n",
            "18971/22300 (epoch 42), train_loss = 0.782, time/batch = 0.093\n",
            "18972/22300 (epoch 42), train_loss = 0.822, time/batch = 0.093\n",
            "18973/22300 (epoch 42), train_loss = 0.784, time/batch = 0.092\n",
            "18974/22300 (epoch 42), train_loss = 0.795, time/batch = 0.093\n",
            "18975/22300 (epoch 42), train_loss = 0.808, time/batch = 0.092\n",
            "18976/22300 (epoch 42), train_loss = 0.758, time/batch = 0.092\n",
            "18977/22300 (epoch 42), train_loss = 0.796, time/batch = 0.092\n",
            "18978/22300 (epoch 42), train_loss = 0.797, time/batch = 0.093\n",
            "18979/22300 (epoch 42), train_loss = 0.725, time/batch = 0.097\n",
            "18980/22300 (epoch 42), train_loss = 0.795, time/batch = 0.093\n",
            "18981/22300 (epoch 42), train_loss = 0.789, time/batch = 0.093\n",
            "18982/22300 (epoch 42), train_loss = 0.790, time/batch = 0.092\n",
            "18983/22300 (epoch 42), train_loss = 0.789, time/batch = 0.093\n",
            "18984/22300 (epoch 42), train_loss = 0.813, time/batch = 0.092\n",
            "18985/22300 (epoch 42), train_loss = 0.806, time/batch = 0.092\n",
            "18986/22300 (epoch 42), train_loss = 0.775, time/batch = 0.092\n",
            "18987/22300 (epoch 42), train_loss = 0.800, time/batch = 0.092\n",
            "18988/22300 (epoch 42), train_loss = 0.767, time/batch = 0.093\n",
            "18989/22300 (epoch 42), train_loss = 0.812, time/batch = 0.093\n",
            "18990/22300 (epoch 42), train_loss = 0.768, time/batch = 0.099\n",
            "18991/22300 (epoch 42), train_loss = 0.808, time/batch = 0.092\n",
            "18992/22300 (epoch 42), train_loss = 0.792, time/batch = 0.091\n",
            "18993/22300 (epoch 42), train_loss = 0.852, time/batch = 0.093\n",
            "18994/22300 (epoch 42), train_loss = 0.815, time/batch = 0.091\n",
            "18995/22300 (epoch 42), train_loss = 0.852, time/batch = 0.092\n",
            "18996/22300 (epoch 42), train_loss = 0.804, time/batch = 0.092\n",
            "18997/22300 (epoch 42), train_loss = 0.834, time/batch = 0.092\n",
            "18998/22300 (epoch 42), train_loss = 0.812, time/batch = 0.092\n",
            "18999/22300 (epoch 42), train_loss = 0.818, time/batch = 0.092\n",
            "19000/22300 (epoch 42), train_loss = 0.789, time/batch = 0.098\n",
            "model saved to save/model.ckpt\n",
            "19001/22300 (epoch 42), train_loss = 0.791, time/batch = 0.085\n",
            "19002/22300 (epoch 42), train_loss = 0.805, time/batch = 0.093\n",
            "19003/22300 (epoch 42), train_loss = 0.798, time/batch = 0.092\n",
            "19004/22300 (epoch 42), train_loss = 0.820, time/batch = 0.091\n",
            "19005/22300 (epoch 42), train_loss = 0.829, time/batch = 0.092\n",
            "19006/22300 (epoch 42), train_loss = 0.822, time/batch = 0.096\n",
            "19007/22300 (epoch 42), train_loss = 0.817, time/batch = 0.091\n",
            "19008/22300 (epoch 42), train_loss = 0.852, time/batch = 0.094\n",
            "19009/22300 (epoch 42), train_loss = 0.816, time/batch = 0.091\n",
            "19010/22300 (epoch 42), train_loss = 0.798, time/batch = 0.101\n",
            "19011/22300 (epoch 42), train_loss = 0.803, time/batch = 0.093\n",
            "19012/22300 (epoch 42), train_loss = 0.803, time/batch = 0.091\n",
            "19013/22300 (epoch 42), train_loss = 0.841, time/batch = 0.091\n",
            "19014/22300 (epoch 42), train_loss = 0.798, time/batch = 0.092\n",
            "19015/22300 (epoch 42), train_loss = 0.782, time/batch = 0.093\n",
            "19016/22300 (epoch 42), train_loss = 0.784, time/batch = 0.091\n",
            "19017/22300 (epoch 42), train_loss = 0.776, time/batch = 0.091\n",
            "19018/22300 (epoch 42), train_loss = 0.814, time/batch = 0.091\n",
            "19019/22300 (epoch 42), train_loss = 0.814, time/batch = 0.091\n",
            "19020/22300 (epoch 42), train_loss = 0.809, time/batch = 0.095\n",
            "19021/22300 (epoch 42), train_loss = 0.803, time/batch = 0.093\n",
            "19022/22300 (epoch 42), train_loss = 0.787, time/batch = 0.093\n",
            "19023/22300 (epoch 42), train_loss = 0.780, time/batch = 0.092\n",
            "19024/22300 (epoch 42), train_loss = 0.819, time/batch = 0.092\n",
            "19025/22300 (epoch 42), train_loss = 0.806, time/batch = 0.092\n",
            "19026/22300 (epoch 42), train_loss = 0.809, time/batch = 0.091\n",
            "19027/22300 (epoch 42), train_loss = 0.804, time/batch = 0.094\n",
            "19028/22300 (epoch 42), train_loss = 0.797, time/batch = 0.090\n",
            "19029/22300 (epoch 42), train_loss = 0.794, time/batch = 0.092\n",
            "19030/22300 (epoch 42), train_loss = 0.789, time/batch = 0.091\n",
            "19031/22300 (epoch 42), train_loss = 0.810, time/batch = 0.093\n",
            "19032/22300 (epoch 42), train_loss = 0.766, time/batch = 0.093\n",
            "19033/22300 (epoch 42), train_loss = 0.793, time/batch = 0.103\n",
            "19034/22300 (epoch 42), train_loss = 0.760, time/batch = 0.091\n",
            "19035/22300 (epoch 42), train_loss = 0.770, time/batch = 0.092\n",
            "19036/22300 (epoch 42), train_loss = 0.797, time/batch = 0.090\n",
            "19037/22300 (epoch 42), train_loss = 0.780, time/batch = 0.092\n",
            "19038/22300 (epoch 42), train_loss = 0.766, time/batch = 0.091\n",
            "19039/22300 (epoch 42), train_loss = 0.817, time/batch = 0.092\n",
            "19040/22300 (epoch 42), train_loss = 0.803, time/batch = 0.092\n",
            "19041/22300 (epoch 42), train_loss = 0.788, time/batch = 0.092\n",
            "19042/22300 (epoch 42), train_loss = 0.772, time/batch = 0.093\n",
            "19043/22300 (epoch 42), train_loss = 0.761, time/batch = 0.093\n",
            "19044/22300 (epoch 42), train_loss = 0.767, time/batch = 0.092\n",
            "19045/22300 (epoch 42), train_loss = 0.775, time/batch = 0.092\n",
            "19046/22300 (epoch 42), train_loss = 0.773, time/batch = 0.092\n",
            "19047/22300 (epoch 42), train_loss = 0.784, time/batch = 0.092\n",
            "19048/22300 (epoch 42), train_loss = 0.771, time/batch = 0.092\n",
            "19049/22300 (epoch 42), train_loss = 0.799, time/batch = 0.092\n",
            "19050/22300 (epoch 42), train_loss = 0.771, time/batch = 0.091\n",
            "19051/22300 (epoch 42), train_loss = 0.750, time/batch = 0.092\n",
            "19052/22300 (epoch 42), train_loss = 0.765, time/batch = 0.092\n",
            "19053/22300 (epoch 42), train_loss = 0.809, time/batch = 0.092\n",
            "19054/22300 (epoch 42), train_loss = 0.789, time/batch = 0.092\n",
            "19055/22300 (epoch 42), train_loss = 0.746, time/batch = 0.092\n",
            "19056/22300 (epoch 42), train_loss = 0.749, time/batch = 0.091\n",
            "19057/22300 (epoch 42), train_loss = 0.785, time/batch = 0.092\n",
            "19058/22300 (epoch 42), train_loss = 0.795, time/batch = 0.092\n",
            "19059/22300 (epoch 42), train_loss = 0.782, time/batch = 0.092\n",
            "19060/22300 (epoch 42), train_loss = 0.815, time/batch = 0.093\n",
            "19061/22300 (epoch 42), train_loss = 0.776, time/batch = 0.095\n",
            "19062/22300 (epoch 42), train_loss = 0.750, time/batch = 0.092\n",
            "19063/22300 (epoch 42), train_loss = 0.780, time/batch = 0.092\n",
            "19064/22300 (epoch 42), train_loss = 0.767, time/batch = 0.092\n",
            "19065/22300 (epoch 42), train_loss = 0.727, time/batch = 0.093\n",
            "19066/22300 (epoch 42), train_loss = 0.770, time/batch = 0.091\n",
            "19067/22300 (epoch 42), train_loss = 0.782, time/batch = 0.092\n",
            "19068/22300 (epoch 42), train_loss = 0.789, time/batch = 0.091\n",
            "19069/22300 (epoch 42), train_loss = 0.744, time/batch = 0.092\n",
            "19070/22300 (epoch 42), train_loss = 0.759, time/batch = 0.092\n",
            "19071/22300 (epoch 42), train_loss = 0.768, time/batch = 0.092\n",
            "19072/22300 (epoch 42), train_loss = 0.795, time/batch = 0.092\n",
            "19073/22300 (epoch 42), train_loss = 0.749, time/batch = 0.093\n",
            "19074/22300 (epoch 42), train_loss = 0.753, time/batch = 0.093\n",
            "19075/22300 (epoch 42), train_loss = 0.781, time/batch = 0.091\n",
            "19076/22300 (epoch 42), train_loss = 0.782, time/batch = 0.092\n",
            "19077/22300 (epoch 42), train_loss = 0.792, time/batch = 0.091\n",
            "19078/22300 (epoch 42), train_loss = 0.775, time/batch = 0.093\n",
            "19079/22300 (epoch 42), train_loss = 0.823, time/batch = 0.094\n",
            "19080/22300 (epoch 42), train_loss = 0.748, time/batch = 0.092\n",
            "19081/22300 (epoch 42), train_loss = 0.789, time/batch = 0.093\n",
            "19082/22300 (epoch 42), train_loss = 0.787, time/batch = 0.091\n",
            "19083/22300 (epoch 42), train_loss = 0.768, time/batch = 0.092\n",
            "19084/22300 (epoch 42), train_loss = 0.770, time/batch = 0.092\n",
            "19085/22300 (epoch 42), train_loss = 0.767, time/batch = 0.092\n",
            "19086/22300 (epoch 42), train_loss = 0.785, time/batch = 0.091\n",
            "19087/22300 (epoch 42), train_loss = 0.756, time/batch = 0.095\n",
            "19088/22300 (epoch 42), train_loss = 0.812, time/batch = 0.099\n",
            "19089/22300 (epoch 42), train_loss = 0.800, time/batch = 0.095\n",
            "19090/22300 (epoch 42), train_loss = 0.776, time/batch = 0.092\n",
            "19091/22300 (epoch 42), train_loss = 0.791, time/batch = 0.092\n",
            "19092/22300 (epoch 42), train_loss = 0.785, time/batch = 0.091\n",
            "19093/22300 (epoch 42), train_loss = 0.791, time/batch = 0.092\n",
            "19094/22300 (epoch 42), train_loss = 0.784, time/batch = 0.092\n",
            "19095/22300 (epoch 42), train_loss = 0.784, time/batch = 0.092\n",
            "19096/22300 (epoch 42), train_loss = 0.802, time/batch = 0.093\n",
            "19097/22300 (epoch 42), train_loss = 0.782, time/batch = 0.093\n",
            "19098/22300 (epoch 42), train_loss = 0.766, time/batch = 0.093\n",
            "19099/22300 (epoch 42), train_loss = 0.737, time/batch = 0.092\n",
            "19100/22300 (epoch 42), train_loss = 0.762, time/batch = 0.091\n",
            "19101/22300 (epoch 42), train_loss = 0.771, time/batch = 0.091\n",
            "19102/22300 (epoch 42), train_loss = 0.792, time/batch = 0.094\n",
            "19103/22300 (epoch 42), train_loss = 0.737, time/batch = 0.098\n",
            "19104/22300 (epoch 42), train_loss = 0.796, time/batch = 0.095\n",
            "19105/22300 (epoch 42), train_loss = 0.742, time/batch = 0.100\n",
            "19106/22300 (epoch 42), train_loss = 0.776, time/batch = 0.092\n",
            "19107/22300 (epoch 42), train_loss = 0.763, time/batch = 0.094\n",
            "19108/22300 (epoch 42), train_loss = 0.764, time/batch = 0.095\n",
            "19109/22300 (epoch 42), train_loss = 0.798, time/batch = 0.093\n",
            "19110/22300 (epoch 42), train_loss = 0.751, time/batch = 0.095\n",
            "19111/22300 (epoch 42), train_loss = 0.752, time/batch = 0.095\n",
            "19112/22300 (epoch 42), train_loss = 0.761, time/batch = 0.093\n",
            "19113/22300 (epoch 42), train_loss = 0.777, time/batch = 0.092\n",
            "19114/22300 (epoch 42), train_loss = 0.777, time/batch = 0.096\n",
            "19115/22300 (epoch 42), train_loss = 0.734, time/batch = 0.094\n",
            "19116/22300 (epoch 42), train_loss = 0.762, time/batch = 0.092\n",
            "19117/22300 (epoch 42), train_loss = 0.770, time/batch = 0.093\n",
            "19118/22300 (epoch 42), train_loss = 0.747, time/batch = 0.093\n",
            "19119/22300 (epoch 42), train_loss = 0.764, time/batch = 0.092\n",
            "19120/22300 (epoch 42), train_loss = 0.742, time/batch = 0.094\n",
            "19121/22300 (epoch 42), train_loss = 0.736, time/batch = 0.094\n",
            "19122/22300 (epoch 42), train_loss = 0.748, time/batch = 0.092\n",
            "19123/22300 (epoch 42), train_loss = 0.737, time/batch = 0.092\n",
            "19124/22300 (epoch 42), train_loss = 0.758, time/batch = 0.092\n",
            "19125/22300 (epoch 42), train_loss = 0.794, time/batch = 0.093\n",
            "19126/22300 (epoch 42), train_loss = 0.758, time/batch = 0.091\n",
            "19127/22300 (epoch 42), train_loss = 0.792, time/batch = 0.092\n",
            "19128/22300 (epoch 42), train_loss = 0.776, time/batch = 0.092\n",
            "19129/22300 (epoch 42), train_loss = 0.763, time/batch = 0.092\n",
            "19130/22300 (epoch 42), train_loss = 0.762, time/batch = 0.093\n",
            "19131/22300 (epoch 42), train_loss = 0.735, time/batch = 0.092\n",
            "19132/22300 (epoch 42), train_loss = 0.763, time/batch = 0.093\n",
            "19133/22300 (epoch 42), train_loss = 0.806, time/batch = 0.092\n",
            "19134/22300 (epoch 42), train_loss = 0.786, time/batch = 0.091\n",
            "19135/22300 (epoch 42), train_loss = 0.768, time/batch = 0.092\n",
            "19136/22300 (epoch 42), train_loss = 0.787, time/batch = 0.092\n",
            "19137/22300 (epoch 42), train_loss = 0.807, time/batch = 0.092\n",
            "19138/22300 (epoch 42), train_loss = 0.788, time/batch = 0.092\n",
            "19139/22300 (epoch 42), train_loss = 0.813, time/batch = 0.092\n",
            "19140/22300 (epoch 42), train_loss = 0.805, time/batch = 0.092\n",
            "19141/22300 (epoch 42), train_loss = 0.814, time/batch = 0.091\n",
            "19142/22300 (epoch 42), train_loss = 0.821, time/batch = 0.092\n",
            "19143/22300 (epoch 42), train_loss = 0.771, time/batch = 0.094\n",
            "19144/22300 (epoch 42), train_loss = 0.795, time/batch = 0.101\n",
            "19145/22300 (epoch 42), train_loss = 0.743, time/batch = 0.094\n",
            "19146/22300 (epoch 42), train_loss = 0.783, time/batch = 0.092\n",
            "19147/22300 (epoch 42), train_loss = 0.801, time/batch = 0.095\n",
            "19148/22300 (epoch 42), train_loss = 0.770, time/batch = 0.093\n",
            "19149/22300 (epoch 42), train_loss = 0.773, time/batch = 0.092\n",
            "19150/22300 (epoch 42), train_loss = 0.781, time/batch = 0.093\n",
            "19151/22300 (epoch 42), train_loss = 0.781, time/batch = 0.092\n",
            "19152/22300 (epoch 42), train_loss = 0.797, time/batch = 0.092\n",
            "19153/22300 (epoch 42), train_loss = 0.793, time/batch = 0.093\n",
            "19154/22300 (epoch 42), train_loss = 0.798, time/batch = 0.092\n",
            "19155/22300 (epoch 42), train_loss = 0.798, time/batch = 0.093\n",
            "19156/22300 (epoch 42), train_loss = 0.808, time/batch = 0.092\n",
            "19157/22300 (epoch 42), train_loss = 0.778, time/batch = 0.092\n",
            "19158/22300 (epoch 42), train_loss = 0.837, time/batch = 0.092\n",
            "19159/22300 (epoch 42), train_loss = 0.780, time/batch = 0.092\n",
            "19160/22300 (epoch 42), train_loss = 0.780, time/batch = 0.093\n",
            "19161/22300 (epoch 42), train_loss = 0.770, time/batch = 0.092\n",
            "19162/22300 (epoch 42), train_loss = 0.762, time/batch = 0.092\n",
            "19163/22300 (epoch 42), train_loss = 0.808, time/batch = 0.092\n",
            "19164/22300 (epoch 42), train_loss = 0.789, time/batch = 0.093\n",
            "19165/22300 (epoch 42), train_loss = 0.799, time/batch = 0.092\n",
            "19166/22300 (epoch 42), train_loss = 0.758, time/batch = 0.092\n",
            "19167/22300 (epoch 42), train_loss = 0.750, time/batch = 0.094\n",
            "19168/22300 (epoch 42), train_loss = 0.754, time/batch = 0.098\n",
            "19169/22300 (epoch 42), train_loss = 0.764, time/batch = 0.092\n",
            "19170/22300 (epoch 42), train_loss = 0.860, time/batch = 0.092\n",
            "19171/22300 (epoch 42), train_loss = 0.789, time/batch = 0.091\n",
            "19172/22300 (epoch 42), train_loss = 0.798, time/batch = 0.093\n",
            "19173/22300 (epoch 42), train_loss = 0.783, time/batch = 0.091\n",
            "19174/22300 (epoch 42), train_loss = 0.794, time/batch = 0.092\n",
            "19175/22300 (epoch 42), train_loss = 0.789, time/batch = 0.093\n",
            "19176/22300 (epoch 42), train_loss = 0.778, time/batch = 0.092\n",
            "19177/22300 (epoch 42), train_loss = 0.782, time/batch = 0.096\n",
            "19178/22300 (epoch 43), train_loss = 0.474, time/batch = 0.085\n",
            "19179/22300 (epoch 43), train_loss = 0.780, time/batch = 0.094\n",
            "19180/22300 (epoch 43), train_loss = 0.804, time/batch = 0.092\n",
            "19181/22300 (epoch 43), train_loss = 0.804, time/batch = 0.100\n",
            "19182/22300 (epoch 43), train_loss = 0.831, time/batch = 0.091\n",
            "19183/22300 (epoch 43), train_loss = 0.775, time/batch = 0.092\n",
            "19184/22300 (epoch 43), train_loss = 0.810, time/batch = 0.091\n",
            "19185/22300 (epoch 43), train_loss = 0.816, time/batch = 0.092\n",
            "19186/22300 (epoch 43), train_loss = 0.782, time/batch = 0.092\n",
            "19187/22300 (epoch 43), train_loss = 0.778, time/batch = 0.099\n",
            "19188/22300 (epoch 43), train_loss = 0.839, time/batch = 0.093\n",
            "19189/22300 (epoch 43), train_loss = 0.791, time/batch = 0.097\n",
            "19190/22300 (epoch 43), train_loss = 0.828, time/batch = 0.091\n",
            "19191/22300 (epoch 43), train_loss = 0.834, time/batch = 0.093\n",
            "19192/22300 (epoch 43), train_loss = 0.813, time/batch = 0.091\n",
            "19193/22300 (epoch 43), train_loss = 0.847, time/batch = 0.091\n",
            "19194/22300 (epoch 43), train_loss = 0.852, time/batch = 0.091\n",
            "19195/22300 (epoch 43), train_loss = 0.799, time/batch = 0.092\n",
            "19196/22300 (epoch 43), train_loss = 0.803, time/batch = 0.091\n",
            "19197/22300 (epoch 43), train_loss = 0.842, time/batch = 0.093\n",
            "19198/22300 (epoch 43), train_loss = 0.815, time/batch = 0.093\n",
            "19199/22300 (epoch 43), train_loss = 0.782, time/batch = 0.093\n",
            "19200/22300 (epoch 43), train_loss = 0.775, time/batch = 0.093\n",
            "19201/22300 (epoch 43), train_loss = 0.760, time/batch = 0.091\n",
            "19202/22300 (epoch 43), train_loss = 0.745, time/batch = 0.092\n",
            "19203/22300 (epoch 43), train_loss = 0.786, time/batch = 0.091\n",
            "19204/22300 (epoch 43), train_loss = 0.789, time/batch = 0.092\n",
            "19205/22300 (epoch 43), train_loss = 0.770, time/batch = 0.091\n",
            "19206/22300 (epoch 43), train_loss = 0.802, time/batch = 0.092\n",
            "19207/22300 (epoch 43), train_loss = 0.775, time/batch = 0.093\n",
            "19208/22300 (epoch 43), train_loss = 0.787, time/batch = 0.093\n",
            "19209/22300 (epoch 43), train_loss = 0.799, time/batch = 0.093\n",
            "19210/22300 (epoch 43), train_loss = 0.791, time/batch = 0.092\n",
            "19211/22300 (epoch 43), train_loss = 0.810, time/batch = 0.094\n",
            "19212/22300 (epoch 43), train_loss = 0.790, time/batch = 0.093\n",
            "19213/22300 (epoch 43), train_loss = 0.760, time/batch = 0.095\n",
            "19214/22300 (epoch 43), train_loss = 0.807, time/batch = 0.091\n",
            "19215/22300 (epoch 43), train_loss = 0.770, time/batch = 0.096\n",
            "19216/22300 (epoch 43), train_loss = 0.788, time/batch = 0.092\n",
            "19217/22300 (epoch 43), train_loss = 0.765, time/batch = 0.093\n",
            "19218/22300 (epoch 43), train_loss = 0.780, time/batch = 0.092\n",
            "19219/22300 (epoch 43), train_loss = 0.795, time/batch = 0.093\n",
            "19220/22300 (epoch 43), train_loss = 0.804, time/batch = 0.093\n",
            "19221/22300 (epoch 43), train_loss = 0.799, time/batch = 0.092\n",
            "19222/22300 (epoch 43), train_loss = 0.777, time/batch = 0.093\n",
            "19223/22300 (epoch 43), train_loss = 0.815, time/batch = 0.092\n",
            "19224/22300 (epoch 43), train_loss = 0.778, time/batch = 0.093\n",
            "19225/22300 (epoch 43), train_loss = 0.762, time/batch = 0.093\n",
            "19226/22300 (epoch 43), train_loss = 0.783, time/batch = 0.092\n",
            "19227/22300 (epoch 43), train_loss = 0.775, time/batch = 0.093\n",
            "19228/22300 (epoch 43), train_loss = 0.790, time/batch = 0.093\n",
            "19229/22300 (epoch 43), train_loss = 0.756, time/batch = 0.092\n",
            "19230/22300 (epoch 43), train_loss = 0.771, time/batch = 0.093\n",
            "19231/22300 (epoch 43), train_loss = 0.770, time/batch = 0.092\n",
            "19232/22300 (epoch 43), train_loss = 0.763, time/batch = 0.096\n",
            "19233/22300 (epoch 43), train_loss = 0.762, time/batch = 0.094\n",
            "19234/22300 (epoch 43), train_loss = 0.764, time/batch = 0.102\n",
            "19235/22300 (epoch 43), train_loss = 0.823, time/batch = 0.092\n",
            "19236/22300 (epoch 43), train_loss = 0.781, time/batch = 0.097\n",
            "19237/22300 (epoch 43), train_loss = 0.770, time/batch = 0.095\n",
            "19238/22300 (epoch 43), train_loss = 0.808, time/batch = 0.092\n",
            "19239/22300 (epoch 43), train_loss = 0.772, time/batch = 0.092\n",
            "19240/22300 (epoch 43), train_loss = 0.748, time/batch = 0.092\n",
            "19241/22300 (epoch 43), train_loss = 0.797, time/batch = 0.092\n",
            "19242/22300 (epoch 43), train_loss = 0.783, time/batch = 0.095\n",
            "19243/22300 (epoch 43), train_loss = 0.787, time/batch = 0.091\n",
            "19244/22300 (epoch 43), train_loss = 0.777, time/batch = 0.094\n",
            "19245/22300 (epoch 43), train_loss = 0.822, time/batch = 0.091\n",
            "19246/22300 (epoch 43), train_loss = 0.796, time/batch = 0.092\n",
            "19247/22300 (epoch 43), train_loss = 0.797, time/batch = 0.093\n",
            "19248/22300 (epoch 43), train_loss = 0.786, time/batch = 0.091\n",
            "19249/22300 (epoch 43), train_loss = 0.753, time/batch = 0.092\n",
            "19250/22300 (epoch 43), train_loss = 0.773, time/batch = 0.092\n",
            "19251/22300 (epoch 43), train_loss = 0.754, time/batch = 0.092\n",
            "19252/22300 (epoch 43), train_loss = 0.755, time/batch = 0.092\n",
            "19253/22300 (epoch 43), train_loss = 0.800, time/batch = 0.092\n",
            "19254/22300 (epoch 43), train_loss = 0.802, time/batch = 0.092\n",
            "19255/22300 (epoch 43), train_loss = 0.777, time/batch = 0.092\n",
            "19256/22300 (epoch 43), train_loss = 0.783, time/batch = 0.093\n",
            "19257/22300 (epoch 43), train_loss = 0.823, time/batch = 0.092\n",
            "19258/22300 (epoch 43), train_loss = 0.796, time/batch = 0.093\n",
            "19259/22300 (epoch 43), train_loss = 0.767, time/batch = 0.092\n",
            "19260/22300 (epoch 43), train_loss = 0.785, time/batch = 0.092\n",
            "19261/22300 (epoch 43), train_loss = 0.773, time/batch = 0.091\n",
            "19262/22300 (epoch 43), train_loss = 0.793, time/batch = 0.093\n",
            "19263/22300 (epoch 43), train_loss = 0.786, time/batch = 0.092\n",
            "19264/22300 (epoch 43), train_loss = 0.765, time/batch = 0.093\n",
            "19265/22300 (epoch 43), train_loss = 0.800, time/batch = 0.092\n",
            "19266/22300 (epoch 43), train_loss = 0.768, time/batch = 0.092\n",
            "19267/22300 (epoch 43), train_loss = 0.806, time/batch = 0.097\n",
            "19268/22300 (epoch 43), train_loss = 0.754, time/batch = 0.095\n",
            "19269/22300 (epoch 43), train_loss = 0.770, time/batch = 0.092\n",
            "19270/22300 (epoch 43), train_loss = 0.784, time/batch = 0.092\n",
            "19271/22300 (epoch 43), train_loss = 0.776, time/batch = 0.091\n",
            "19272/22300 (epoch 43), train_loss = 0.791, time/batch = 0.092\n",
            "19273/22300 (epoch 43), train_loss = 0.798, time/batch = 0.092\n",
            "19274/22300 (epoch 43), train_loss = 0.745, time/batch = 0.092\n",
            "19275/22300 (epoch 43), train_loss = 0.797, time/batch = 0.092\n",
            "19276/22300 (epoch 43), train_loss = 0.758, time/batch = 0.092\n",
            "19277/22300 (epoch 43), train_loss = 0.757, time/batch = 0.094\n",
            "19278/22300 (epoch 43), train_loss = 0.757, time/batch = 0.092\n",
            "19279/22300 (epoch 43), train_loss = 0.757, time/batch = 0.091\n",
            "19280/22300 (epoch 43), train_loss = 0.769, time/batch = 0.092\n",
            "19281/22300 (epoch 43), train_loss = 0.740, time/batch = 0.092\n",
            "19282/22300 (epoch 43), train_loss = 0.773, time/batch = 0.092\n",
            "19283/22300 (epoch 43), train_loss = 0.739, time/batch = 0.092\n",
            "19284/22300 (epoch 43), train_loss = 0.732, time/batch = 0.093\n",
            "19285/22300 (epoch 43), train_loss = 0.745, time/batch = 0.092\n",
            "19286/22300 (epoch 43), train_loss = 0.749, time/batch = 0.092\n",
            "19287/22300 (epoch 43), train_loss = 0.740, time/batch = 0.096\n",
            "19288/22300 (epoch 43), train_loss = 0.716, time/batch = 0.091\n",
            "19289/22300 (epoch 43), train_loss = 0.713, time/batch = 0.093\n",
            "19290/22300 (epoch 43), train_loss = 0.756, time/batch = 0.092\n",
            "19291/22300 (epoch 43), train_loss = 0.728, time/batch = 0.092\n",
            "19292/22300 (epoch 43), train_loss = 0.765, time/batch = 0.092\n",
            "19293/22300 (epoch 43), train_loss = 0.740, time/batch = 0.095\n",
            "19294/22300 (epoch 43), train_loss = 0.753, time/batch = 0.091\n",
            "19295/22300 (epoch 43), train_loss = 0.775, time/batch = 0.092\n",
            "19296/22300 (epoch 43), train_loss = 0.805, time/batch = 0.092\n",
            "19297/22300 (epoch 43), train_loss = 0.755, time/batch = 0.092\n",
            "19298/22300 (epoch 43), train_loss = 0.791, time/batch = 0.093\n",
            "19299/22300 (epoch 43), train_loss = 0.776, time/batch = 0.092\n",
            "19300/22300 (epoch 43), train_loss = 0.756, time/batch = 0.093\n",
            "19301/22300 (epoch 43), train_loss = 0.762, time/batch = 0.091\n",
            "19302/22300 (epoch 43), train_loss = 0.786, time/batch = 0.092\n",
            "19303/22300 (epoch 43), train_loss = 0.790, time/batch = 0.092\n",
            "19304/22300 (epoch 43), train_loss = 0.769, time/batch = 0.092\n",
            "19305/22300 (epoch 43), train_loss = 0.793, time/batch = 0.093\n",
            "19306/22300 (epoch 43), train_loss = 0.788, time/batch = 0.092\n",
            "19307/22300 (epoch 43), train_loss = 0.770, time/batch = 0.092\n",
            "19308/22300 (epoch 43), train_loss = 0.757, time/batch = 0.092\n",
            "19309/22300 (epoch 43), train_loss = 0.772, time/batch = 0.093\n",
            "19310/22300 (epoch 43), train_loss = 0.778, time/batch = 0.091\n",
            "19311/22300 (epoch 43), train_loss = 0.783, time/batch = 0.092\n",
            "19312/22300 (epoch 43), train_loss = 0.757, time/batch = 0.091\n",
            "19313/22300 (epoch 43), train_loss = 0.788, time/batch = 0.092\n",
            "19314/22300 (epoch 43), train_loss = 0.810, time/batch = 0.092\n",
            "19315/22300 (epoch 43), train_loss = 0.795, time/batch = 0.092\n",
            "19316/22300 (epoch 43), train_loss = 0.795, time/batch = 0.092\n",
            "19317/22300 (epoch 43), train_loss = 0.813, time/batch = 0.098\n",
            "19318/22300 (epoch 43), train_loss = 0.777, time/batch = 0.093\n",
            "19319/22300 (epoch 43), train_loss = 0.791, time/batch = 0.094\n",
            "19320/22300 (epoch 43), train_loss = 0.768, time/batch = 0.102\n",
            "19321/22300 (epoch 43), train_loss = 0.789, time/batch = 0.098\n",
            "19322/22300 (epoch 43), train_loss = 0.784, time/batch = 0.101\n",
            "19323/22300 (epoch 43), train_loss = 0.853, time/batch = 0.091\n",
            "19324/22300 (epoch 43), train_loss = 0.794, time/batch = 0.097\n",
            "19325/22300 (epoch 43), train_loss = 0.773, time/batch = 0.092\n",
            "19326/22300 (epoch 43), train_loss = 0.796, time/batch = 0.092\n",
            "19327/22300 (epoch 43), train_loss = 0.786, time/batch = 0.092\n",
            "19328/22300 (epoch 43), train_loss = 0.800, time/batch = 0.092\n",
            "19329/22300 (epoch 43), train_loss = 0.803, time/batch = 0.093\n",
            "19330/22300 (epoch 43), train_loss = 0.797, time/batch = 0.093\n",
            "19331/22300 (epoch 43), train_loss = 0.768, time/batch = 0.094\n",
            "19332/22300 (epoch 43), train_loss = 0.801, time/batch = 0.092\n",
            "19333/22300 (epoch 43), train_loss = 0.789, time/batch = 0.092\n",
            "19334/22300 (epoch 43), train_loss = 0.790, time/batch = 0.093\n",
            "19335/22300 (epoch 43), train_loss = 0.773, time/batch = 0.091\n",
            "19336/22300 (epoch 43), train_loss = 0.818, time/batch = 0.091\n",
            "19337/22300 (epoch 43), train_loss = 0.785, time/batch = 0.091\n",
            "19338/22300 (epoch 43), train_loss = 0.771, time/batch = 0.092\n",
            "19339/22300 (epoch 43), train_loss = 0.797, time/batch = 0.091\n",
            "19340/22300 (epoch 43), train_loss = 0.784, time/batch = 0.092\n",
            "19341/22300 (epoch 43), train_loss = 0.776, time/batch = 0.093\n",
            "19342/22300 (epoch 43), train_loss = 0.797, time/batch = 0.092\n",
            "19343/22300 (epoch 43), train_loss = 0.804, time/batch = 0.092\n",
            "19344/22300 (epoch 43), train_loss = 0.778, time/batch = 0.092\n",
            "19345/22300 (epoch 43), train_loss = 0.784, time/batch = 0.092\n",
            "19346/22300 (epoch 43), train_loss = 0.781, time/batch = 0.091\n",
            "19347/22300 (epoch 43), train_loss = 0.808, time/batch = 0.092\n",
            "19348/22300 (epoch 43), train_loss = 0.787, time/batch = 0.096\n",
            "19349/22300 (epoch 43), train_loss = 0.770, time/batch = 0.092\n",
            "19350/22300 (epoch 43), train_loss = 0.756, time/batch = 0.102\n",
            "19351/22300 (epoch 43), train_loss = 0.751, time/batch = 0.093\n",
            "19352/22300 (epoch 43), train_loss = 0.757, time/batch = 0.093\n",
            "19353/22300 (epoch 43), train_loss = 0.748, time/batch = 0.100\n",
            "19354/22300 (epoch 43), train_loss = 0.717, time/batch = 0.092\n",
            "19355/22300 (epoch 43), train_loss = 0.760, time/batch = 0.093\n",
            "19356/22300 (epoch 43), train_loss = 0.705, time/batch = 0.093\n",
            "19357/22300 (epoch 43), train_loss = 0.776, time/batch = 0.092\n",
            "19358/22300 (epoch 43), train_loss = 0.754, time/batch = 0.092\n",
            "19359/22300 (epoch 43), train_loss = 0.735, time/batch = 0.092\n",
            "19360/22300 (epoch 43), train_loss = 0.781, time/batch = 0.092\n",
            "19361/22300 (epoch 43), train_loss = 0.756, time/batch = 0.092\n",
            "19362/22300 (epoch 43), train_loss = 0.742, time/batch = 0.093\n",
            "19363/22300 (epoch 43), train_loss = 0.735, time/batch = 0.093\n",
            "19364/22300 (epoch 43), train_loss = 0.800, time/batch = 0.093\n",
            "19365/22300 (epoch 43), train_loss = 0.755, time/batch = 0.091\n",
            "19366/22300 (epoch 43), train_loss = 0.789, time/batch = 0.092\n",
            "19367/22300 (epoch 43), train_loss = 0.754, time/batch = 0.093\n",
            "19368/22300 (epoch 43), train_loss = 0.778, time/batch = 0.092\n",
            "19369/22300 (epoch 43), train_loss = 0.784, time/batch = 0.093\n",
            "19370/22300 (epoch 43), train_loss = 0.772, time/batch = 0.092\n",
            "19371/22300 (epoch 43), train_loss = 0.791, time/batch = 0.092\n",
            "19372/22300 (epoch 43), train_loss = 0.795, time/batch = 0.094\n",
            "19373/22300 (epoch 43), train_loss = 0.767, time/batch = 0.092\n",
            "19374/22300 (epoch 43), train_loss = 0.766, time/batch = 0.092\n",
            "19375/22300 (epoch 43), train_loss = 0.761, time/batch = 0.091\n",
            "19376/22300 (epoch 43), train_loss = 0.796, time/batch = 0.092\n",
            "19377/22300 (epoch 43), train_loss = 0.770, time/batch = 0.091\n",
            "19378/22300 (epoch 43), train_loss = 0.810, time/batch = 0.092\n",
            "19379/22300 (epoch 43), train_loss = 0.771, time/batch = 0.092\n",
            "19380/22300 (epoch 43), train_loss = 0.755, time/batch = 0.092\n",
            "19381/22300 (epoch 43), train_loss = 0.747, time/batch = 0.092\n",
            "19382/22300 (epoch 43), train_loss = 0.747, time/batch = 0.092\n",
            "19383/22300 (epoch 43), train_loss = 0.759, time/batch = 0.092\n",
            "19384/22300 (epoch 43), train_loss = 0.813, time/batch = 0.092\n",
            "19385/22300 (epoch 43), train_loss = 0.759, time/batch = 0.092\n",
            "19386/22300 (epoch 43), train_loss = 0.804, time/batch = 0.091\n",
            "19387/22300 (epoch 43), train_loss = 0.756, time/batch = 0.092\n",
            "19388/22300 (epoch 43), train_loss = 0.758, time/batch = 0.092\n",
            "19389/22300 (epoch 43), train_loss = 0.743, time/batch = 0.092\n",
            "19390/22300 (epoch 43), train_loss = 0.754, time/batch = 0.092\n",
            "19391/22300 (epoch 43), train_loss = 0.760, time/batch = 0.092\n",
            "19392/22300 (epoch 43), train_loss = 0.735, time/batch = 0.092\n",
            "19393/22300 (epoch 43), train_loss = 0.746, time/batch = 0.092\n",
            "19394/22300 (epoch 43), train_loss = 0.753, time/batch = 0.093\n",
            "19395/22300 (epoch 43), train_loss = 0.766, time/batch = 0.092\n",
            "19396/22300 (epoch 43), train_loss = 0.776, time/batch = 0.091\n",
            "19397/22300 (epoch 43), train_loss = 0.770, time/batch = 0.091\n",
            "19398/22300 (epoch 43), train_loss = 0.790, time/batch = 0.092\n",
            "19399/22300 (epoch 43), train_loss = 0.766, time/batch = 0.092\n",
            "19400/22300 (epoch 43), train_loss = 0.774, time/batch = 0.092\n",
            "19401/22300 (epoch 43), train_loss = 0.808, time/batch = 0.091\n",
            "19402/22300 (epoch 43), train_loss = 0.768, time/batch = 0.092\n",
            "19403/22300 (epoch 43), train_loss = 0.786, time/batch = 0.093\n",
            "19404/22300 (epoch 43), train_loss = 0.807, time/batch = 0.093\n",
            "19405/22300 (epoch 43), train_loss = 0.784, time/batch = 0.092\n",
            "19406/22300 (epoch 43), train_loss = 0.787, time/batch = 0.091\n",
            "19407/22300 (epoch 43), train_loss = 0.780, time/batch = 0.092\n",
            "19408/22300 (epoch 43), train_loss = 0.781, time/batch = 0.092\n",
            "19409/22300 (epoch 43), train_loss = 0.783, time/batch = 0.093\n",
            "19410/22300 (epoch 43), train_loss = 0.757, time/batch = 0.092\n",
            "19411/22300 (epoch 43), train_loss = 0.777, time/batch = 0.093\n",
            "19412/22300 (epoch 43), train_loss = 0.798, time/batch = 0.093\n",
            "19413/22300 (epoch 43), train_loss = 0.773, time/batch = 0.093\n",
            "19414/22300 (epoch 43), train_loss = 0.780, time/batch = 0.092\n",
            "19415/22300 (epoch 43), train_loss = 0.754, time/batch = 0.092\n",
            "19416/22300 (epoch 43), train_loss = 0.780, time/batch = 0.091\n",
            "19417/22300 (epoch 43), train_loss = 0.775, time/batch = 0.092\n",
            "19418/22300 (epoch 43), train_loss = 0.806, time/batch = 0.092\n",
            "19419/22300 (epoch 43), train_loss = 0.782, time/batch = 0.091\n",
            "19420/22300 (epoch 43), train_loss = 0.794, time/batch = 0.095\n",
            "19421/22300 (epoch 43), train_loss = 0.819, time/batch = 0.092\n",
            "19422/22300 (epoch 43), train_loss = 0.754, time/batch = 0.091\n",
            "19423/22300 (epoch 43), train_loss = 0.795, time/batch = 0.094\n",
            "19424/22300 (epoch 43), train_loss = 0.793, time/batch = 0.091\n",
            "19425/22300 (epoch 43), train_loss = 0.725, time/batch = 0.092\n",
            "19426/22300 (epoch 43), train_loss = 0.765, time/batch = 0.092\n",
            "19427/22300 (epoch 43), train_loss = 0.761, time/batch = 0.092\n",
            "19428/22300 (epoch 43), train_loss = 0.760, time/batch = 0.093\n",
            "19429/22300 (epoch 43), train_loss = 0.761, time/batch = 0.091\n",
            "19430/22300 (epoch 43), train_loss = 0.791, time/batch = 0.093\n",
            "19431/22300 (epoch 43), train_loss = 0.781, time/batch = 0.092\n",
            "19432/22300 (epoch 43), train_loss = 0.768, time/batch = 0.092\n",
            "19433/22300 (epoch 43), train_loss = 0.790, time/batch = 0.092\n",
            "19434/22300 (epoch 43), train_loss = 0.767, time/batch = 0.091\n",
            "19435/22300 (epoch 43), train_loss = 0.803, time/batch = 0.093\n",
            "19436/22300 (epoch 43), train_loss = 0.761, time/batch = 0.092\n",
            "19437/22300 (epoch 43), train_loss = 0.781, time/batch = 0.092\n",
            "19438/22300 (epoch 43), train_loss = 0.748, time/batch = 0.092\n",
            "19439/22300 (epoch 43), train_loss = 0.814, time/batch = 0.093\n",
            "19440/22300 (epoch 43), train_loss = 0.768, time/batch = 0.092\n",
            "19441/22300 (epoch 43), train_loss = 0.821, time/batch = 0.091\n",
            "19442/22300 (epoch 43), train_loss = 0.783, time/batch = 0.091\n",
            "19443/22300 (epoch 43), train_loss = 0.805, time/batch = 0.092\n",
            "19444/22300 (epoch 43), train_loss = 0.799, time/batch = 0.092\n",
            "19445/22300 (epoch 43), train_loss = 0.802, time/batch = 0.095\n",
            "19446/22300 (epoch 43), train_loss = 0.785, time/batch = 0.092\n",
            "19447/22300 (epoch 43), train_loss = 0.768, time/batch = 0.092\n",
            "19448/22300 (epoch 43), train_loss = 0.799, time/batch = 0.094\n",
            "19449/22300 (epoch 43), train_loss = 0.787, time/batch = 0.092\n",
            "19450/22300 (epoch 43), train_loss = 0.811, time/batch = 0.093\n",
            "19451/22300 (epoch 43), train_loss = 0.808, time/batch = 0.093\n",
            "19452/22300 (epoch 43), train_loss = 0.812, time/batch = 0.092\n",
            "19453/22300 (epoch 43), train_loss = 0.815, time/batch = 0.092\n",
            "19454/22300 (epoch 43), train_loss = 0.837, time/batch = 0.092\n",
            "19455/22300 (epoch 43), train_loss = 0.795, time/batch = 0.092\n",
            "19456/22300 (epoch 43), train_loss = 0.777, time/batch = 0.092\n",
            "19457/22300 (epoch 43), train_loss = 0.792, time/batch = 0.093\n",
            "19458/22300 (epoch 43), train_loss = 0.788, time/batch = 0.093\n",
            "19459/22300 (epoch 43), train_loss = 0.831, time/batch = 0.092\n",
            "19460/22300 (epoch 43), train_loss = 0.789, time/batch = 0.092\n",
            "19461/22300 (epoch 43), train_loss = 0.779, time/batch = 0.092\n",
            "19462/22300 (epoch 43), train_loss = 0.770, time/batch = 0.092\n",
            "19463/22300 (epoch 43), train_loss = 0.769, time/batch = 0.092\n",
            "19464/22300 (epoch 43), train_loss = 0.798, time/batch = 0.092\n",
            "19465/22300 (epoch 43), train_loss = 0.797, time/batch = 0.093\n",
            "19466/22300 (epoch 43), train_loss = 0.798, time/batch = 0.092\n",
            "19467/22300 (epoch 43), train_loss = 0.804, time/batch = 0.093\n",
            "19468/22300 (epoch 43), train_loss = 0.787, time/batch = 0.092\n",
            "19469/22300 (epoch 43), train_loss = 0.777, time/batch = 0.092\n",
            "19470/22300 (epoch 43), train_loss = 0.808, time/batch = 0.096\n",
            "19471/22300 (epoch 43), train_loss = 0.787, time/batch = 0.092\n",
            "19472/22300 (epoch 43), train_loss = 0.791, time/batch = 0.093\n",
            "19473/22300 (epoch 43), train_loss = 0.788, time/batch = 0.092\n",
            "19474/22300 (epoch 43), train_loss = 0.789, time/batch = 0.092\n",
            "19475/22300 (epoch 43), train_loss = 0.796, time/batch = 0.091\n",
            "19476/22300 (epoch 43), train_loss = 0.795, time/batch = 0.092\n",
            "19477/22300 (epoch 43), train_loss = 0.819, time/batch = 0.093\n",
            "19478/22300 (epoch 43), train_loss = 0.768, time/batch = 0.092\n",
            "19479/22300 (epoch 43), train_loss = 0.801, time/batch = 0.093\n",
            "19480/22300 (epoch 43), train_loss = 0.755, time/batch = 0.092\n",
            "19481/22300 (epoch 43), train_loss = 0.767, time/batch = 0.093\n",
            "19482/22300 (epoch 43), train_loss = 0.791, time/batch = 0.091\n",
            "19483/22300 (epoch 43), train_loss = 0.770, time/batch = 0.093\n",
            "19484/22300 (epoch 43), train_loss = 0.754, time/batch = 0.092\n",
            "19485/22300 (epoch 43), train_loss = 0.816, time/batch = 0.091\n",
            "19486/22300 (epoch 43), train_loss = 0.808, time/batch = 0.097\n",
            "19487/22300 (epoch 43), train_loss = 0.796, time/batch = 0.091\n",
            "19488/22300 (epoch 43), train_loss = 0.763, time/batch = 0.092\n",
            "19489/22300 (epoch 43), train_loss = 0.755, time/batch = 0.093\n",
            "19490/22300 (epoch 43), train_loss = 0.756, time/batch = 0.092\n",
            "19491/22300 (epoch 43), train_loss = 0.768, time/batch = 0.093\n",
            "19492/22300 (epoch 43), train_loss = 0.756, time/batch = 0.091\n",
            "19493/22300 (epoch 43), train_loss = 0.763, time/batch = 0.092\n",
            "19494/22300 (epoch 43), train_loss = 0.761, time/batch = 0.092\n",
            "19495/22300 (epoch 43), train_loss = 0.787, time/batch = 0.094\n",
            "19496/22300 (epoch 43), train_loss = 0.762, time/batch = 0.092\n",
            "19497/22300 (epoch 43), train_loss = 0.739, time/batch = 0.090\n",
            "19498/22300 (epoch 43), train_loss = 0.749, time/batch = 0.092\n",
            "19499/22300 (epoch 43), train_loss = 0.800, time/batch = 0.092\n",
            "19500/22300 (epoch 43), train_loss = 0.782, time/batch = 0.093\n",
            "19501/22300 (epoch 43), train_loss = 0.742, time/batch = 0.092\n",
            "19502/22300 (epoch 43), train_loss = 0.754, time/batch = 0.092\n",
            "19503/22300 (epoch 43), train_loss = 0.784, time/batch = 0.093\n",
            "19504/22300 (epoch 43), train_loss = 0.788, time/batch = 0.092\n",
            "19505/22300 (epoch 43), train_loss = 0.778, time/batch = 0.092\n",
            "19506/22300 (epoch 43), train_loss = 0.816, time/batch = 0.092\n",
            "19507/22300 (epoch 43), train_loss = 0.770, time/batch = 0.092\n",
            "19508/22300 (epoch 43), train_loss = 0.740, time/batch = 0.091\n",
            "19509/22300 (epoch 43), train_loss = 0.766, time/batch = 0.093\n",
            "19510/22300 (epoch 43), train_loss = 0.761, time/batch = 0.093\n",
            "19511/22300 (epoch 43), train_loss = 0.727, time/batch = 0.092\n",
            "19512/22300 (epoch 43), train_loss = 0.772, time/batch = 0.093\n",
            "19513/22300 (epoch 43), train_loss = 0.774, time/batch = 0.092\n",
            "19514/22300 (epoch 43), train_loss = 0.782, time/batch = 0.093\n",
            "19515/22300 (epoch 43), train_loss = 0.748, time/batch = 0.091\n",
            "19516/22300 (epoch 43), train_loss = 0.760, time/batch = 0.092\n",
            "19517/22300 (epoch 43), train_loss = 0.762, time/batch = 0.092\n",
            "19518/22300 (epoch 43), train_loss = 0.781, time/batch = 0.092\n",
            "19519/22300 (epoch 43), train_loss = 0.737, time/batch = 0.093\n",
            "19520/22300 (epoch 43), train_loss = 0.740, time/batch = 0.092\n",
            "19521/22300 (epoch 43), train_loss = 0.766, time/batch = 0.093\n",
            "19522/22300 (epoch 43), train_loss = 0.768, time/batch = 0.092\n",
            "19523/22300 (epoch 43), train_loss = 0.777, time/batch = 0.092\n",
            "19524/22300 (epoch 43), train_loss = 0.766, time/batch = 0.092\n",
            "19525/22300 (epoch 43), train_loss = 0.812, time/batch = 0.092\n",
            "19526/22300 (epoch 43), train_loss = 0.751, time/batch = 0.093\n",
            "19527/22300 (epoch 43), train_loss = 0.784, time/batch = 0.091\n",
            "19528/22300 (epoch 43), train_loss = 0.772, time/batch = 0.092\n",
            "19529/22300 (epoch 43), train_loss = 0.767, time/batch = 0.091\n",
            "19530/22300 (epoch 43), train_loss = 0.760, time/batch = 0.093\n",
            "19531/22300 (epoch 43), train_loss = 0.764, time/batch = 0.092\n",
            "19532/22300 (epoch 43), train_loss = 0.778, time/batch = 0.101\n",
            "19533/22300 (epoch 43), train_loss = 0.743, time/batch = 0.090\n",
            "19534/22300 (epoch 43), train_loss = 0.792, time/batch = 0.091\n",
            "19535/22300 (epoch 43), train_loss = 0.792, time/batch = 0.093\n",
            "19536/22300 (epoch 43), train_loss = 0.759, time/batch = 0.092\n",
            "19537/22300 (epoch 43), train_loss = 0.771, time/batch = 0.093\n",
            "19538/22300 (epoch 43), train_loss = 0.774, time/batch = 0.092\n",
            "19539/22300 (epoch 43), train_loss = 0.776, time/batch = 0.092\n",
            "19540/22300 (epoch 43), train_loss = 0.770, time/batch = 0.093\n",
            "19541/22300 (epoch 43), train_loss = 0.773, time/batch = 0.092\n",
            "19542/22300 (epoch 43), train_loss = 0.793, time/batch = 0.093\n",
            "19543/22300 (epoch 43), train_loss = 0.765, time/batch = 0.092\n",
            "19544/22300 (epoch 43), train_loss = 0.760, time/batch = 0.091\n",
            "19545/22300 (epoch 43), train_loss = 0.730, time/batch = 0.093\n",
            "19546/22300 (epoch 43), train_loss = 0.756, time/batch = 0.092\n",
            "19547/22300 (epoch 43), train_loss = 0.769, time/batch = 0.094\n",
            "19548/22300 (epoch 43), train_loss = 0.792, time/batch = 0.092\n",
            "19549/22300 (epoch 43), train_loss = 0.732, time/batch = 0.092\n",
            "19550/22300 (epoch 43), train_loss = 0.790, time/batch = 0.104\n",
            "19551/22300 (epoch 43), train_loss = 0.745, time/batch = 0.091\n",
            "19552/22300 (epoch 43), train_loss = 0.773, time/batch = 0.092\n",
            "19553/22300 (epoch 43), train_loss = 0.754, time/batch = 0.093\n",
            "19554/22300 (epoch 43), train_loss = 0.744, time/batch = 0.092\n",
            "19555/22300 (epoch 43), train_loss = 0.783, time/batch = 0.093\n",
            "19556/22300 (epoch 43), train_loss = 0.732, time/batch = 0.091\n",
            "19557/22300 (epoch 43), train_loss = 0.733, time/batch = 0.093\n",
            "19558/22300 (epoch 43), train_loss = 0.749, time/batch = 0.092\n",
            "19559/22300 (epoch 43), train_loss = 0.759, time/batch = 0.092\n",
            "19560/22300 (epoch 43), train_loss = 0.755, time/batch = 0.094\n",
            "19561/22300 (epoch 43), train_loss = 0.717, time/batch = 0.091\n",
            "19562/22300 (epoch 43), train_loss = 0.755, time/batch = 0.092\n",
            "19563/22300 (epoch 43), train_loss = 0.770, time/batch = 0.092\n",
            "19564/22300 (epoch 43), train_loss = 0.759, time/batch = 0.093\n",
            "19565/22300 (epoch 43), train_loss = 0.773, time/batch = 0.093\n",
            "19566/22300 (epoch 43), train_loss = 0.731, time/batch = 0.093\n",
            "19567/22300 (epoch 43), train_loss = 0.737, time/batch = 0.093\n",
            "19568/22300 (epoch 43), train_loss = 0.735, time/batch = 0.093\n",
            "19569/22300 (epoch 43), train_loss = 0.723, time/batch = 0.093\n",
            "19570/22300 (epoch 43), train_loss = 0.753, time/batch = 0.092\n",
            "19571/22300 (epoch 43), train_loss = 0.787, time/batch = 0.092\n",
            "19572/22300 (epoch 43), train_loss = 0.746, time/batch = 0.101\n",
            "19573/22300 (epoch 43), train_loss = 0.773, time/batch = 0.092\n",
            "19574/22300 (epoch 43), train_loss = 0.774, time/batch = 0.092\n",
            "19575/22300 (epoch 43), train_loss = 0.748, time/batch = 0.093\n",
            "19576/22300 (epoch 43), train_loss = 0.756, time/batch = 0.092\n",
            "19577/22300 (epoch 43), train_loss = 0.725, time/batch = 0.094\n",
            "19578/22300 (epoch 43), train_loss = 0.751, time/batch = 0.092\n",
            "19579/22300 (epoch 43), train_loss = 0.785, time/batch = 0.092\n",
            "19580/22300 (epoch 43), train_loss = 0.769, time/batch = 0.096\n",
            "19581/22300 (epoch 43), train_loss = 0.750, time/batch = 0.092\n",
            "19582/22300 (epoch 43), train_loss = 0.760, time/batch = 0.093\n",
            "19583/22300 (epoch 43), train_loss = 0.794, time/batch = 0.098\n",
            "19584/22300 (epoch 43), train_loss = 0.786, time/batch = 0.092\n",
            "19585/22300 (epoch 43), train_loss = 0.817, time/batch = 0.092\n",
            "19586/22300 (epoch 43), train_loss = 0.787, time/batch = 0.092\n",
            "19587/22300 (epoch 43), train_loss = 0.804, time/batch = 0.093\n",
            "19588/22300 (epoch 43), train_loss = 0.805, time/batch = 0.091\n",
            "19589/22300 (epoch 43), train_loss = 0.765, time/batch = 0.092\n",
            "19590/22300 (epoch 43), train_loss = 0.792, time/batch = 0.093\n",
            "19591/22300 (epoch 43), train_loss = 0.745, time/batch = 0.092\n",
            "19592/22300 (epoch 43), train_loss = 0.786, time/batch = 0.094\n",
            "19593/22300 (epoch 43), train_loss = 0.802, time/batch = 0.092\n",
            "19594/22300 (epoch 43), train_loss = 0.772, time/batch = 0.096\n",
            "19595/22300 (epoch 43), train_loss = 0.771, time/batch = 0.093\n",
            "19596/22300 (epoch 43), train_loss = 0.777, time/batch = 0.092\n",
            "19597/22300 (epoch 43), train_loss = 0.765, time/batch = 0.093\n",
            "19598/22300 (epoch 43), train_loss = 0.760, time/batch = 0.092\n",
            "19599/22300 (epoch 43), train_loss = 0.769, time/batch = 0.091\n",
            "19600/22300 (epoch 43), train_loss = 0.784, time/batch = 0.092\n",
            "19601/22300 (epoch 43), train_loss = 0.788, time/batch = 0.092\n",
            "19602/22300 (epoch 43), train_loss = 0.800, time/batch = 0.094\n",
            "19603/22300 (epoch 43), train_loss = 0.782, time/batch = 0.093\n",
            "19604/22300 (epoch 43), train_loss = 0.827, time/batch = 0.094\n",
            "19605/22300 (epoch 43), train_loss = 0.773, time/batch = 0.092\n",
            "19606/22300 (epoch 43), train_loss = 0.782, time/batch = 0.092\n",
            "19607/22300 (epoch 43), train_loss = 0.763, time/batch = 0.093\n",
            "19608/22300 (epoch 43), train_loss = 0.759, time/batch = 0.091\n",
            "19609/22300 (epoch 43), train_loss = 0.810, time/batch = 0.093\n",
            "19610/22300 (epoch 43), train_loss = 0.772, time/batch = 0.092\n",
            "19611/22300 (epoch 43), train_loss = 0.785, time/batch = 0.091\n",
            "19612/22300 (epoch 43), train_loss = 0.747, time/batch = 0.093\n",
            "19613/22300 (epoch 43), train_loss = 0.738, time/batch = 0.092\n",
            "19614/22300 (epoch 43), train_loss = 0.743, time/batch = 0.092\n",
            "19615/22300 (epoch 43), train_loss = 0.759, time/batch = 0.092\n",
            "19616/22300 (epoch 43), train_loss = 0.847, time/batch = 0.092\n",
            "19617/22300 (epoch 43), train_loss = 0.786, time/batch = 0.092\n",
            "19618/22300 (epoch 43), train_loss = 0.781, time/batch = 0.092\n",
            "19619/22300 (epoch 43), train_loss = 0.761, time/batch = 0.093\n",
            "19620/22300 (epoch 43), train_loss = 0.771, time/batch = 0.092\n",
            "19621/22300 (epoch 43), train_loss = 0.782, time/batch = 0.092\n",
            "19622/22300 (epoch 43), train_loss = 0.772, time/batch = 0.092\n",
            "19623/22300 (epoch 43), train_loss = 0.775, time/batch = 0.092\n",
            "19624/22300 (epoch 44), train_loss = 0.478, time/batch = 0.092\n",
            "19625/22300 (epoch 44), train_loss = 0.784, time/batch = 0.094\n",
            "19626/22300 (epoch 44), train_loss = 0.804, time/batch = 0.091\n",
            "19627/22300 (epoch 44), train_loss = 0.800, time/batch = 0.094\n",
            "19628/22300 (epoch 44), train_loss = 0.826, time/batch = 0.093\n",
            "19629/22300 (epoch 44), train_loss = 0.758, time/batch = 0.092\n",
            "19630/22300 (epoch 44), train_loss = 0.787, time/batch = 0.093\n",
            "19631/22300 (epoch 44), train_loss = 0.809, time/batch = 0.091\n",
            "19632/22300 (epoch 44), train_loss = 0.769, time/batch = 0.092\n",
            "19633/22300 (epoch 44), train_loss = 0.768, time/batch = 0.098\n",
            "19634/22300 (epoch 44), train_loss = 0.836, time/batch = 0.092\n",
            "19635/22300 (epoch 44), train_loss = 0.781, time/batch = 0.092\n",
            "19636/22300 (epoch 44), train_loss = 0.822, time/batch = 0.091\n",
            "19637/22300 (epoch 44), train_loss = 0.813, time/batch = 0.095\n",
            "19638/22300 (epoch 44), train_loss = 0.790, time/batch = 0.093\n",
            "19639/22300 (epoch 44), train_loss = 0.813, time/batch = 0.096\n",
            "19640/22300 (epoch 44), train_loss = 0.828, time/batch = 0.092\n",
            "19641/22300 (epoch 44), train_loss = 0.784, time/batch = 0.091\n",
            "19642/22300 (epoch 44), train_loss = 0.785, time/batch = 0.091\n",
            "19643/22300 (epoch 44), train_loss = 0.827, time/batch = 0.094\n",
            "19644/22300 (epoch 44), train_loss = 0.812, time/batch = 0.093\n",
            "19645/22300 (epoch 44), train_loss = 0.771, time/batch = 0.097\n",
            "19646/22300 (epoch 44), train_loss = 0.771, time/batch = 0.093\n",
            "19647/22300 (epoch 44), train_loss = 0.758, time/batch = 0.091\n",
            "19648/22300 (epoch 44), train_loss = 0.736, time/batch = 0.093\n",
            "19649/22300 (epoch 44), train_loss = 0.777, time/batch = 0.092\n",
            "19650/22300 (epoch 44), train_loss = 0.771, time/batch = 0.093\n",
            "19651/22300 (epoch 44), train_loss = 0.756, time/batch = 0.093\n",
            "19652/22300 (epoch 44), train_loss = 0.782, time/batch = 0.092\n",
            "19653/22300 (epoch 44), train_loss = 0.756, time/batch = 0.093\n",
            "19654/22300 (epoch 44), train_loss = 0.759, time/batch = 0.105\n",
            "19655/22300 (epoch 44), train_loss = 0.777, time/batch = 0.093\n",
            "19656/22300 (epoch 44), train_loss = 0.779, time/batch = 0.099\n",
            "19657/22300 (epoch 44), train_loss = 0.804, time/batch = 0.093\n",
            "19658/22300 (epoch 44), train_loss = 0.789, time/batch = 0.096\n",
            "19659/22300 (epoch 44), train_loss = 0.766, time/batch = 0.090\n",
            "19660/22300 (epoch 44), train_loss = 0.805, time/batch = 0.092\n",
            "19661/22300 (epoch 44), train_loss = 0.762, time/batch = 0.092\n",
            "19662/22300 (epoch 44), train_loss = 0.786, time/batch = 0.091\n",
            "19663/22300 (epoch 44), train_loss = 0.753, time/batch = 0.092\n",
            "19664/22300 (epoch 44), train_loss = 0.768, time/batch = 0.095\n",
            "19665/22300 (epoch 44), train_loss = 0.782, time/batch = 0.092\n",
            "19666/22300 (epoch 44), train_loss = 0.778, time/batch = 0.093\n",
            "19667/22300 (epoch 44), train_loss = 0.782, time/batch = 0.094\n",
            "19668/22300 (epoch 44), train_loss = 0.754, time/batch = 0.092\n",
            "19669/22300 (epoch 44), train_loss = 0.807, time/batch = 0.092\n",
            "19670/22300 (epoch 44), train_loss = 0.769, time/batch = 0.092\n",
            "19671/22300 (epoch 44), train_loss = 0.756, time/batch = 0.091\n",
            "19672/22300 (epoch 44), train_loss = 0.775, time/batch = 0.091\n",
            "19673/22300 (epoch 44), train_loss = 0.775, time/batch = 0.092\n",
            "19674/22300 (epoch 44), train_loss = 0.786, time/batch = 0.092\n",
            "19675/22300 (epoch 44), train_loss = 0.746, time/batch = 0.099\n",
            "19676/22300 (epoch 44), train_loss = 0.759, time/batch = 0.092\n",
            "19677/22300 (epoch 44), train_loss = 0.767, time/batch = 0.092\n",
            "19678/22300 (epoch 44), train_loss = 0.749, time/batch = 0.094\n",
            "19679/22300 (epoch 44), train_loss = 0.752, time/batch = 0.092\n",
            "19680/22300 (epoch 44), train_loss = 0.746, time/batch = 0.092\n",
            "19681/22300 (epoch 44), train_loss = 0.798, time/batch = 0.092\n",
            "19682/22300 (epoch 44), train_loss = 0.760, time/batch = 0.091\n",
            "19683/22300 (epoch 44), train_loss = 0.746, time/batch = 0.093\n",
            "19684/22300 (epoch 44), train_loss = 0.790, time/batch = 0.091\n",
            "19685/22300 (epoch 44), train_loss = 0.748, time/batch = 0.092\n",
            "19686/22300 (epoch 44), train_loss = 0.737, time/batch = 0.092\n",
            "19687/22300 (epoch 44), train_loss = 0.783, time/batch = 0.092\n",
            "19688/22300 (epoch 44), train_loss = 0.760, time/batch = 0.095\n",
            "19689/22300 (epoch 44), train_loss = 0.767, time/batch = 0.090\n",
            "19690/22300 (epoch 44), train_loss = 0.776, time/batch = 0.092\n",
            "19691/22300 (epoch 44), train_loss = 0.820, time/batch = 0.091\n",
            "19692/22300 (epoch 44), train_loss = 0.785, time/batch = 0.092\n",
            "19693/22300 (epoch 44), train_loss = 0.789, time/batch = 0.092\n",
            "19694/22300 (epoch 44), train_loss = 0.778, time/batch = 0.092\n",
            "19695/22300 (epoch 44), train_loss = 0.722, time/batch = 0.094\n",
            "19696/22300 (epoch 44), train_loss = 0.755, time/batch = 0.091\n",
            "19697/22300 (epoch 44), train_loss = 0.742, time/batch = 0.093\n",
            "19698/22300 (epoch 44), train_loss = 0.732, time/batch = 0.092\n",
            "19699/22300 (epoch 44), train_loss = 0.783, time/batch = 0.091\n",
            "19700/22300 (epoch 44), train_loss = 0.785, time/batch = 0.092\n",
            "19701/22300 (epoch 44), train_loss = 0.767, time/batch = 0.092\n",
            "19702/22300 (epoch 44), train_loss = 0.757, time/batch = 0.093\n",
            "19703/22300 (epoch 44), train_loss = 0.807, time/batch = 0.091\n",
            "19704/22300 (epoch 44), train_loss = 0.783, time/batch = 0.093\n",
            "19705/22300 (epoch 44), train_loss = 0.762, time/batch = 0.092\n",
            "19706/22300 (epoch 44), train_loss = 0.778, time/batch = 0.093\n",
            "19707/22300 (epoch 44), train_loss = 0.770, time/batch = 0.093\n",
            "19708/22300 (epoch 44), train_loss = 0.788, time/batch = 0.092\n",
            "19709/22300 (epoch 44), train_loss = 0.770, time/batch = 0.092\n",
            "19710/22300 (epoch 44), train_loss = 0.758, time/batch = 0.092\n",
            "19711/22300 (epoch 44), train_loss = 0.796, time/batch = 0.093\n",
            "19712/22300 (epoch 44), train_loss = 0.752, time/batch = 0.091\n",
            "19713/22300 (epoch 44), train_loss = 0.792, time/batch = 0.091\n",
            "19714/22300 (epoch 44), train_loss = 0.744, time/batch = 0.091\n",
            "19715/22300 (epoch 44), train_loss = 0.761, time/batch = 0.092\n",
            "19716/22300 (epoch 44), train_loss = 0.787, time/batch = 0.091\n",
            "19717/22300 (epoch 44), train_loss = 0.769, time/batch = 0.092\n",
            "19718/22300 (epoch 44), train_loss = 0.784, time/batch = 0.093\n",
            "19719/22300 (epoch 44), train_loss = 0.787, time/batch = 0.092\n",
            "19720/22300 (epoch 44), train_loss = 0.741, time/batch = 0.093\n",
            "19721/22300 (epoch 44), train_loss = 0.794, time/batch = 0.091\n",
            "19722/22300 (epoch 44), train_loss = 0.741, time/batch = 0.093\n",
            "19723/22300 (epoch 44), train_loss = 0.755, time/batch = 0.092\n",
            "19724/22300 (epoch 44), train_loss = 0.752, time/batch = 0.092\n",
            "19725/22300 (epoch 44), train_loss = 0.749, time/batch = 0.092\n",
            "19726/22300 (epoch 44), train_loss = 0.756, time/batch = 0.092\n",
            "19727/22300 (epoch 44), train_loss = 0.726, time/batch = 0.093\n",
            "19728/22300 (epoch 44), train_loss = 0.751, time/batch = 0.092\n",
            "19729/22300 (epoch 44), train_loss = 0.715, time/batch = 0.093\n",
            "19730/22300 (epoch 44), train_loss = 0.713, time/batch = 0.092\n",
            "19731/22300 (epoch 44), train_loss = 0.730, time/batch = 0.092\n",
            "19732/22300 (epoch 44), train_loss = 0.745, time/batch = 0.091\n",
            "19733/22300 (epoch 44), train_loss = 0.739, time/batch = 0.092\n",
            "19734/22300 (epoch 44), train_loss = 0.711, time/batch = 0.092\n",
            "19735/22300 (epoch 44), train_loss = 0.706, time/batch = 0.092\n",
            "19736/22300 (epoch 44), train_loss = 0.745, time/batch = 0.092\n",
            "19737/22300 (epoch 44), train_loss = 0.717, time/batch = 0.092\n",
            "19738/22300 (epoch 44), train_loss = 0.750, time/batch = 0.093\n",
            "19739/22300 (epoch 44), train_loss = 0.728, time/batch = 0.091\n",
            "19740/22300 (epoch 44), train_loss = 0.738, time/batch = 0.093\n",
            "19741/22300 (epoch 44), train_loss = 0.764, time/batch = 0.092\n",
            "19742/22300 (epoch 44), train_loss = 0.784, time/batch = 0.092\n",
            "19743/22300 (epoch 44), train_loss = 0.750, time/batch = 0.093\n",
            "19744/22300 (epoch 44), train_loss = 0.779, time/batch = 0.092\n",
            "19745/22300 (epoch 44), train_loss = 0.765, time/batch = 0.093\n",
            "19746/22300 (epoch 44), train_loss = 0.746, time/batch = 0.091\n",
            "19747/22300 (epoch 44), train_loss = 0.746, time/batch = 0.092\n",
            "19748/22300 (epoch 44), train_loss = 0.771, time/batch = 0.092\n",
            "19749/22300 (epoch 44), train_loss = 0.779, time/batch = 0.093\n",
            "19750/22300 (epoch 44), train_loss = 0.752, time/batch = 0.095\n",
            "19751/22300 (epoch 44), train_loss = 0.782, time/batch = 0.091\n",
            "19752/22300 (epoch 44), train_loss = 0.779, time/batch = 0.093\n",
            "19753/22300 (epoch 44), train_loss = 0.759, time/batch = 0.094\n",
            "19754/22300 (epoch 44), train_loss = 0.747, time/batch = 0.092\n",
            "19755/22300 (epoch 44), train_loss = 0.764, time/batch = 0.093\n",
            "19756/22300 (epoch 44), train_loss = 0.776, time/batch = 0.092\n",
            "19757/22300 (epoch 44), train_loss = 0.778, time/batch = 0.092\n",
            "19758/22300 (epoch 44), train_loss = 0.752, time/batch = 0.092\n",
            "19759/22300 (epoch 44), train_loss = 0.782, time/batch = 0.092\n",
            "19760/22300 (epoch 44), train_loss = 0.794, time/batch = 0.092\n",
            "19761/22300 (epoch 44), train_loss = 0.780, time/batch = 0.093\n",
            "19762/22300 (epoch 44), train_loss = 0.780, time/batch = 0.106\n",
            "19763/22300 (epoch 44), train_loss = 0.799, time/batch = 0.096\n",
            "19764/22300 (epoch 44), train_loss = 0.759, time/batch = 0.099\n",
            "19765/22300 (epoch 44), train_loss = 0.776, time/batch = 0.091\n",
            "19766/22300 (epoch 44), train_loss = 0.753, time/batch = 0.091\n",
            "19767/22300 (epoch 44), train_loss = 0.765, time/batch = 0.092\n",
            "19768/22300 (epoch 44), train_loss = 0.774, time/batch = 0.091\n",
            "19769/22300 (epoch 44), train_loss = 0.831, time/batch = 0.093\n",
            "19770/22300 (epoch 44), train_loss = 0.783, time/batch = 0.091\n",
            "19771/22300 (epoch 44), train_loss = 0.764, time/batch = 0.093\n",
            "19772/22300 (epoch 44), train_loss = 0.770, time/batch = 0.091\n",
            "19773/22300 (epoch 44), train_loss = 0.769, time/batch = 0.092\n",
            "19774/22300 (epoch 44), train_loss = 0.774, time/batch = 0.093\n",
            "19775/22300 (epoch 44), train_loss = 0.780, time/batch = 0.092\n",
            "19776/22300 (epoch 44), train_loss = 0.767, time/batch = 0.092\n",
            "19777/22300 (epoch 44), train_loss = 0.745, time/batch = 0.096\n",
            "19778/22300 (epoch 44), train_loss = 0.781, time/batch = 0.095\n",
            "19779/22300 (epoch 44), train_loss = 0.771, time/batch = 0.093\n",
            "19780/22300 (epoch 44), train_loss = 0.773, time/batch = 0.094\n",
            "19781/22300 (epoch 44), train_loss = 0.757, time/batch = 0.094\n",
            "19782/22300 (epoch 44), train_loss = 0.806, time/batch = 0.095\n",
            "19783/22300 (epoch 44), train_loss = 0.770, time/batch = 0.091\n",
            "19784/22300 (epoch 44), train_loss = 0.762, time/batch = 0.092\n",
            "19785/22300 (epoch 44), train_loss = 0.787, time/batch = 0.092\n",
            "19786/22300 (epoch 44), train_loss = 0.770, time/batch = 0.093\n",
            "19787/22300 (epoch 44), train_loss = 0.761, time/batch = 0.092\n",
            "19788/22300 (epoch 44), train_loss = 0.777, time/batch = 0.092\n",
            "19789/22300 (epoch 44), train_loss = 0.774, time/batch = 0.092\n",
            "19790/22300 (epoch 44), train_loss = 0.746, time/batch = 0.092\n",
            "19791/22300 (epoch 44), train_loss = 0.740, time/batch = 0.093\n",
            "19792/22300 (epoch 44), train_loss = 0.750, time/batch = 0.092\n",
            "19793/22300 (epoch 44), train_loss = 0.784, time/batch = 0.092\n",
            "19794/22300 (epoch 44), train_loss = 0.783, time/batch = 0.092\n",
            "19795/22300 (epoch 44), train_loss = 0.771, time/batch = 0.092\n",
            "19796/22300 (epoch 44), train_loss = 0.755, time/batch = 0.093\n",
            "19797/22300 (epoch 44), train_loss = 0.749, time/batch = 0.092\n",
            "19798/22300 (epoch 44), train_loss = 0.756, time/batch = 0.092\n",
            "19799/22300 (epoch 44), train_loss = 0.735, time/batch = 0.092\n",
            "19800/22300 (epoch 44), train_loss = 0.710, time/batch = 0.092\n",
            "19801/22300 (epoch 44), train_loss = 0.748, time/batch = 0.096\n",
            "19802/22300 (epoch 44), train_loss = 0.686, time/batch = 0.095\n",
            "19803/22300 (epoch 44), train_loss = 0.752, time/batch = 0.092\n",
            "19804/22300 (epoch 44), train_loss = 0.734, time/batch = 0.091\n",
            "19805/22300 (epoch 44), train_loss = 0.707, time/batch = 0.092\n",
            "19806/22300 (epoch 44), train_loss = 0.749, time/batch = 0.093\n",
            "19807/22300 (epoch 44), train_loss = 0.723, time/batch = 0.095\n",
            "19808/22300 (epoch 44), train_loss = 0.713, time/batch = 0.092\n",
            "19809/22300 (epoch 44), train_loss = 0.715, time/batch = 0.092\n",
            "19810/22300 (epoch 44), train_loss = 0.783, time/batch = 0.092\n",
            "19811/22300 (epoch 44), train_loss = 0.751, time/batch = 0.093\n",
            "19812/22300 (epoch 44), train_loss = 0.774, time/batch = 0.092\n",
            "19813/22300 (epoch 44), train_loss = 0.751, time/batch = 0.092\n",
            "19814/22300 (epoch 44), train_loss = 0.768, time/batch = 0.092\n",
            "19815/22300 (epoch 44), train_loss = 0.759, time/batch = 0.092\n",
            "19816/22300 (epoch 44), train_loss = 0.753, time/batch = 0.092\n",
            "19817/22300 (epoch 44), train_loss = 0.777, time/batch = 0.093\n",
            "19818/22300 (epoch 44), train_loss = 0.768, time/batch = 0.091\n",
            "19819/22300 (epoch 44), train_loss = 0.749, time/batch = 0.092\n",
            "19820/22300 (epoch 44), train_loss = 0.758, time/batch = 0.092\n",
            "19821/22300 (epoch 44), train_loss = 0.747, time/batch = 0.093\n",
            "19822/22300 (epoch 44), train_loss = 0.790, time/batch = 0.093\n",
            "19823/22300 (epoch 44), train_loss = 0.759, time/batch = 0.092\n",
            "19824/22300 (epoch 44), train_loss = 0.787, time/batch = 0.093\n",
            "19825/22300 (epoch 44), train_loss = 0.755, time/batch = 0.092\n",
            "19826/22300 (epoch 44), train_loss = 0.747, time/batch = 0.093\n",
            "19827/22300 (epoch 44), train_loss = 0.734, time/batch = 0.095\n",
            "19828/22300 (epoch 44), train_loss = 0.741, time/batch = 0.092\n",
            "19829/22300 (epoch 44), train_loss = 0.746, time/batch = 0.092\n",
            "19830/22300 (epoch 44), train_loss = 0.790, time/batch = 0.091\n",
            "19831/22300 (epoch 44), train_loss = 0.743, time/batch = 0.092\n",
            "19832/22300 (epoch 44), train_loss = 0.788, time/batch = 0.092\n",
            "19833/22300 (epoch 44), train_loss = 0.740, time/batch = 0.094\n",
            "19834/22300 (epoch 44), train_loss = 0.745, time/batch = 0.092\n",
            "19835/22300 (epoch 44), train_loss = 0.732, time/batch = 0.091\n",
            "19836/22300 (epoch 44), train_loss = 0.750, time/batch = 0.092\n",
            "19837/22300 (epoch 44), train_loss = 0.751, time/batch = 0.092\n",
            "19838/22300 (epoch 44), train_loss = 0.720, time/batch = 0.092\n",
            "19839/22300 (epoch 44), train_loss = 0.736, time/batch = 0.092\n",
            "19840/22300 (epoch 44), train_loss = 0.738, time/batch = 0.094\n",
            "19841/22300 (epoch 44), train_loss = 0.759, time/batch = 0.092\n",
            "19842/22300 (epoch 44), train_loss = 0.765, time/batch = 0.091\n",
            "19843/22300 (epoch 44), train_loss = 0.760, time/batch = 0.094\n",
            "19844/22300 (epoch 44), train_loss = 0.781, time/batch = 0.091\n",
            "19845/22300 (epoch 44), train_loss = 0.745, time/batch = 0.092\n",
            "19846/22300 (epoch 44), train_loss = 0.750, time/batch = 0.092\n",
            "19847/22300 (epoch 44), train_loss = 0.793, time/batch = 0.091\n",
            "19848/22300 (epoch 44), train_loss = 0.739, time/batch = 0.092\n",
            "19849/22300 (epoch 44), train_loss = 0.764, time/batch = 0.092\n",
            "19850/22300 (epoch 44), train_loss = 0.791, time/batch = 0.092\n",
            "19851/22300 (epoch 44), train_loss = 0.783, time/batch = 0.091\n",
            "19852/22300 (epoch 44), train_loss = 0.794, time/batch = 0.092\n",
            "19853/22300 (epoch 44), train_loss = 0.786, time/batch = 0.092\n",
            "19854/22300 (epoch 44), train_loss = 0.774, time/batch = 0.092\n",
            "19855/22300 (epoch 44), train_loss = 0.767, time/batch = 0.093\n",
            "19856/22300 (epoch 44), train_loss = 0.748, time/batch = 0.091\n",
            "19857/22300 (epoch 44), train_loss = 0.765, time/batch = 0.091\n",
            "19858/22300 (epoch 44), train_loss = 0.783, time/batch = 0.091\n",
            "19859/22300 (epoch 44), train_loss = 0.767, time/batch = 0.093\n",
            "19860/22300 (epoch 44), train_loss = 0.762, time/batch = 0.092\n",
            "19861/22300 (epoch 44), train_loss = 0.733, time/batch = 0.092\n",
            "19862/22300 (epoch 44), train_loss = 0.758, time/batch = 0.093\n",
            "19863/22300 (epoch 44), train_loss = 0.761, time/batch = 0.092\n",
            "19864/22300 (epoch 44), train_loss = 0.791, time/batch = 0.099\n",
            "19865/22300 (epoch 44), train_loss = 0.770, time/batch = 0.091\n",
            "19866/22300 (epoch 44), train_loss = 0.783, time/batch = 0.093\n",
            "19867/22300 (epoch 44), train_loss = 0.805, time/batch = 0.091\n",
            "19868/22300 (epoch 44), train_loss = 0.758, time/batch = 0.092\n",
            "19869/22300 (epoch 44), train_loss = 0.799, time/batch = 0.092\n",
            "19870/22300 (epoch 44), train_loss = 0.789, time/batch = 0.092\n",
            "19871/22300 (epoch 44), train_loss = 0.733, time/batch = 0.093\n",
            "19872/22300 (epoch 44), train_loss = 0.764, time/batch = 0.093\n",
            "19873/22300 (epoch 44), train_loss = 0.751, time/batch = 0.092\n",
            "19874/22300 (epoch 44), train_loss = 0.745, time/batch = 0.094\n",
            "19875/22300 (epoch 44), train_loss = 0.740, time/batch = 0.092\n",
            "19876/22300 (epoch 44), train_loss = 0.766, time/batch = 0.092\n",
            "19877/22300 (epoch 44), train_loss = 0.772, time/batch = 0.091\n",
            "19878/22300 (epoch 44), train_loss = 0.756, time/batch = 0.091\n",
            "19879/22300 (epoch 44), train_loss = 0.778, time/batch = 0.091\n",
            "19880/22300 (epoch 44), train_loss = 0.759, time/batch = 0.092\n",
            "19881/22300 (epoch 44), train_loss = 0.797, time/batch = 0.092\n",
            "19882/22300 (epoch 44), train_loss = 0.747, time/batch = 0.093\n",
            "19883/22300 (epoch 44), train_loss = 0.762, time/batch = 0.092\n",
            "19884/22300 (epoch 44), train_loss = 0.741, time/batch = 0.092\n",
            "19885/22300 (epoch 44), train_loss = 0.801, time/batch = 0.092\n",
            "19886/22300 (epoch 44), train_loss = 0.749, time/batch = 0.092\n",
            "19887/22300 (epoch 44), train_loss = 0.800, time/batch = 0.092\n",
            "19888/22300 (epoch 44), train_loss = 0.770, time/batch = 0.093\n",
            "19889/22300 (epoch 44), train_loss = 0.787, time/batch = 0.094\n",
            "19890/22300 (epoch 44), train_loss = 0.787, time/batch = 0.091\n",
            "19891/22300 (epoch 44), train_loss = 0.789, time/batch = 0.091\n",
            "19892/22300 (epoch 44), train_loss = 0.778, time/batch = 0.092\n",
            "19893/22300 (epoch 44), train_loss = 0.761, time/batch = 0.092\n",
            "19894/22300 (epoch 44), train_loss = 0.789, time/batch = 0.092\n",
            "19895/22300 (epoch 44), train_loss = 0.773, time/batch = 0.093\n",
            "19896/22300 (epoch 44), train_loss = 0.800, time/batch = 0.092\n",
            "19897/22300 (epoch 44), train_loss = 0.797, time/batch = 0.093\n",
            "19898/22300 (epoch 44), train_loss = 0.805, time/batch = 0.092\n",
            "19899/22300 (epoch 44), train_loss = 0.798, time/batch = 0.092\n",
            "19900/22300 (epoch 44), train_loss = 0.826, time/batch = 0.092\n",
            "19901/22300 (epoch 44), train_loss = 0.778, time/batch = 0.092\n",
            "19902/22300 (epoch 44), train_loss = 0.761, time/batch = 0.100\n",
            "19903/22300 (epoch 44), train_loss = 0.779, time/batch = 0.094\n",
            "19904/22300 (epoch 44), train_loss = 0.787, time/batch = 0.093\n",
            "19905/22300 (epoch 44), train_loss = 0.829, time/batch = 0.094\n",
            "19906/22300 (epoch 44), train_loss = 0.796, time/batch = 0.092\n",
            "19907/22300 (epoch 44), train_loss = 0.774, time/batch = 0.093\n",
            "19908/22300 (epoch 44), train_loss = 0.769, time/batch = 0.092\n",
            "19909/22300 (epoch 44), train_loss = 0.762, time/batch = 0.092\n",
            "19910/22300 (epoch 44), train_loss = 0.788, time/batch = 0.092\n",
            "19911/22300 (epoch 44), train_loss = 0.782, time/batch = 0.092\n",
            "19912/22300 (epoch 44), train_loss = 0.777, time/batch = 0.093\n",
            "19913/22300 (epoch 44), train_loss = 0.784, time/batch = 0.091\n",
            "19914/22300 (epoch 44), train_loss = 0.761, time/batch = 0.092\n",
            "19915/22300 (epoch 44), train_loss = 0.775, time/batch = 0.091\n",
            "19916/22300 (epoch 44), train_loss = 0.802, time/batch = 0.093\n",
            "19917/22300 (epoch 44), train_loss = 0.793, time/batch = 0.092\n",
            "19918/22300 (epoch 44), train_loss = 0.793, time/batch = 0.092\n",
            "19919/22300 (epoch 44), train_loss = 0.779, time/batch = 0.094\n",
            "19920/22300 (epoch 44), train_loss = 0.757, time/batch = 0.091\n",
            "19921/22300 (epoch 44), train_loss = 0.767, time/batch = 0.092\n",
            "19922/22300 (epoch 44), train_loss = 0.759, time/batch = 0.092\n",
            "19923/22300 (epoch 44), train_loss = 0.796, time/batch = 0.092\n",
            "19924/22300 (epoch 44), train_loss = 0.747, time/batch = 0.094\n",
            "19925/22300 (epoch 44), train_loss = 0.791, time/batch = 0.092\n",
            "19926/22300 (epoch 44), train_loss = 0.750, time/batch = 0.093\n",
            "19927/22300 (epoch 44), train_loss = 0.763, time/batch = 0.092\n",
            "19928/22300 (epoch 44), train_loss = 0.778, time/batch = 0.092\n",
            "19929/22300 (epoch 44), train_loss = 0.763, time/batch = 0.093\n",
            "19930/22300 (epoch 44), train_loss = 0.735, time/batch = 0.092\n",
            "19931/22300 (epoch 44), train_loss = 0.797, time/batch = 0.092\n",
            "19932/22300 (epoch 44), train_loss = 0.776, time/batch = 0.092\n",
            "19933/22300 (epoch 44), train_loss = 0.778, time/batch = 0.093\n",
            "19934/22300 (epoch 44), train_loss = 0.753, time/batch = 0.091\n",
            "19935/22300 (epoch 44), train_loss = 0.748, time/batch = 0.092\n",
            "19936/22300 (epoch 44), train_loss = 0.746, time/batch = 0.092\n",
            "19937/22300 (epoch 44), train_loss = 0.766, time/batch = 0.092\n",
            "19938/22300 (epoch 44), train_loss = 0.749, time/batch = 0.092\n",
            "19939/22300 (epoch 44), train_loss = 0.750, time/batch = 0.095\n",
            "19940/22300 (epoch 44), train_loss = 0.734, time/batch = 0.091\n",
            "19941/22300 (epoch 44), train_loss = 0.780, time/batch = 0.092\n",
            "19942/22300 (epoch 44), train_loss = 0.740, time/batch = 0.092\n",
            "19943/22300 (epoch 44), train_loss = 0.729, time/batch = 0.093\n",
            "19944/22300 (epoch 44), train_loss = 0.746, time/batch = 0.092\n",
            "19945/22300 (epoch 44), train_loss = 0.798, time/batch = 0.092\n",
            "19946/22300 (epoch 44), train_loss = 0.767, time/batch = 0.091\n",
            "19947/22300 (epoch 44), train_loss = 0.729, time/batch = 0.092\n",
            "19948/22300 (epoch 44), train_loss = 0.742, time/batch = 0.092\n",
            "19949/22300 (epoch 44), train_loss = 0.767, time/batch = 0.092\n",
            "19950/22300 (epoch 44), train_loss = 0.770, time/batch = 0.093\n",
            "19951/22300 (epoch 44), train_loss = 0.778, time/batch = 0.091\n",
            "19952/22300 (epoch 44), train_loss = 0.816, time/batch = 0.093\n",
            "19953/22300 (epoch 44), train_loss = 0.765, time/batch = 0.093\n",
            "19954/22300 (epoch 44), train_loss = 0.739, time/batch = 0.094\n",
            "19955/22300 (epoch 44), train_loss = 0.765, time/batch = 0.093\n",
            "19956/22300 (epoch 44), train_loss = 0.752, time/batch = 0.091\n",
            "19957/22300 (epoch 44), train_loss = 0.725, time/batch = 0.095\n",
            "19958/22300 (epoch 44), train_loss = 0.751, time/batch = 0.092\n",
            "19959/22300 (epoch 44), train_loss = 0.769, time/batch = 0.092\n",
            "19960/22300 (epoch 44), train_loss = 0.787, time/batch = 0.092\n",
            "19961/22300 (epoch 44), train_loss = 0.766, time/batch = 0.092\n",
            "19962/22300 (epoch 44), train_loss = 0.788, time/batch = 0.093\n",
            "19963/22300 (epoch 44), train_loss = 0.780, time/batch = 0.092\n",
            "19964/22300 (epoch 44), train_loss = 0.786, time/batch = 0.092\n",
            "19965/22300 (epoch 44), train_loss = 0.743, time/batch = 0.092\n",
            "19966/22300 (epoch 44), train_loss = 0.734, time/batch = 0.095\n",
            "19967/22300 (epoch 44), train_loss = 0.758, time/batch = 0.094\n",
            "19968/22300 (epoch 44), train_loss = 0.754, time/batch = 0.091\n",
            "19969/22300 (epoch 44), train_loss = 0.776, time/batch = 0.092\n",
            "19970/22300 (epoch 44), train_loss = 0.748, time/batch = 0.091\n",
            "19971/22300 (epoch 44), train_loss = 0.796, time/batch = 0.092\n",
            "19972/22300 (epoch 44), train_loss = 0.743, time/batch = 0.092\n",
            "19973/22300 (epoch 44), train_loss = 0.768, time/batch = 0.092\n",
            "19974/22300 (epoch 44), train_loss = 0.758, time/batch = 0.093\n",
            "19975/22300 (epoch 44), train_loss = 0.764, time/batch = 0.092\n",
            "19976/22300 (epoch 44), train_loss = 0.759, time/batch = 0.093\n",
            "19977/22300 (epoch 44), train_loss = 0.761, time/batch = 0.092\n",
            "19978/22300 (epoch 44), train_loss = 0.776, time/batch = 0.093\n",
            "19979/22300 (epoch 44), train_loss = 0.743, time/batch = 0.096\n",
            "19980/22300 (epoch 44), train_loss = 0.797, time/batch = 0.092\n",
            "19981/22300 (epoch 44), train_loss = 0.788, time/batch = 0.095\n",
            "19982/22300 (epoch 44), train_loss = 0.761, time/batch = 0.092\n",
            "19983/22300 (epoch 44), train_loss = 0.765, time/batch = 0.091\n",
            "19984/22300 (epoch 44), train_loss = 0.763, time/batch = 0.092\n",
            "19985/22300 (epoch 44), train_loss = 0.746, time/batch = 0.092\n",
            "19986/22300 (epoch 44), train_loss = 0.745, time/batch = 0.093\n",
            "19987/22300 (epoch 44), train_loss = 0.754, time/batch = 0.092\n",
            "19988/22300 (epoch 44), train_loss = 0.772, time/batch = 0.093\n",
            "19989/22300 (epoch 44), train_loss = 0.755, time/batch = 0.091\n",
            "19990/22300 (epoch 44), train_loss = 0.741, time/batch = 0.093\n",
            "19991/22300 (epoch 44), train_loss = 0.727, time/batch = 0.093\n",
            "19992/22300 (epoch 44), train_loss = 0.745, time/batch = 0.093\n",
            "19993/22300 (epoch 44), train_loss = 0.758, time/batch = 0.100\n",
            "19994/22300 (epoch 44), train_loss = 0.782, time/batch = 0.092\n",
            "19995/22300 (epoch 44), train_loss = 0.723, time/batch = 0.099\n",
            "19996/22300 (epoch 44), train_loss = 0.783, time/batch = 0.092\n",
            "19997/22300 (epoch 44), train_loss = 0.743, time/batch = 0.093\n",
            "19998/22300 (epoch 44), train_loss = 0.771, time/batch = 0.093\n",
            "19999/22300 (epoch 44), train_loss = 0.742, time/batch = 0.092\n",
            "20000/22300 (epoch 44), train_loss = 0.741, time/batch = 0.092\n",
            "model saved to save/model.ckpt\n",
            "20001/22300 (epoch 44), train_loss = 0.768, time/batch = 0.087\n",
            "20002/22300 (epoch 44), train_loss = 0.718, time/batch = 0.092\n",
            "20003/22300 (epoch 44), train_loss = 0.715, time/batch = 0.092\n",
            "20004/22300 (epoch 44), train_loss = 0.720, time/batch = 0.091\n",
            "20005/22300 (epoch 44), train_loss = 0.741, time/batch = 0.097\n",
            "20006/22300 (epoch 44), train_loss = 0.731, time/batch = 0.090\n",
            "20007/22300 (epoch 44), train_loss = 0.698, time/batch = 0.092\n",
            "20008/22300 (epoch 44), train_loss = 0.743, time/batch = 0.092\n",
            "20009/22300 (epoch 44), train_loss = 0.744, time/batch = 0.091\n",
            "20010/22300 (epoch 44), train_loss = 0.737, time/batch = 0.101\n",
            "20011/22300 (epoch 44), train_loss = 0.764, time/batch = 0.093\n",
            "20012/22300 (epoch 44), train_loss = 0.725, time/batch = 0.091\n",
            "20013/22300 (epoch 44), train_loss = 0.731, time/batch = 0.092\n",
            "20014/22300 (epoch 44), train_loss = 0.731, time/batch = 0.091\n",
            "20015/22300 (epoch 44), train_loss = 0.712, time/batch = 0.092\n",
            "20016/22300 (epoch 44), train_loss = 0.736, time/batch = 0.091\n",
            "20017/22300 (epoch 44), train_loss = 0.775, time/batch = 0.092\n",
            "20018/22300 (epoch 44), train_loss = 0.731, time/batch = 0.091\n",
            "20019/22300 (epoch 44), train_loss = 0.761, time/batch = 0.099\n",
            "20020/22300 (epoch 44), train_loss = 0.768, time/batch = 0.093\n",
            "20021/22300 (epoch 44), train_loss = 0.746, time/batch = 0.095\n",
            "20022/22300 (epoch 44), train_loss = 0.754, time/batch = 0.092\n",
            "20023/22300 (epoch 44), train_loss = 0.717, time/batch = 0.093\n",
            "20024/22300 (epoch 44), train_loss = 0.743, time/batch = 0.091\n",
            "20025/22300 (epoch 44), train_loss = 0.765, time/batch = 0.092\n",
            "20026/22300 (epoch 44), train_loss = 0.754, time/batch = 0.091\n",
            "20027/22300 (epoch 44), train_loss = 0.728, time/batch = 0.093\n",
            "20028/22300 (epoch 44), train_loss = 0.742, time/batch = 0.092\n",
            "20029/22300 (epoch 44), train_loss = 0.763, time/batch = 0.092\n",
            "20030/22300 (epoch 44), train_loss = 0.759, time/batch = 0.093\n",
            "20031/22300 (epoch 44), train_loss = 0.794, time/batch = 0.093\n",
            "20032/22300 (epoch 44), train_loss = 0.769, time/batch = 0.093\n",
            "20033/22300 (epoch 44), train_loss = 0.792, time/batch = 0.092\n",
            "20034/22300 (epoch 44), train_loss = 0.799, time/batch = 0.093\n",
            "20035/22300 (epoch 44), train_loss = 0.750, time/batch = 0.100\n",
            "20036/22300 (epoch 44), train_loss = 0.780, time/batch = 0.094\n",
            "20037/22300 (epoch 44), train_loss = 0.739, time/batch = 0.091\n",
            "20038/22300 (epoch 44), train_loss = 0.785, time/batch = 0.092\n",
            "20039/22300 (epoch 44), train_loss = 0.802, time/batch = 0.091\n",
            "20040/22300 (epoch 44), train_loss = 0.768, time/batch = 0.091\n",
            "20041/22300 (epoch 44), train_loss = 0.772, time/batch = 0.092\n",
            "20042/22300 (epoch 44), train_loss = 0.774, time/batch = 0.093\n",
            "20043/22300 (epoch 44), train_loss = 0.756, time/batch = 0.092\n",
            "20044/22300 (epoch 44), train_loss = 0.749, time/batch = 0.093\n",
            "20045/22300 (epoch 44), train_loss = 0.753, time/batch = 0.093\n",
            "20046/22300 (epoch 44), train_loss = 0.758, time/batch = 0.091\n",
            "20047/22300 (epoch 44), train_loss = 0.763, time/batch = 0.094\n",
            "20048/22300 (epoch 44), train_loss = 0.786, time/batch = 0.091\n",
            "20049/22300 (epoch 44), train_loss = 0.758, time/batch = 0.091\n",
            "20050/22300 (epoch 44), train_loss = 0.812, time/batch = 0.092\n",
            "20051/22300 (epoch 44), train_loss = 0.757, time/batch = 0.098\n",
            "20052/22300 (epoch 44), train_loss = 0.782, time/batch = 0.092\n",
            "20053/22300 (epoch 44), train_loss = 0.751, time/batch = 0.092\n",
            "20054/22300 (epoch 44), train_loss = 0.767, time/batch = 0.093\n",
            "20055/22300 (epoch 44), train_loss = 0.800, time/batch = 0.094\n",
            "20056/22300 (epoch 44), train_loss = 0.774, time/batch = 0.091\n",
            "20057/22300 (epoch 44), train_loss = 0.780, time/batch = 0.093\n",
            "20058/22300 (epoch 44), train_loss = 0.733, time/batch = 0.092\n",
            "20059/22300 (epoch 44), train_loss = 0.728, time/batch = 0.092\n",
            "20060/22300 (epoch 44), train_loss = 0.714, time/batch = 0.098\n",
            "20061/22300 (epoch 44), train_loss = 0.730, time/batch = 0.090\n",
            "20062/22300 (epoch 44), train_loss = 0.814, time/batch = 0.092\n",
            "20063/22300 (epoch 44), train_loss = 0.756, time/batch = 0.091\n",
            "20064/22300 (epoch 44), train_loss = 0.768, time/batch = 0.093\n",
            "20065/22300 (epoch 44), train_loss = 0.757, time/batch = 0.092\n",
            "20066/22300 (epoch 44), train_loss = 0.768, time/batch = 0.095\n",
            "20067/22300 (epoch 44), train_loss = 0.773, time/batch = 0.093\n",
            "20068/22300 (epoch 44), train_loss = 0.762, time/batch = 0.092\n",
            "20069/22300 (epoch 44), train_loss = 0.765, time/batch = 0.091\n",
            "20070/22300 (epoch 45), train_loss = 0.464, time/batch = 0.086\n",
            "20071/22300 (epoch 45), train_loss = 0.779, time/batch = 0.094\n",
            "20072/22300 (epoch 45), train_loss = 0.800, time/batch = 0.091\n",
            "20073/22300 (epoch 45), train_loss = 0.790, time/batch = 0.094\n",
            "20074/22300 (epoch 45), train_loss = 0.809, time/batch = 0.092\n",
            "20075/22300 (epoch 45), train_loss = 0.744, time/batch = 0.091\n",
            "20076/22300 (epoch 45), train_loss = 0.768, time/batch = 0.091\n",
            "20077/22300 (epoch 45), train_loss = 0.791, time/batch = 0.092\n",
            "20078/22300 (epoch 45), train_loss = 0.749, time/batch = 0.092\n",
            "20079/22300 (epoch 45), train_loss = 0.748, time/batch = 0.099\n",
            "20080/22300 (epoch 45), train_loss = 0.818, time/batch = 0.093\n",
            "20081/22300 (epoch 45), train_loss = 0.753, time/batch = 0.092\n",
            "20082/22300 (epoch 45), train_loss = 0.797, time/batch = 0.092\n",
            "20083/22300 (epoch 45), train_loss = 0.809, time/batch = 0.099\n",
            "20084/22300 (epoch 45), train_loss = 0.780, time/batch = 0.092\n",
            "20085/22300 (epoch 45), train_loss = 0.809, time/batch = 0.092\n",
            "20086/22300 (epoch 45), train_loss = 0.823, time/batch = 0.093\n",
            "20087/22300 (epoch 45), train_loss = 0.775, time/batch = 0.096\n",
            "20088/22300 (epoch 45), train_loss = 0.788, time/batch = 0.091\n",
            "20089/22300 (epoch 45), train_loss = 0.805, time/batch = 0.094\n",
            "20090/22300 (epoch 45), train_loss = 0.791, time/batch = 0.099\n",
            "20091/22300 (epoch 45), train_loss = 0.760, time/batch = 0.092\n",
            "20092/22300 (epoch 45), train_loss = 0.762, time/batch = 0.093\n",
            "20093/22300 (epoch 45), train_loss = 0.744, time/batch = 0.094\n",
            "20094/22300 (epoch 45), train_loss = 0.729, time/batch = 0.092\n",
            "20095/22300 (epoch 45), train_loss = 0.762, time/batch = 0.093\n",
            "20096/22300 (epoch 45), train_loss = 0.762, time/batch = 0.092\n",
            "20097/22300 (epoch 45), train_loss = 0.748, time/batch = 0.093\n",
            "20098/22300 (epoch 45), train_loss = 0.781, time/batch = 0.091\n",
            "20099/22300 (epoch 45), train_loss = 0.745, time/batch = 0.096\n",
            "20100/22300 (epoch 45), train_loss = 0.744, time/batch = 0.095\n",
            "20101/22300 (epoch 45), train_loss = 0.765, time/batch = 0.093\n",
            "20102/22300 (epoch 45), train_loss = 0.753, time/batch = 0.092\n",
            "20103/22300 (epoch 45), train_loss = 0.772, time/batch = 0.093\n",
            "20104/22300 (epoch 45), train_loss = 0.765, time/batch = 0.092\n",
            "20105/22300 (epoch 45), train_loss = 0.747, time/batch = 0.092\n",
            "20106/22300 (epoch 45), train_loss = 0.786, time/batch = 0.091\n",
            "20107/22300 (epoch 45), train_loss = 0.764, time/batch = 0.092\n",
            "20108/22300 (epoch 45), train_loss = 0.784, time/batch = 0.093\n",
            "20109/22300 (epoch 45), train_loss = 0.748, time/batch = 0.092\n",
            "20110/22300 (epoch 45), train_loss = 0.765, time/batch = 0.096\n",
            "20111/22300 (epoch 45), train_loss = 0.784, time/batch = 0.092\n",
            "20112/22300 (epoch 45), train_loss = 0.763, time/batch = 0.092\n",
            "20113/22300 (epoch 45), train_loss = 0.762, time/batch = 0.093\n",
            "20114/22300 (epoch 45), train_loss = 0.732, time/batch = 0.092\n",
            "20115/22300 (epoch 45), train_loss = 0.788, time/batch = 0.093\n",
            "20116/22300 (epoch 45), train_loss = 0.746, time/batch = 0.095\n",
            "20117/22300 (epoch 45), train_loss = 0.733, time/batch = 0.094\n",
            "20118/22300 (epoch 45), train_loss = 0.766, time/batch = 0.093\n",
            "20119/22300 (epoch 45), train_loss = 0.765, time/batch = 0.091\n",
            "20120/22300 (epoch 45), train_loss = 0.784, time/batch = 0.093\n",
            "20121/22300 (epoch 45), train_loss = 0.735, time/batch = 0.093\n",
            "20122/22300 (epoch 45), train_loss = 0.755, time/batch = 0.128\n",
            "20123/22300 (epoch 45), train_loss = 0.760, time/batch = 0.092\n",
            "20124/22300 (epoch 45), train_loss = 0.743, time/batch = 0.091\n",
            "20125/22300 (epoch 45), train_loss = 0.748, time/batch = 0.092\n",
            "20126/22300 (epoch 45), train_loss = 0.748, time/batch = 0.092\n",
            "20127/22300 (epoch 45), train_loss = 0.786, time/batch = 0.094\n",
            "20128/22300 (epoch 45), train_loss = 0.750, time/batch = 0.092\n",
            "20129/22300 (epoch 45), train_loss = 0.729, time/batch = 0.092\n",
            "20130/22300 (epoch 45), train_loss = 0.775, time/batch = 0.091\n",
            "20131/22300 (epoch 45), train_loss = 0.732, time/batch = 0.096\n",
            "20132/22300 (epoch 45), train_loss = 0.717, time/batch = 0.098\n",
            "20133/22300 (epoch 45), train_loss = 0.765, time/batch = 0.091\n",
            "20134/22300 (epoch 45), train_loss = 0.744, time/batch = 0.091\n",
            "20135/22300 (epoch 45), train_loss = 0.754, time/batch = 0.091\n",
            "20136/22300 (epoch 45), train_loss = 0.763, time/batch = 0.093\n",
            "20137/22300 (epoch 45), train_loss = 0.805, time/batch = 0.092\n",
            "20138/22300 (epoch 45), train_loss = 0.768, time/batch = 0.092\n",
            "20139/22300 (epoch 45), train_loss = 0.775, time/batch = 0.092\n",
            "20140/22300 (epoch 45), train_loss = 0.768, time/batch = 0.091\n",
            "20141/22300 (epoch 45), train_loss = 0.707, time/batch = 0.094\n",
            "20142/22300 (epoch 45), train_loss = 0.737, time/batch = 0.094\n",
            "20143/22300 (epoch 45), train_loss = 0.726, time/batch = 0.093\n",
            "20144/22300 (epoch 45), train_loss = 0.711, time/batch = 0.097\n",
            "20145/22300 (epoch 45), train_loss = 0.769, time/batch = 0.094\n",
            "20146/22300 (epoch 45), train_loss = 0.758, time/batch = 0.091\n",
            "20147/22300 (epoch 45), train_loss = 0.747, time/batch = 0.094\n",
            "20148/22300 (epoch 45), train_loss = 0.735, time/batch = 0.091\n",
            "20149/22300 (epoch 45), train_loss = 0.786, time/batch = 0.092\n",
            "20150/22300 (epoch 45), train_loss = 0.765, time/batch = 0.091\n",
            "20151/22300 (epoch 45), train_loss = 0.739, time/batch = 0.093\n",
            "20152/22300 (epoch 45), train_loss = 0.766, time/batch = 0.093\n",
            "20153/22300 (epoch 45), train_loss = 0.755, time/batch = 0.092\n",
            "20154/22300 (epoch 45), train_loss = 0.771, time/batch = 0.094\n",
            "20155/22300 (epoch 45), train_loss = 0.761, time/batch = 0.092\n",
            "20156/22300 (epoch 45), train_loss = 0.746, time/batch = 0.093\n",
            "20157/22300 (epoch 45), train_loss = 0.782, time/batch = 0.091\n",
            "20158/22300 (epoch 45), train_loss = 0.747, time/batch = 0.093\n",
            "20159/22300 (epoch 45), train_loss = 0.783, time/batch = 0.093\n",
            "20160/22300 (epoch 45), train_loss = 0.730, time/batch = 0.091\n",
            "20161/22300 (epoch 45), train_loss = 0.759, time/batch = 0.092\n",
            "20162/22300 (epoch 45), train_loss = 0.777, time/batch = 0.093\n",
            "20163/22300 (epoch 45), train_loss = 0.754, time/batch = 0.093\n",
            "20164/22300 (epoch 45), train_loss = 0.766, time/batch = 0.093\n",
            "20165/22300 (epoch 45), train_loss = 0.777, time/batch = 0.093\n",
            "20166/22300 (epoch 45), train_loss = 0.734, time/batch = 0.099\n",
            "20167/22300 (epoch 45), train_loss = 0.785, time/batch = 0.094\n",
            "20168/22300 (epoch 45), train_loss = 0.725, time/batch = 0.091\n",
            "20169/22300 (epoch 45), train_loss = 0.753, time/batch = 0.092\n",
            "20170/22300 (epoch 45), train_loss = 0.749, time/batch = 0.091\n",
            "20171/22300 (epoch 45), train_loss = 0.754, time/batch = 0.099\n",
            "20172/22300 (epoch 45), train_loss = 0.757, time/batch = 0.093\n",
            "20173/22300 (epoch 45), train_loss = 0.723, time/batch = 0.092\n",
            "20174/22300 (epoch 45), train_loss = 0.751, time/batch = 0.093\n",
            "20175/22300 (epoch 45), train_loss = 0.716, time/batch = 0.093\n",
            "20176/22300 (epoch 45), train_loss = 0.700, time/batch = 0.096\n",
            "20177/22300 (epoch 45), train_loss = 0.723, time/batch = 0.093\n",
            "20178/22300 (epoch 45), train_loss = 0.724, time/batch = 0.092\n",
            "20179/22300 (epoch 45), train_loss = 0.727, time/batch = 0.093\n",
            "20180/22300 (epoch 45), train_loss = 0.703, time/batch = 0.093\n",
            "20181/22300 (epoch 45), train_loss = 0.699, time/batch = 0.091\n",
            "20182/22300 (epoch 45), train_loss = 0.734, time/batch = 0.092\n",
            "20183/22300 (epoch 45), train_loss = 0.696, time/batch = 0.093\n",
            "20184/22300 (epoch 45), train_loss = 0.742, time/batch = 0.099\n",
            "20185/22300 (epoch 45), train_loss = 0.716, time/batch = 0.093\n",
            "20186/22300 (epoch 45), train_loss = 0.716, time/batch = 0.093\n",
            "20187/22300 (epoch 45), train_loss = 0.750, time/batch = 0.092\n",
            "20188/22300 (epoch 45), train_loss = 0.771, time/batch = 0.093\n",
            "20189/22300 (epoch 45), train_loss = 0.734, time/batch = 0.096\n",
            "20190/22300 (epoch 45), train_loss = 0.777, time/batch = 0.092\n",
            "20191/22300 (epoch 45), train_loss = 0.745, time/batch = 0.092\n",
            "20192/22300 (epoch 45), train_loss = 0.738, time/batch = 0.092\n",
            "20193/22300 (epoch 45), train_loss = 0.731, time/batch = 0.093\n",
            "20194/22300 (epoch 45), train_loss = 0.763, time/batch = 0.091\n",
            "20195/22300 (epoch 45), train_loss = 0.766, time/batch = 0.093\n",
            "20196/22300 (epoch 45), train_loss = 0.738, time/batch = 0.094\n",
            "20197/22300 (epoch 45), train_loss = 0.769, time/batch = 0.092\n",
            "20198/22300 (epoch 45), train_loss = 0.772, time/batch = 0.095\n",
            "20199/22300 (epoch 45), train_loss = 0.755, time/batch = 0.092\n",
            "20200/22300 (epoch 45), train_loss = 0.741, time/batch = 0.092\n",
            "20201/22300 (epoch 45), train_loss = 0.746, time/batch = 0.091\n",
            "20202/22300 (epoch 45), train_loss = 0.758, time/batch = 0.092\n",
            "20203/22300 (epoch 45), train_loss = 0.769, time/batch = 0.092\n",
            "20204/22300 (epoch 45), train_loss = 0.741, time/batch = 0.092\n",
            "20205/22300 (epoch 45), train_loss = 0.778, time/batch = 0.092\n",
            "20206/22300 (epoch 45), train_loss = 0.777, time/batch = 0.092\n",
            "20207/22300 (epoch 45), train_loss = 0.771, time/batch = 0.093\n",
            "20208/22300 (epoch 45), train_loss = 0.764, time/batch = 0.092\n",
            "20209/22300 (epoch 45), train_loss = 0.785, time/batch = 0.092\n",
            "20210/22300 (epoch 45), train_loss = 0.748, time/batch = 0.092\n",
            "20211/22300 (epoch 45), train_loss = 0.762, time/batch = 0.092\n",
            "20212/22300 (epoch 45), train_loss = 0.742, time/batch = 0.093\n",
            "20213/22300 (epoch 45), train_loss = 0.755, time/batch = 0.092\n",
            "20214/22300 (epoch 45), train_loss = 0.765, time/batch = 0.093\n",
            "20215/22300 (epoch 45), train_loss = 0.818, time/batch = 0.092\n",
            "20216/22300 (epoch 45), train_loss = 0.773, time/batch = 0.093\n",
            "20217/22300 (epoch 45), train_loss = 0.752, time/batch = 0.094\n",
            "20218/22300 (epoch 45), train_loss = 0.761, time/batch = 0.100\n",
            "20219/22300 (epoch 45), train_loss = 0.760, time/batch = 0.092\n",
            "20220/22300 (epoch 45), train_loss = 0.764, time/batch = 0.093\n",
            "20221/22300 (epoch 45), train_loss = 0.769, time/batch = 0.092\n",
            "20222/22300 (epoch 45), train_loss = 0.748, time/batch = 0.094\n",
            "20223/22300 (epoch 45), train_loss = 0.723, time/batch = 0.092\n",
            "20224/22300 (epoch 45), train_loss = 0.761, time/batch = 0.093\n",
            "20225/22300 (epoch 45), train_loss = 0.746, time/batch = 0.093\n",
            "20226/22300 (epoch 45), train_loss = 0.757, time/batch = 0.093\n",
            "20227/22300 (epoch 45), train_loss = 0.744, time/batch = 0.093\n",
            "20228/22300 (epoch 45), train_loss = 0.788, time/batch = 0.100\n",
            "20229/22300 (epoch 45), train_loss = 0.754, time/batch = 0.092\n",
            "20230/22300 (epoch 45), train_loss = 0.740, time/batch = 0.092\n",
            "20231/22300 (epoch 45), train_loss = 0.774, time/batch = 0.092\n",
            "20232/22300 (epoch 45), train_loss = 0.756, time/batch = 0.093\n",
            "20233/22300 (epoch 45), train_loss = 0.749, time/batch = 0.091\n",
            "20234/22300 (epoch 45), train_loss = 0.761, time/batch = 0.093\n",
            "20235/22300 (epoch 45), train_loss = 0.759, time/batch = 0.092\n",
            "20236/22300 (epoch 45), train_loss = 0.721, time/batch = 0.092\n",
            "20237/22300 (epoch 45), train_loss = 0.733, time/batch = 0.094\n",
            "20238/22300 (epoch 45), train_loss = 0.728, time/batch = 0.097\n",
            "20239/22300 (epoch 45), train_loss = 0.757, time/batch = 0.090\n",
            "20240/22300 (epoch 45), train_loss = 0.751, time/batch = 0.093\n",
            "20241/22300 (epoch 45), train_loss = 0.742, time/batch = 0.091\n",
            "20242/22300 (epoch 45), train_loss = 0.732, time/batch = 0.092\n",
            "20243/22300 (epoch 45), train_loss = 0.730, time/batch = 0.091\n",
            "20244/22300 (epoch 45), train_loss = 0.742, time/batch = 0.092\n",
            "20245/22300 (epoch 45), train_loss = 0.722, time/batch = 0.092\n",
            "20246/22300 (epoch 45), train_loss = 0.702, time/batch = 0.092\n",
            "20247/22300 (epoch 45), train_loss = 0.727, time/batch = 0.093\n",
            "20248/22300 (epoch 45), train_loss = 0.673, time/batch = 0.093\n",
            "20249/22300 (epoch 45), train_loss = 0.749, time/batch = 0.094\n",
            "20250/22300 (epoch 45), train_loss = 0.737, time/batch = 0.091\n",
            "20251/22300 (epoch 45), train_loss = 0.706, time/batch = 0.092\n",
            "20252/22300 (epoch 45), train_loss = 0.741, time/batch = 0.093\n",
            "20253/22300 (epoch 45), train_loss = 0.715, time/batch = 0.092\n",
            "20254/22300 (epoch 45), train_loss = 0.694, time/batch = 0.093\n",
            "20255/22300 (epoch 45), train_loss = 0.682, time/batch = 0.092\n",
            "20256/22300 (epoch 45), train_loss = 0.745, time/batch = 0.091\n",
            "20257/22300 (epoch 45), train_loss = 0.722, time/batch = 0.092\n",
            "20258/22300 (epoch 45), train_loss = 0.752, time/batch = 0.093\n",
            "20259/22300 (epoch 45), train_loss = 0.730, time/batch = 0.094\n",
            "20260/22300 (epoch 45), train_loss = 0.751, time/batch = 0.092\n",
            "20261/22300 (epoch 45), train_loss = 0.750, time/batch = 0.096\n",
            "20262/22300 (epoch 45), train_loss = 0.746, time/batch = 0.090\n",
            "20263/22300 (epoch 45), train_loss = 0.763, time/batch = 0.092\n",
            "20264/22300 (epoch 45), train_loss = 0.752, time/batch = 0.093\n",
            "20265/22300 (epoch 45), train_loss = 0.729, time/batch = 0.092\n",
            "20266/22300 (epoch 45), train_loss = 0.751, time/batch = 0.093\n",
            "20267/22300 (epoch 45), train_loss = 0.730, time/batch = 0.092\n",
            "20268/22300 (epoch 45), train_loss = 0.776, time/batch = 0.092\n",
            "20269/22300 (epoch 45), train_loss = 0.742, time/batch = 0.093\n",
            "20270/22300 (epoch 45), train_loss = 0.764, time/batch = 0.093\n",
            "20271/22300 (epoch 45), train_loss = 0.731, time/batch = 0.094\n",
            "20272/22300 (epoch 45), train_loss = 0.724, time/batch = 0.092\n",
            "20273/22300 (epoch 45), train_loss = 0.729, time/batch = 0.093\n",
            "20274/22300 (epoch 45), train_loss = 0.726, time/batch = 0.092\n",
            "20275/22300 (epoch 45), train_loss = 0.742, time/batch = 0.092\n",
            "20276/22300 (epoch 45), train_loss = 0.784, time/batch = 0.093\n",
            "20277/22300 (epoch 45), train_loss = 0.740, time/batch = 0.091\n",
            "20278/22300 (epoch 45), train_loss = 0.790, time/batch = 0.093\n",
            "20279/22300 (epoch 45), train_loss = 0.729, time/batch = 0.092\n",
            "20280/22300 (epoch 45), train_loss = 0.737, time/batch = 0.092\n",
            "20281/22300 (epoch 45), train_loss = 0.723, time/batch = 0.092\n",
            "20282/22300 (epoch 45), train_loss = 0.746, time/batch = 0.092\n",
            "20283/22300 (epoch 45), train_loss = 0.744, time/batch = 0.093\n",
            "20284/22300 (epoch 45), train_loss = 0.714, time/batch = 0.091\n",
            "20285/22300 (epoch 45), train_loss = 0.728, time/batch = 0.092\n",
            "20286/22300 (epoch 45), train_loss = 0.721, time/batch = 0.092\n",
            "20287/22300 (epoch 45), train_loss = 0.748, time/batch = 0.093\n",
            "20288/22300 (epoch 45), train_loss = 0.749, time/batch = 0.102\n",
            "20289/22300 (epoch 45), train_loss = 0.748, time/batch = 0.092\n",
            "20290/22300 (epoch 45), train_loss = 0.774, time/batch = 0.093\n",
            "20291/22300 (epoch 45), train_loss = 0.738, time/batch = 0.093\n",
            "20292/22300 (epoch 45), train_loss = 0.745, time/batch = 0.092\n",
            "20293/22300 (epoch 45), train_loss = 0.782, time/batch = 0.092\n",
            "20294/22300 (epoch 45), train_loss = 0.723, time/batch = 0.092\n",
            "20295/22300 (epoch 45), train_loss = 0.754, time/batch = 0.092\n",
            "20296/22300 (epoch 45), train_loss = 0.779, time/batch = 0.093\n",
            "20297/22300 (epoch 45), train_loss = 0.766, time/batch = 0.094\n",
            "20298/22300 (epoch 45), train_loss = 0.779, time/batch = 0.093\n",
            "20299/22300 (epoch 45), train_loss = 0.789, time/batch = 0.094\n",
            "20300/22300 (epoch 45), train_loss = 0.780, time/batch = 0.092\n",
            "20301/22300 (epoch 45), train_loss = 0.777, time/batch = 0.099\n",
            "20302/22300 (epoch 45), train_loss = 0.749, time/batch = 0.090\n",
            "20303/22300 (epoch 45), train_loss = 0.756, time/batch = 0.093\n",
            "20304/22300 (epoch 45), train_loss = 0.768, time/batch = 0.093\n",
            "20305/22300 (epoch 45), train_loss = 0.747, time/batch = 0.092\n",
            "20306/22300 (epoch 45), train_loss = 0.743, time/batch = 0.092\n",
            "20307/22300 (epoch 45), train_loss = 0.730, time/batch = 0.092\n",
            "20308/22300 (epoch 45), train_loss = 0.745, time/batch = 0.093\n",
            "20309/22300 (epoch 45), train_loss = 0.762, time/batch = 0.094\n",
            "20310/22300 (epoch 45), train_loss = 0.788, time/batch = 0.092\n",
            "20311/22300 (epoch 45), train_loss = 0.753, time/batch = 0.093\n",
            "20312/22300 (epoch 45), train_loss = 0.774, time/batch = 0.092\n",
            "20313/22300 (epoch 45), train_loss = 0.783, time/batch = 0.093\n",
            "20314/22300 (epoch 45), train_loss = 0.739, time/batch = 0.097\n",
            "20315/22300 (epoch 45), train_loss = 0.781, time/batch = 0.091\n",
            "20316/22300 (epoch 45), train_loss = 0.789, time/batch = 0.092\n",
            "20317/22300 (epoch 45), train_loss = 0.746, time/batch = 0.092\n",
            "20318/22300 (epoch 45), train_loss = 0.777, time/batch = 0.096\n",
            "20319/22300 (epoch 45), train_loss = 0.770, time/batch = 0.093\n",
            "20320/22300 (epoch 45), train_loss = 0.757, time/batch = 0.093\n",
            "20321/22300 (epoch 45), train_loss = 0.740, time/batch = 0.093\n",
            "20322/22300 (epoch 45), train_loss = 0.756, time/batch = 0.093\n",
            "20323/22300 (epoch 45), train_loss = 0.753, time/batch = 0.091\n",
            "20324/22300 (epoch 45), train_loss = 0.736, time/batch = 0.094\n",
            "20325/22300 (epoch 45), train_loss = 0.766, time/batch = 0.092\n",
            "20326/22300 (epoch 45), train_loss = 0.751, time/batch = 0.093\n",
            "20327/22300 (epoch 45), train_loss = 0.786, time/batch = 0.093\n",
            "20328/22300 (epoch 45), train_loss = 0.747, time/batch = 0.092\n",
            "20329/22300 (epoch 45), train_loss = 0.762, time/batch = 0.096\n",
            "20330/22300 (epoch 45), train_loss = 0.736, time/batch = 0.093\n",
            "20331/22300 (epoch 45), train_loss = 0.790, time/batch = 0.092\n",
            "20332/22300 (epoch 45), train_loss = 0.738, time/batch = 0.094\n",
            "20333/22300 (epoch 45), train_loss = 0.783, time/batch = 0.091\n",
            "20334/22300 (epoch 45), train_loss = 0.756, time/batch = 0.094\n",
            "20335/22300 (epoch 45), train_loss = 0.782, time/batch = 0.092\n",
            "20336/22300 (epoch 45), train_loss = 0.785, time/batch = 0.092\n",
            "20337/22300 (epoch 45), train_loss = 0.802, time/batch = 0.093\n",
            "20338/22300 (epoch 45), train_loss = 0.781, time/batch = 0.092\n",
            "20339/22300 (epoch 45), train_loss = 0.772, time/batch = 0.093\n",
            "20340/22300 (epoch 45), train_loss = 0.792, time/batch = 0.092\n",
            "20341/22300 (epoch 45), train_loss = 0.770, time/batch = 0.092\n",
            "20342/22300 (epoch 45), train_loss = 0.793, time/batch = 0.094\n",
            "20343/22300 (epoch 45), train_loss = 0.797, time/batch = 0.092\n",
            "20344/22300 (epoch 45), train_loss = 0.786, time/batch = 0.094\n",
            "20345/22300 (epoch 45), train_loss = 0.785, time/batch = 0.092\n",
            "20346/22300 (epoch 45), train_loss = 0.819, time/batch = 0.093\n",
            "20347/22300 (epoch 45), train_loss = 0.768, time/batch = 0.092\n",
            "20348/22300 (epoch 45), train_loss = 0.751, time/batch = 0.093\n",
            "20349/22300 (epoch 45), train_loss = 0.766, time/batch = 0.094\n",
            "20350/22300 (epoch 45), train_loss = 0.773, time/batch = 0.092\n",
            "20351/22300 (epoch 45), train_loss = 0.814, time/batch = 0.092\n",
            "20352/22300 (epoch 45), train_loss = 0.783, time/batch = 0.092\n",
            "20353/22300 (epoch 45), train_loss = 0.766, time/batch = 0.093\n",
            "20354/22300 (epoch 45), train_loss = 0.773, time/batch = 0.093\n",
            "20355/22300 (epoch 45), train_loss = 0.767, time/batch = 0.092\n",
            "20356/22300 (epoch 45), train_loss = 0.790, time/batch = 0.094\n",
            "20357/22300 (epoch 45), train_loss = 0.780, time/batch = 0.097\n",
            "20358/22300 (epoch 45), train_loss = 0.769, time/batch = 0.093\n",
            "20359/22300 (epoch 45), train_loss = 0.769, time/batch = 0.094\n",
            "20360/22300 (epoch 45), train_loss = 0.736, time/batch = 0.092\n",
            "20361/22300 (epoch 45), train_loss = 0.751, time/batch = 0.093\n",
            "20362/22300 (epoch 45), train_loss = 0.789, time/batch = 0.092\n",
            "20363/22300 (epoch 45), train_loss = 0.778, time/batch = 0.092\n",
            "20364/22300 (epoch 45), train_loss = 0.787, time/batch = 0.094\n",
            "20365/22300 (epoch 45), train_loss = 0.773, time/batch = 0.092\n",
            "20366/22300 (epoch 45), train_loss = 0.763, time/batch = 0.092\n",
            "20367/22300 (epoch 45), train_loss = 0.771, time/batch = 0.093\n",
            "20368/22300 (epoch 45), train_loss = 0.759, time/batch = 0.092\n",
            "20369/22300 (epoch 45), train_loss = 0.784, time/batch = 0.093\n",
            "20370/22300 (epoch 45), train_loss = 0.738, time/batch = 0.092\n",
            "20371/22300 (epoch 45), train_loss = 0.768, time/batch = 0.093\n",
            "20372/22300 (epoch 45), train_loss = 0.735, time/batch = 0.092\n",
            "20373/22300 (epoch 45), train_loss = 0.752, time/batch = 0.092\n",
            "20374/22300 (epoch 45), train_loss = 0.768, time/batch = 0.093\n",
            "20375/22300 (epoch 45), train_loss = 0.753, time/batch = 0.092\n",
            "20376/22300 (epoch 45), train_loss = 0.731, time/batch = 0.092\n",
            "20377/22300 (epoch 45), train_loss = 0.779, time/batch = 0.092\n",
            "20378/22300 (epoch 45), train_loss = 0.762, time/batch = 0.093\n",
            "20379/22300 (epoch 45), train_loss = 0.765, time/batch = 0.092\n",
            "20380/22300 (epoch 45), train_loss = 0.732, time/batch = 0.093\n",
            "20381/22300 (epoch 45), train_loss = 0.739, time/batch = 0.101\n",
            "20382/22300 (epoch 45), train_loss = 0.743, time/batch = 0.093\n",
            "20383/22300 (epoch 45), train_loss = 0.762, time/batch = 0.093\n",
            "20384/22300 (epoch 45), train_loss = 0.747, time/batch = 0.093\n",
            "20385/22300 (epoch 45), train_loss = 0.751, time/batch = 0.091\n",
            "20386/22300 (epoch 45), train_loss = 0.730, time/batch = 0.092\n",
            "20387/22300 (epoch 45), train_loss = 0.775, time/batch = 0.092\n",
            "20388/22300 (epoch 45), train_loss = 0.726, time/batch = 0.093\n",
            "20389/22300 (epoch 45), train_loss = 0.718, time/batch = 0.100\n",
            "20390/22300 (epoch 45), train_loss = 0.735, time/batch = 0.092\n",
            "20391/22300 (epoch 45), train_loss = 0.780, time/batch = 0.092\n",
            "20392/22300 (epoch 45), train_loss = 0.758, time/batch = 0.093\n",
            "20393/22300 (epoch 45), train_loss = 0.720, time/batch = 0.092\n",
            "20394/22300 (epoch 45), train_loss = 0.728, time/batch = 0.094\n",
            "20395/22300 (epoch 45), train_loss = 0.759, time/batch = 0.092\n",
            "20396/22300 (epoch 45), train_loss = 0.756, time/batch = 0.099\n",
            "20397/22300 (epoch 45), train_loss = 0.757, time/batch = 0.092\n",
            "20398/22300 (epoch 45), train_loss = 0.799, time/batch = 0.093\n",
            "20399/22300 (epoch 45), train_loss = 0.754, time/batch = 0.099\n",
            "20400/22300 (epoch 45), train_loss = 0.726, time/batch = 0.093\n",
            "20401/22300 (epoch 45), train_loss = 0.749, time/batch = 0.095\n",
            "20402/22300 (epoch 45), train_loss = 0.737, time/batch = 0.092\n",
            "20403/22300 (epoch 45), train_loss = 0.707, time/batch = 0.092\n",
            "20404/22300 (epoch 45), train_loss = 0.737, time/batch = 0.093\n",
            "20405/22300 (epoch 45), train_loss = 0.745, time/batch = 0.093\n",
            "20406/22300 (epoch 45), train_loss = 0.753, time/batch = 0.093\n",
            "20407/22300 (epoch 45), train_loss = 0.733, time/batch = 0.092\n",
            "20408/22300 (epoch 45), train_loss = 0.758, time/batch = 0.092\n",
            "20409/22300 (epoch 45), train_loss = 0.769, time/batch = 0.092\n",
            "20410/22300 (epoch 45), train_loss = 0.775, time/batch = 0.094\n",
            "20411/22300 (epoch 45), train_loss = 0.738, time/batch = 0.100\n",
            "20412/22300 (epoch 45), train_loss = 0.727, time/batch = 0.092\n",
            "20413/22300 (epoch 45), train_loss = 0.750, time/batch = 0.093\n",
            "20414/22300 (epoch 45), train_loss = 0.743, time/batch = 0.092\n",
            "20415/22300 (epoch 45), train_loss = 0.750, time/batch = 0.092\n",
            "20416/22300 (epoch 45), train_loss = 0.724, time/batch = 0.093\n",
            "20417/22300 (epoch 45), train_loss = 0.764, time/batch = 0.091\n",
            "20418/22300 (epoch 45), train_loss = 0.720, time/batch = 0.093\n",
            "20419/22300 (epoch 45), train_loss = 0.752, time/batch = 0.092\n",
            "20420/22300 (epoch 45), train_loss = 0.744, time/batch = 0.093\n",
            "20421/22300 (epoch 45), train_loss = 0.755, time/batch = 0.095\n",
            "20422/22300 (epoch 45), train_loss = 0.736, time/batch = 0.091\n",
            "20423/22300 (epoch 45), train_loss = 0.739, time/batch = 0.092\n",
            "20424/22300 (epoch 45), train_loss = 0.755, time/batch = 0.092\n",
            "20425/22300 (epoch 45), train_loss = 0.727, time/batch = 0.091\n",
            "20426/22300 (epoch 45), train_loss = 0.774, time/batch = 0.091\n",
            "20427/22300 (epoch 45), train_loss = 0.770, time/batch = 0.092\n",
            "20428/22300 (epoch 45), train_loss = 0.759, time/batch = 0.092\n",
            "20429/22300 (epoch 45), train_loss = 0.752, time/batch = 0.092\n",
            "20430/22300 (epoch 45), train_loss = 0.754, time/batch = 0.093\n",
            "20431/22300 (epoch 45), train_loss = 0.749, time/batch = 0.093\n",
            "20432/22300 (epoch 45), train_loss = 0.747, time/batch = 0.092\n",
            "20433/22300 (epoch 45), train_loss = 0.755, time/batch = 0.093\n",
            "20434/22300 (epoch 45), train_loss = 0.775, time/batch = 0.092\n",
            "20435/22300 (epoch 45), train_loss = 0.752, time/batch = 0.093\n",
            "20436/22300 (epoch 45), train_loss = 0.747, time/batch = 0.091\n",
            "20437/22300 (epoch 45), train_loss = 0.735, time/batch = 0.092\n",
            "20438/22300 (epoch 45), train_loss = 0.736, time/batch = 0.092\n",
            "20439/22300 (epoch 45), train_loss = 0.758, time/batch = 0.092\n",
            "20440/22300 (epoch 45), train_loss = 0.783, time/batch = 0.093\n",
            "20441/22300 (epoch 45), train_loss = 0.713, time/batch = 0.093\n",
            "20442/22300 (epoch 45), train_loss = 0.775, time/batch = 0.093\n",
            "20443/22300 (epoch 45), train_loss = 0.740, time/batch = 0.092\n",
            "20444/22300 (epoch 45), train_loss = 0.767, time/batch = 0.093\n",
            "20445/22300 (epoch 45), train_loss = 0.743, time/batch = 0.092\n",
            "20446/22300 (epoch 45), train_loss = 0.738, time/batch = 0.092\n",
            "20447/22300 (epoch 45), train_loss = 0.759, time/batch = 0.094\n",
            "20448/22300 (epoch 45), train_loss = 0.718, time/batch = 0.092\n",
            "20449/22300 (epoch 45), train_loss = 0.700, time/batch = 0.093\n",
            "20450/22300 (epoch 45), train_loss = 0.717, time/batch = 0.091\n",
            "20451/22300 (epoch 45), train_loss = 0.723, time/batch = 0.092\n",
            "20452/22300 (epoch 45), train_loss = 0.698, time/batch = 0.092\n",
            "20453/22300 (epoch 45), train_loss = 0.685, time/batch = 0.092\n",
            "20454/22300 (epoch 45), train_loss = 0.717, time/batch = 0.093\n",
            "20455/22300 (epoch 45), train_loss = 0.723, time/batch = 0.092\n",
            "20456/22300 (epoch 45), train_loss = 0.733, time/batch = 0.093\n",
            "20457/22300 (epoch 45), train_loss = 0.755, time/batch = 0.092\n",
            "20458/22300 (epoch 45), train_loss = 0.730, time/batch = 0.092\n",
            "20459/22300 (epoch 45), train_loss = 0.718, time/batch = 0.091\n",
            "20460/22300 (epoch 45), train_loss = 0.722, time/batch = 0.093\n",
            "20461/22300 (epoch 45), train_loss = 0.713, time/batch = 0.093\n",
            "20462/22300 (epoch 45), train_loss = 0.731, time/batch = 0.092\n",
            "20463/22300 (epoch 45), train_loss = 0.770, time/batch = 0.093\n",
            "20464/22300 (epoch 45), train_loss = 0.716, time/batch = 0.092\n",
            "20465/22300 (epoch 45), train_loss = 0.740, time/batch = 0.093\n",
            "20466/22300 (epoch 45), train_loss = 0.746, time/batch = 0.094\n",
            "20467/22300 (epoch 45), train_loss = 0.730, time/batch = 0.092\n",
            "20468/22300 (epoch 45), train_loss = 0.753, time/batch = 0.093\n",
            "20469/22300 (epoch 45), train_loss = 0.713, time/batch = 0.091\n",
            "20470/22300 (epoch 45), train_loss = 0.742, time/batch = 0.092\n",
            "20471/22300 (epoch 45), train_loss = 0.765, time/batch = 0.102\n",
            "20472/22300 (epoch 45), train_loss = 0.742, time/batch = 0.089\n",
            "20473/22300 (epoch 45), train_loss = 0.735, time/batch = 0.092\n",
            "20474/22300 (epoch 45), train_loss = 0.735, time/batch = 0.092\n",
            "20475/22300 (epoch 45), train_loss = 0.744, time/batch = 0.093\n",
            "20476/22300 (epoch 45), train_loss = 0.734, time/batch = 0.095\n",
            "20477/22300 (epoch 45), train_loss = 0.770, time/batch = 0.092\n",
            "20478/22300 (epoch 45), train_loss = 0.740, time/batch = 0.093\n",
            "20479/22300 (epoch 45), train_loss = 0.766, time/batch = 0.093\n",
            "20480/22300 (epoch 45), train_loss = 0.780, time/batch = 0.092\n",
            "20481/22300 (epoch 45), train_loss = 0.743, time/batch = 0.094\n",
            "20482/22300 (epoch 45), train_loss = 0.764, time/batch = 0.092\n",
            "20483/22300 (epoch 45), train_loss = 0.732, time/batch = 0.093\n",
            "20484/22300 (epoch 45), train_loss = 0.769, time/batch = 0.091\n",
            "20485/22300 (epoch 45), train_loss = 0.781, time/batch = 0.092\n",
            "20486/22300 (epoch 45), train_loss = 0.764, time/batch = 0.092\n",
            "20487/22300 (epoch 45), train_loss = 0.775, time/batch = 0.092\n",
            "20488/22300 (epoch 45), train_loss = 0.769, time/batch = 0.094\n",
            "20489/22300 (epoch 45), train_loss = 0.754, time/batch = 0.092\n",
            "20490/22300 (epoch 45), train_loss = 0.746, time/batch = 0.093\n",
            "20491/22300 (epoch 45), train_loss = 0.734, time/batch = 0.092\n",
            "20492/22300 (epoch 45), train_loss = 0.750, time/batch = 0.092\n",
            "20493/22300 (epoch 45), train_loss = 0.736, time/batch = 0.093\n",
            "20494/22300 (epoch 45), train_loss = 0.762, time/batch = 0.092\n",
            "20495/22300 (epoch 45), train_loss = 0.748, time/batch = 0.092\n",
            "20496/22300 (epoch 45), train_loss = 0.800, time/batch = 0.091\n",
            "20497/22300 (epoch 45), train_loss = 0.731, time/batch = 0.092\n",
            "20498/22300 (epoch 45), train_loss = 0.762, time/batch = 0.092\n",
            "20499/22300 (epoch 45), train_loss = 0.730, time/batch = 0.091\n",
            "20500/22300 (epoch 45), train_loss = 0.749, time/batch = 0.092\n",
            "20501/22300 (epoch 45), train_loss = 0.785, time/batch = 0.092\n",
            "20502/22300 (epoch 45), train_loss = 0.763, time/batch = 0.093\n",
            "20503/22300 (epoch 45), train_loss = 0.768, time/batch = 0.092\n",
            "20504/22300 (epoch 45), train_loss = 0.728, time/batch = 0.093\n",
            "20505/22300 (epoch 45), train_loss = 0.726, time/batch = 0.092\n",
            "20506/22300 (epoch 45), train_loss = 0.707, time/batch = 0.093\n",
            "20507/22300 (epoch 45), train_loss = 0.726, time/batch = 0.092\n",
            "20508/22300 (epoch 45), train_loss = 0.796, time/batch = 0.097\n",
            "20509/22300 (epoch 45), train_loss = 0.733, time/batch = 0.097\n",
            "20510/22300 (epoch 45), train_loss = 0.747, time/batch = 0.092\n",
            "20511/22300 (epoch 45), train_loss = 0.736, time/batch = 0.091\n",
            "20512/22300 (epoch 45), train_loss = 0.749, time/batch = 0.094\n",
            "20513/22300 (epoch 45), train_loss = 0.746, time/batch = 0.096\n",
            "20514/22300 (epoch 45), train_loss = 0.752, time/batch = 0.092\n",
            "20515/22300 (epoch 45), train_loss = 0.734, time/batch = 0.092\n",
            "20516/22300 (epoch 46), train_loss = 0.423, time/batch = 0.085\n",
            "20517/22300 (epoch 46), train_loss = 0.738, time/batch = 0.093\n",
            "20518/22300 (epoch 46), train_loss = 0.774, time/batch = 0.091\n",
            "20519/22300 (epoch 46), train_loss = 0.788, time/batch = 0.091\n",
            "20520/22300 (epoch 46), train_loss = 0.807, time/batch = 0.092\n",
            "20521/22300 (epoch 46), train_loss = 0.750, time/batch = 0.091\n",
            "20522/22300 (epoch 46), train_loss = 0.778, time/batch = 0.091\n",
            "20523/22300 (epoch 46), train_loss = 0.781, time/batch = 0.091\n",
            "20524/22300 (epoch 46), train_loss = 0.744, time/batch = 0.091\n",
            "20525/22300 (epoch 46), train_loss = 0.735, time/batch = 0.100\n",
            "20526/22300 (epoch 46), train_loss = 0.800, time/batch = 0.093\n",
            "20527/22300 (epoch 46), train_loss = 0.738, time/batch = 0.091\n",
            "20528/22300 (epoch 46), train_loss = 0.778, time/batch = 0.092\n",
            "20529/22300 (epoch 46), train_loss = 0.786, time/batch = 0.091\n",
            "20530/22300 (epoch 46), train_loss = 0.763, time/batch = 0.093\n",
            "20531/22300 (epoch 46), train_loss = 0.792, time/batch = 0.091\n",
            "20532/22300 (epoch 46), train_loss = 0.808, time/batch = 0.093\n",
            "20533/22300 (epoch 46), train_loss = 0.763, time/batch = 0.091\n",
            "20534/22300 (epoch 46), train_loss = 0.769, time/batch = 0.092\n",
            "20535/22300 (epoch 46), train_loss = 0.799, time/batch = 0.095\n",
            "20536/22300 (epoch 46), train_loss = 0.770, time/batch = 0.094\n",
            "20537/22300 (epoch 46), train_loss = 0.760, time/batch = 0.096\n",
            "20538/22300 (epoch 46), train_loss = 0.754, time/batch = 0.095\n",
            "20539/22300 (epoch 46), train_loss = 0.744, time/batch = 0.092\n",
            "20540/22300 (epoch 46), train_loss = 0.712, time/batch = 0.092\n",
            "20541/22300 (epoch 46), train_loss = 0.744, time/batch = 0.091\n",
            "20542/22300 (epoch 46), train_loss = 0.751, time/batch = 0.091\n",
            "20543/22300 (epoch 46), train_loss = 0.732, time/batch = 0.091\n",
            "20544/22300 (epoch 46), train_loss = 0.763, time/batch = 0.091\n",
            "20545/22300 (epoch 46), train_loss = 0.733, time/batch = 0.092\n",
            "20546/22300 (epoch 46), train_loss = 0.730, time/batch = 0.096\n",
            "20547/22300 (epoch 46), train_loss = 0.756, time/batch = 0.095\n",
            "20548/22300 (epoch 46), train_loss = 0.750, time/batch = 0.092\n",
            "20549/22300 (epoch 46), train_loss = 0.758, time/batch = 0.092\n",
            "20550/22300 (epoch 46), train_loss = 0.754, time/batch = 0.093\n",
            "20551/22300 (epoch 46), train_loss = 0.736, time/batch = 0.092\n",
            "20552/22300 (epoch 46), train_loss = 0.764, time/batch = 0.092\n",
            "20553/22300 (epoch 46), train_loss = 0.740, time/batch = 0.091\n",
            "20554/22300 (epoch 46), train_loss = 0.769, time/batch = 0.092\n",
            "20555/22300 (epoch 46), train_loss = 0.734, time/batch = 0.092\n",
            "20556/22300 (epoch 46), train_loss = 0.748, time/batch = 0.092\n",
            "20557/22300 (epoch 46), train_loss = 0.765, time/batch = 0.092\n",
            "20558/22300 (epoch 46), train_loss = 0.752, time/batch = 0.092\n",
            "20559/22300 (epoch 46), train_loss = 0.752, time/batch = 0.094\n",
            "20560/22300 (epoch 46), train_loss = 0.719, time/batch = 0.092\n",
            "20561/22300 (epoch 46), train_loss = 0.774, time/batch = 0.100\n",
            "20562/22300 (epoch 46), train_loss = 0.732, time/batch = 0.093\n",
            "20563/22300 (epoch 46), train_loss = 0.719, time/batch = 0.091\n",
            "20564/22300 (epoch 46), train_loss = 0.745, time/batch = 0.092\n",
            "20565/22300 (epoch 46), train_loss = 0.745, time/batch = 0.091\n",
            "20566/22300 (epoch 46), train_loss = 0.764, time/batch = 0.093\n",
            "20567/22300 (epoch 46), train_loss = 0.715, time/batch = 0.091\n",
            "20568/22300 (epoch 46), train_loss = 0.740, time/batch = 0.092\n",
            "20569/22300 (epoch 46), train_loss = 0.742, time/batch = 0.093\n",
            "20570/22300 (epoch 46), train_loss = 0.733, time/batch = 0.094\n",
            "20571/22300 (epoch 46), train_loss = 0.737, time/batch = 0.093\n",
            "20572/22300 (epoch 46), train_loss = 0.739, time/batch = 0.092\n",
            "20573/22300 (epoch 46), train_loss = 0.775, time/batch = 0.093\n",
            "20574/22300 (epoch 46), train_loss = 0.743, time/batch = 0.092\n",
            "20575/22300 (epoch 46), train_loss = 0.723, time/batch = 0.096\n",
            "20576/22300 (epoch 46), train_loss = 0.759, time/batch = 0.093\n",
            "20577/22300 (epoch 46), train_loss = 0.719, time/batch = 0.095\n",
            "20578/22300 (epoch 46), train_loss = 0.709, time/batch = 0.091\n",
            "20579/22300 (epoch 46), train_loss = 0.765, time/batch = 0.094\n",
            "20580/22300 (epoch 46), train_loss = 0.738, time/batch = 0.093\n",
            "20581/22300 (epoch 46), train_loss = 0.745, time/batch = 0.093\n",
            "20582/22300 (epoch 46), train_loss = 0.748, time/batch = 0.092\n",
            "20583/22300 (epoch 46), train_loss = 0.790, time/batch = 0.092\n",
            "20584/22300 (epoch 46), train_loss = 0.750, time/batch = 0.093\n",
            "20585/22300 (epoch 46), train_loss = 0.764, time/batch = 0.092\n",
            "20586/22300 (epoch 46), train_loss = 0.753, time/batch = 0.096\n",
            "20587/22300 (epoch 46), train_loss = 0.693, time/batch = 0.092\n",
            "20588/22300 (epoch 46), train_loss = 0.729, time/batch = 0.091\n",
            "20589/22300 (epoch 46), train_loss = 0.717, time/batch = 0.092\n",
            "20590/22300 (epoch 46), train_loss = 0.692, time/batch = 0.092\n",
            "20591/22300 (epoch 46), train_loss = 0.753, time/batch = 0.092\n",
            "20592/22300 (epoch 46), train_loss = 0.746, time/batch = 0.092\n",
            "20593/22300 (epoch 46), train_loss = 0.724, time/batch = 0.093\n",
            "20594/22300 (epoch 46), train_loss = 0.717, time/batch = 0.092\n",
            "20595/22300 (epoch 46), train_loss = 0.766, time/batch = 0.092\n",
            "20596/22300 (epoch 46), train_loss = 0.731, time/batch = 0.093\n",
            "20597/22300 (epoch 46), train_loss = 0.720, time/batch = 0.093\n",
            "20598/22300 (epoch 46), train_loss = 0.741, time/batch = 0.092\n",
            "20599/22300 (epoch 46), train_loss = 0.727, time/batch = 0.091\n",
            "20600/22300 (epoch 46), train_loss = 0.750, time/batch = 0.095\n",
            "20601/22300 (epoch 46), train_loss = 0.742, time/batch = 0.093\n",
            "20602/22300 (epoch 46), train_loss = 0.733, time/batch = 0.092\n",
            "20603/22300 (epoch 46), train_loss = 0.768, time/batch = 0.093\n",
            "20604/22300 (epoch 46), train_loss = 0.734, time/batch = 0.091\n",
            "20605/22300 (epoch 46), train_loss = 0.766, time/batch = 0.093\n",
            "20606/22300 (epoch 46), train_loss = 0.717, time/batch = 0.091\n",
            "20607/22300 (epoch 46), train_loss = 0.744, time/batch = 0.093\n",
            "20608/22300 (epoch 46), train_loss = 0.765, time/batch = 0.093\n",
            "20609/22300 (epoch 46), train_loss = 0.737, time/batch = 0.092\n",
            "20610/22300 (epoch 46), train_loss = 0.747, time/batch = 0.092\n",
            "20611/22300 (epoch 46), train_loss = 0.758, time/batch = 0.091\n",
            "20612/22300 (epoch 46), train_loss = 0.714, time/batch = 0.092\n",
            "20613/22300 (epoch 46), train_loss = 0.764, time/batch = 0.091\n",
            "20614/22300 (epoch 46), train_loss = 0.710, time/batch = 0.092\n",
            "20615/22300 (epoch 46), train_loss = 0.734, time/batch = 0.092\n",
            "20616/22300 (epoch 46), train_loss = 0.730, time/batch = 0.092\n",
            "20617/22300 (epoch 46), train_loss = 0.738, time/batch = 0.093\n",
            "20618/22300 (epoch 46), train_loss = 0.746, time/batch = 0.092\n",
            "20619/22300 (epoch 46), train_loss = 0.723, time/batch = 0.092\n",
            "20620/22300 (epoch 46), train_loss = 0.744, time/batch = 0.092\n",
            "20621/22300 (epoch 46), train_loss = 0.717, time/batch = 0.093\n",
            "20622/22300 (epoch 46), train_loss = 0.706, time/batch = 0.091\n",
            "20623/22300 (epoch 46), train_loss = 0.711, time/batch = 0.092\n",
            "20624/22300 (epoch 46), train_loss = 0.717, time/batch = 0.092\n",
            "20625/22300 (epoch 46), train_loss = 0.721, time/batch = 0.091\n",
            "20626/22300 (epoch 46), train_loss = 0.693, time/batch = 0.093\n",
            "20627/22300 (epoch 46), train_loss = 0.695, time/batch = 0.092\n",
            "20628/22300 (epoch 46), train_loss = 0.727, time/batch = 0.092\n",
            "20629/22300 (epoch 46), train_loss = 0.687, time/batch = 0.092\n",
            "20630/22300 (epoch 46), train_loss = 0.737, time/batch = 0.092\n",
            "20631/22300 (epoch 46), train_loss = 0.719, time/batch = 0.092\n",
            "20632/22300 (epoch 46), train_loss = 0.711, time/batch = 0.092\n",
            "20633/22300 (epoch 46), train_loss = 0.742, time/batch = 0.099\n",
            "20634/22300 (epoch 46), train_loss = 0.765, time/batch = 0.091\n",
            "20635/22300 (epoch 46), train_loss = 0.720, time/batch = 0.092\n",
            "20636/22300 (epoch 46), train_loss = 0.764, time/batch = 0.092\n",
            "20637/22300 (epoch 46), train_loss = 0.735, time/batch = 0.092\n",
            "20638/22300 (epoch 46), train_loss = 0.734, time/batch = 0.093\n",
            "20639/22300 (epoch 46), train_loss = 0.732, time/batch = 0.091\n",
            "20640/22300 (epoch 46), train_loss = 0.759, time/batch = 0.093\n",
            "20641/22300 (epoch 46), train_loss = 0.768, time/batch = 0.092\n",
            "20642/22300 (epoch 46), train_loss = 0.741, time/batch = 0.091\n",
            "20643/22300 (epoch 46), train_loss = 0.751, time/batch = 0.093\n",
            "20644/22300 (epoch 46), train_loss = 0.766, time/batch = 0.092\n",
            "20645/22300 (epoch 46), train_loss = 0.748, time/batch = 0.092\n",
            "20646/22300 (epoch 46), train_loss = 0.740, time/batch = 0.091\n",
            "20647/22300 (epoch 46), train_loss = 0.743, time/batch = 0.092\n",
            "20648/22300 (epoch 46), train_loss = 0.766, time/batch = 0.092\n",
            "20649/22300 (epoch 46), train_loss = 0.767, time/batch = 0.093\n",
            "20650/22300 (epoch 46), train_loss = 0.735, time/batch = 0.093\n",
            "20651/22300 (epoch 46), train_loss = 0.771, time/batch = 0.092\n",
            "20652/22300 (epoch 46), train_loss = 0.769, time/batch = 0.092\n",
            "20653/22300 (epoch 46), train_loss = 0.763, time/batch = 0.092\n",
            "20654/22300 (epoch 46), train_loss = 0.752, time/batch = 0.093\n",
            "20655/22300 (epoch 46), train_loss = 0.780, time/batch = 0.092\n",
            "20656/22300 (epoch 46), train_loss = 0.738, time/batch = 0.091\n",
            "20657/22300 (epoch 46), train_loss = 0.755, time/batch = 0.092\n",
            "20658/22300 (epoch 46), train_loss = 0.730, time/batch = 0.092\n",
            "20659/22300 (epoch 46), train_loss = 0.747, time/batch = 0.098\n",
            "20660/22300 (epoch 46), train_loss = 0.753, time/batch = 0.096\n",
            "20661/22300 (epoch 46), train_loss = 0.807, time/batch = 0.092\n",
            "20662/22300 (epoch 46), train_loss = 0.766, time/batch = 0.092\n",
            "20663/22300 (epoch 46), train_loss = 0.750, time/batch = 0.092\n",
            "20664/22300 (epoch 46), train_loss = 0.767, time/batch = 0.093\n",
            "20665/22300 (epoch 46), train_loss = 0.760, time/batch = 0.091\n",
            "20666/22300 (epoch 46), train_loss = 0.758, time/batch = 0.092\n",
            "20667/22300 (epoch 46), train_loss = 0.760, time/batch = 0.093\n",
            "20668/22300 (epoch 46), train_loss = 0.742, time/batch = 0.092\n",
            "20669/22300 (epoch 46), train_loss = 0.713, time/batch = 0.094\n",
            "20670/22300 (epoch 46), train_loss = 0.747, time/batch = 0.092\n",
            "20671/22300 (epoch 46), train_loss = 0.727, time/batch = 0.092\n",
            "20672/22300 (epoch 46), train_loss = 0.738, time/batch = 0.092\n",
            "20673/22300 (epoch 46), train_loss = 0.722, time/batch = 0.092\n",
            "20674/22300 (epoch 46), train_loss = 0.765, time/batch = 0.093\n",
            "20675/22300 (epoch 46), train_loss = 0.742, time/batch = 0.096\n",
            "20676/22300 (epoch 46), train_loss = 0.732, time/batch = 0.093\n",
            "20677/22300 (epoch 46), train_loss = 0.771, time/batch = 0.092\n",
            "20678/22300 (epoch 46), train_loss = 0.760, time/batch = 0.091\n",
            "20679/22300 (epoch 46), train_loss = 0.758, time/batch = 0.093\n",
            "20680/22300 (epoch 46), train_loss = 0.756, time/batch = 0.092\n",
            "20681/22300 (epoch 46), train_loss = 0.755, time/batch = 0.093\n",
            "20682/22300 (epoch 46), train_loss = 0.712, time/batch = 0.091\n",
            "20683/22300 (epoch 46), train_loss = 0.730, time/batch = 0.092\n",
            "20684/22300 (epoch 46), train_loss = 0.723, time/batch = 0.094\n",
            "20685/22300 (epoch 46), train_loss = 0.743, time/batch = 0.092\n",
            "20686/22300 (epoch 46), train_loss = 0.726, time/batch = 0.094\n",
            "20687/22300 (epoch 46), train_loss = 0.726, time/batch = 0.091\n",
            "20688/22300 (epoch 46), train_loss = 0.722, time/batch = 0.092\n",
            "20689/22300 (epoch 46), train_loss = 0.703, time/batch = 0.091\n",
            "20690/22300 (epoch 46), train_loss = 0.725, time/batch = 0.095\n",
            "20691/22300 (epoch 46), train_loss = 0.708, time/batch = 0.092\n",
            "20692/22300 (epoch 46), train_loss = 0.689, time/batch = 0.091\n",
            "20693/22300 (epoch 46), train_loss = 0.713, time/batch = 0.092\n",
            "20694/22300 (epoch 46), train_loss = 0.672, time/batch = 0.092\n",
            "20695/22300 (epoch 46), train_loss = 0.742, time/batch = 0.092\n",
            "20696/22300 (epoch 46), train_loss = 0.731, time/batch = 0.093\n",
            "20697/22300 (epoch 46), train_loss = 0.712, time/batch = 0.092\n",
            "20698/22300 (epoch 46), train_loss = 0.734, time/batch = 0.093\n",
            "20699/22300 (epoch 46), train_loss = 0.723, time/batch = 0.098\n",
            "20700/22300 (epoch 46), train_loss = 0.705, time/batch = 0.094\n",
            "20701/22300 (epoch 46), train_loss = 0.686, time/batch = 0.091\n",
            "20702/22300 (epoch 46), train_loss = 0.742, time/batch = 0.091\n",
            "20703/22300 (epoch 46), train_loss = 0.716, time/batch = 0.093\n",
            "20704/22300 (epoch 46), train_loss = 0.751, time/batch = 0.092\n",
            "20705/22300 (epoch 46), train_loss = 0.712, time/batch = 0.092\n",
            "20706/22300 (epoch 46), train_loss = 0.745, time/batch = 0.094\n",
            "20707/22300 (epoch 46), train_loss = 0.742, time/batch = 0.091\n",
            "20708/22300 (epoch 46), train_loss = 0.746, time/batch = 0.093\n",
            "20709/22300 (epoch 46), train_loss = 0.751, time/batch = 0.092\n",
            "20710/22300 (epoch 46), train_loss = 0.749, time/batch = 0.093\n",
            "20711/22300 (epoch 46), train_loss = 0.722, time/batch = 0.092\n",
            "20712/22300 (epoch 46), train_loss = 0.739, time/batch = 0.091\n",
            "20713/22300 (epoch 46), train_loss = 0.730, time/batch = 0.093\n",
            "20714/22300 (epoch 46), train_loss = 0.781, time/batch = 0.092\n",
            "20715/22300 (epoch 46), train_loss = 0.751, time/batch = 0.093\n",
            "20716/22300 (epoch 46), train_loss = 0.778, time/batch = 0.092\n",
            "20717/22300 (epoch 46), train_loss = 0.730, time/batch = 0.092\n",
            "20718/22300 (epoch 46), train_loss = 0.718, time/batch = 0.092\n",
            "20719/22300 (epoch 46), train_loss = 0.706, time/batch = 0.092\n",
            "20720/22300 (epoch 46), train_loss = 0.719, time/batch = 0.093\n",
            "20721/22300 (epoch 46), train_loss = 0.714, time/batch = 0.092\n",
            "20722/22300 (epoch 46), train_loss = 0.765, time/batch = 0.092\n",
            "20723/22300 (epoch 46), train_loss = 0.725, time/batch = 0.091\n",
            "20724/22300 (epoch 46), train_loss = 0.777, time/batch = 0.093\n",
            "20725/22300 (epoch 46), train_loss = 0.709, time/batch = 0.092\n",
            "20726/22300 (epoch 46), train_loss = 0.726, time/batch = 0.092\n",
            "20727/22300 (epoch 46), train_loss = 0.718, time/batch = 0.094\n",
            "20728/22300 (epoch 46), train_loss = 0.740, time/batch = 0.093\n",
            "20729/22300 (epoch 46), train_loss = 0.742, time/batch = 0.092\n",
            "20730/22300 (epoch 46), train_loss = 0.714, time/batch = 0.096\n",
            "20731/22300 (epoch 46), train_loss = 0.727, time/batch = 0.091\n",
            "20732/22300 (epoch 46), train_loss = 0.722, time/batch = 0.092\n",
            "20733/22300 (epoch 46), train_loss = 0.745, time/batch = 0.092\n",
            "20734/22300 (epoch 46), train_loss = 0.722, time/batch = 0.093\n",
            "20735/22300 (epoch 46), train_loss = 0.728, time/batch = 0.092\n",
            "20736/22300 (epoch 46), train_loss = 0.747, time/batch = 0.093\n",
            "20737/22300 (epoch 46), train_loss = 0.721, time/batch = 0.093\n",
            "20738/22300 (epoch 46), train_loss = 0.739, time/batch = 0.091\n",
            "20739/22300 (epoch 46), train_loss = 0.779, time/batch = 0.093\n",
            "20740/22300 (epoch 46), train_loss = 0.740, time/batch = 0.096\n",
            "20741/22300 (epoch 46), train_loss = 0.766, time/batch = 0.092\n",
            "20742/22300 (epoch 46), train_loss = 0.785, time/batch = 0.092\n",
            "20743/22300 (epoch 46), train_loss = 0.751, time/batch = 0.091\n",
            "20744/22300 (epoch 46), train_loss = 0.758, time/batch = 0.093\n",
            "20745/22300 (epoch 46), train_loss = 0.770, time/batch = 0.091\n",
            "20746/22300 (epoch 46), train_loss = 0.766, time/batch = 0.092\n",
            "20747/22300 (epoch 46), train_loss = 0.770, time/batch = 0.091\n",
            "20748/22300 (epoch 46), train_loss = 0.761, time/batch = 0.092\n",
            "20749/22300 (epoch 46), train_loss = 0.764, time/batch = 0.092\n",
            "20750/22300 (epoch 46), train_loss = 0.783, time/batch = 0.093\n",
            "20751/22300 (epoch 46), train_loss = 0.749, time/batch = 0.093\n",
            "20752/22300 (epoch 46), train_loss = 0.747, time/batch = 0.092\n",
            "20753/22300 (epoch 46), train_loss = 0.720, time/batch = 0.092\n",
            "20754/22300 (epoch 46), train_loss = 0.740, time/batch = 0.092\n",
            "20755/22300 (epoch 46), train_loss = 0.734, time/batch = 0.092\n",
            "20756/22300 (epoch 46), train_loss = 0.759, time/batch = 0.092\n",
            "20757/22300 (epoch 46), train_loss = 0.726, time/batch = 0.092\n",
            "20758/22300 (epoch 46), train_loss = 0.756, time/batch = 0.093\n",
            "20759/22300 (epoch 46), train_loss = 0.770, time/batch = 0.092\n",
            "20760/22300 (epoch 46), train_loss = 0.728, time/batch = 0.092\n",
            "20761/22300 (epoch 46), train_loss = 0.775, time/batch = 0.091\n",
            "20762/22300 (epoch 46), train_loss = 0.772, time/batch = 0.094\n",
            "20763/22300 (epoch 46), train_loss = 0.729, time/batch = 0.092\n",
            "20764/22300 (epoch 46), train_loss = 0.769, time/batch = 0.092\n",
            "20765/22300 (epoch 46), train_loss = 0.759, time/batch = 0.092\n",
            "20766/22300 (epoch 46), train_loss = 0.758, time/batch = 0.092\n",
            "20767/22300 (epoch 46), train_loss = 0.743, time/batch = 0.093\n",
            "20768/22300 (epoch 46), train_loss = 0.755, time/batch = 0.093\n",
            "20769/22300 (epoch 46), train_loss = 0.756, time/batch = 0.093\n",
            "20770/22300 (epoch 46), train_loss = 0.731, time/batch = 0.092\n",
            "20771/22300 (epoch 46), train_loss = 0.753, time/batch = 0.092\n",
            "20772/22300 (epoch 46), train_loss = 0.735, time/batch = 0.093\n",
            "20773/22300 (epoch 46), train_loss = 0.752, time/batch = 0.092\n",
            "20774/22300 (epoch 46), train_loss = 0.736, time/batch = 0.093\n",
            "20775/22300 (epoch 46), train_loss = 0.751, time/batch = 0.091\n",
            "20776/22300 (epoch 46), train_loss = 0.718, time/batch = 0.092\n",
            "20777/22300 (epoch 46), train_loss = 0.789, time/batch = 0.095\n",
            "20778/22300 (epoch 46), train_loss = 0.729, time/batch = 0.092\n",
            "20779/22300 (epoch 46), train_loss = 0.777, time/batch = 0.093\n",
            "20780/22300 (epoch 46), train_loss = 0.751, time/batch = 0.092\n",
            "20781/22300 (epoch 46), train_loss = 0.767, time/batch = 0.092\n",
            "20782/22300 (epoch 46), train_loss = 0.753, time/batch = 0.091\n",
            "20783/22300 (epoch 46), train_loss = 0.782, time/batch = 0.091\n",
            "20784/22300 (epoch 46), train_loss = 0.769, time/batch = 0.091\n",
            "20785/22300 (epoch 46), train_loss = 0.759, time/batch = 0.092\n",
            "20786/22300 (epoch 46), train_loss = 0.787, time/batch = 0.092\n",
            "20787/22300 (epoch 46), train_loss = 0.783, time/batch = 0.092\n",
            "20788/22300 (epoch 46), train_loss = 0.797, time/batch = 0.093\n",
            "20789/22300 (epoch 46), train_loss = 0.801, time/batch = 0.092\n",
            "20790/22300 (epoch 46), train_loss = 0.784, time/batch = 0.093\n",
            "20791/22300 (epoch 46), train_loss = 0.783, time/batch = 0.091\n",
            "20792/22300 (epoch 46), train_loss = 0.811, time/batch = 0.092\n",
            "20793/22300 (epoch 46), train_loss = 0.750, time/batch = 0.093\n",
            "20794/22300 (epoch 46), train_loss = 0.740, time/batch = 0.092\n",
            "20795/22300 (epoch 46), train_loss = 0.752, time/batch = 0.092\n",
            "20796/22300 (epoch 46), train_loss = 0.755, time/batch = 0.092\n",
            "20797/22300 (epoch 46), train_loss = 0.800, time/batch = 0.092\n",
            "20798/22300 (epoch 46), train_loss = 0.775, time/batch = 0.092\n",
            "20799/22300 (epoch 46), train_loss = 0.753, time/batch = 0.093\n",
            "20800/22300 (epoch 46), train_loss = 0.755, time/batch = 0.093\n",
            "20801/22300 (epoch 46), train_loss = 0.757, time/batch = 0.092\n",
            "20802/22300 (epoch 46), train_loss = 0.784, time/batch = 0.093\n",
            "20803/22300 (epoch 46), train_loss = 0.778, time/batch = 0.098\n",
            "20804/22300 (epoch 46), train_loss = 0.756, time/batch = 0.094\n",
            "20805/22300 (epoch 46), train_loss = 0.757, time/batch = 0.094\n",
            "20806/22300 (epoch 46), train_loss = 0.727, time/batch = 0.091\n",
            "20807/22300 (epoch 46), train_loss = 0.738, time/batch = 0.093\n",
            "20808/22300 (epoch 46), train_loss = 0.768, time/batch = 0.091\n",
            "20809/22300 (epoch 46), train_loss = 0.753, time/batch = 0.093\n",
            "20810/22300 (epoch 46), train_loss = 0.771, time/batch = 0.093\n",
            "20811/22300 (epoch 46), train_loss = 0.763, time/batch = 0.092\n",
            "20812/22300 (epoch 46), train_loss = 0.748, time/batch = 0.093\n",
            "20813/22300 (epoch 46), train_loss = 0.763, time/batch = 0.095\n",
            "20814/22300 (epoch 46), train_loss = 0.751, time/batch = 0.096\n",
            "20815/22300 (epoch 46), train_loss = 0.781, time/batch = 0.093\n",
            "20816/22300 (epoch 46), train_loss = 0.723, time/batch = 0.094\n",
            "20817/22300 (epoch 46), train_loss = 0.760, time/batch = 0.092\n",
            "20818/22300 (epoch 46), train_loss = 0.720, time/batch = 0.092\n",
            "20819/22300 (epoch 46), train_loss = 0.741, time/batch = 0.091\n",
            "20820/22300 (epoch 46), train_loss = 0.756, time/batch = 0.094\n",
            "20821/22300 (epoch 46), train_loss = 0.752, time/batch = 0.092\n",
            "20822/22300 (epoch 46), train_loss = 0.718, time/batch = 0.093\n",
            "20823/22300 (epoch 46), train_loss = 0.766, time/batch = 0.094\n",
            "20824/22300 (epoch 46), train_loss = 0.755, time/batch = 0.092\n",
            "20825/22300 (epoch 46), train_loss = 0.751, time/batch = 0.093\n",
            "20826/22300 (epoch 46), train_loss = 0.723, time/batch = 0.091\n",
            "20827/22300 (epoch 46), train_loss = 0.719, time/batch = 0.092\n",
            "20828/22300 (epoch 46), train_loss = 0.732, time/batch = 0.092\n",
            "20829/22300 (epoch 46), train_loss = 0.752, time/batch = 0.092\n",
            "20830/22300 (epoch 46), train_loss = 0.732, time/batch = 0.093\n",
            "20831/22300 (epoch 46), train_loss = 0.737, time/batch = 0.092\n",
            "20832/22300 (epoch 46), train_loss = 0.721, time/batch = 0.093\n",
            "20833/22300 (epoch 46), train_loss = 0.763, time/batch = 0.092\n",
            "20834/22300 (epoch 46), train_loss = 0.722, time/batch = 0.092\n",
            "20835/22300 (epoch 46), train_loss = 0.716, time/batch = 0.092\n",
            "20836/22300 (epoch 46), train_loss = 0.725, time/batch = 0.093\n",
            "20837/22300 (epoch 46), train_loss = 0.763, time/batch = 0.092\n",
            "20838/22300 (epoch 46), train_loss = 0.748, time/batch = 0.092\n",
            "20839/22300 (epoch 46), train_loss = 0.703, time/batch = 0.101\n",
            "20840/22300 (epoch 46), train_loss = 0.714, time/batch = 0.097\n",
            "20841/22300 (epoch 46), train_loss = 0.741, time/batch = 0.092\n",
            "20842/22300 (epoch 46), train_loss = 0.757, time/batch = 0.092\n",
            "20843/22300 (epoch 46), train_loss = 0.743, time/batch = 0.096\n",
            "20844/22300 (epoch 46), train_loss = 0.784, time/batch = 0.093\n",
            "20845/22300 (epoch 46), train_loss = 0.742, time/batch = 0.095\n",
            "20846/22300 (epoch 46), train_loss = 0.723, time/batch = 0.095\n",
            "20847/22300 (epoch 46), train_loss = 0.752, time/batch = 0.091\n",
            "20848/22300 (epoch 46), train_loss = 0.733, time/batch = 0.093\n",
            "20849/22300 (epoch 46), train_loss = 0.696, time/batch = 0.096\n",
            "20850/22300 (epoch 46), train_loss = 0.727, time/batch = 0.091\n",
            "20851/22300 (epoch 46), train_loss = 0.739, time/batch = 0.092\n",
            "20852/22300 (epoch 46), train_loss = 0.745, time/batch = 0.091\n",
            "20853/22300 (epoch 46), train_loss = 0.719, time/batch = 0.092\n",
            "20854/22300 (epoch 46), train_loss = 0.735, time/batch = 0.093\n",
            "20855/22300 (epoch 46), train_loss = 0.744, time/batch = 0.100\n",
            "20856/22300 (epoch 46), train_loss = 0.759, time/batch = 0.092\n",
            "20857/22300 (epoch 46), train_loss = 0.720, time/batch = 0.092\n",
            "20858/22300 (epoch 46), train_loss = 0.714, time/batch = 0.091\n",
            "20859/22300 (epoch 46), train_loss = 0.740, time/batch = 0.094\n",
            "20860/22300 (epoch 46), train_loss = 0.738, time/batch = 0.091\n",
            "20861/22300 (epoch 46), train_loss = 0.741, time/batch = 0.092\n",
            "20862/22300 (epoch 46), train_loss = 0.710, time/batch = 0.092\n",
            "20863/22300 (epoch 46), train_loss = 0.754, time/batch = 0.093\n",
            "20864/22300 (epoch 46), train_loss = 0.701, time/batch = 0.093\n",
            "20865/22300 (epoch 46), train_loss = 0.729, time/batch = 0.094\n",
            "20866/22300 (epoch 46), train_loss = 0.719, time/batch = 0.095\n",
            "20867/22300 (epoch 46), train_loss = 0.731, time/batch = 0.092\n",
            "20868/22300 (epoch 46), train_loss = 0.713, time/batch = 0.092\n",
            "20869/22300 (epoch 46), train_loss = 0.723, time/batch = 0.093\n",
            "20870/22300 (epoch 46), train_loss = 0.741, time/batch = 0.095\n",
            "20871/22300 (epoch 46), train_loss = 0.708, time/batch = 0.092\n",
            "20872/22300 (epoch 46), train_loss = 0.757, time/batch = 0.093\n",
            "20873/22300 (epoch 46), train_loss = 0.750, time/batch = 0.091\n",
            "20874/22300 (epoch 46), train_loss = 0.743, time/batch = 0.092\n",
            "20875/22300 (epoch 46), train_loss = 0.753, time/batch = 0.092\n",
            "20876/22300 (epoch 46), train_loss = 0.754, time/batch = 0.093\n",
            "20877/22300 (epoch 46), train_loss = 0.745, time/batch = 0.091\n",
            "20878/22300 (epoch 46), train_loss = 0.748, time/batch = 0.092\n",
            "20879/22300 (epoch 46), train_loss = 0.721, time/batch = 0.093\n",
            "20880/22300 (epoch 46), train_loss = 0.750, time/batch = 0.092\n",
            "20881/22300 (epoch 46), train_loss = 0.730, time/batch = 0.093\n",
            "20882/22300 (epoch 46), train_loss = 0.719, time/batch = 0.093\n",
            "20883/22300 (epoch 46), train_loss = 0.700, time/batch = 0.092\n",
            "20884/22300 (epoch 46), train_loss = 0.705, time/batch = 0.092\n",
            "20885/22300 (epoch 46), train_loss = 0.722, time/batch = 0.095\n",
            "20886/22300 (epoch 46), train_loss = 0.748, time/batch = 0.093\n",
            "20887/22300 (epoch 46), train_loss = 0.691, time/batch = 0.091\n",
            "20888/22300 (epoch 46), train_loss = 0.757, time/batch = 0.097\n",
            "20889/22300 (epoch 46), train_loss = 0.720, time/batch = 0.095\n",
            "20890/22300 (epoch 46), train_loss = 0.756, time/batch = 0.092\n",
            "20891/22300 (epoch 46), train_loss = 0.739, time/batch = 0.092\n",
            "20892/22300 (epoch 46), train_loss = 0.729, time/batch = 0.092\n",
            "20893/22300 (epoch 46), train_loss = 0.753, time/batch = 0.092\n",
            "20894/22300 (epoch 46), train_loss = 0.714, time/batch = 0.095\n",
            "20895/22300 (epoch 46), train_loss = 0.691, time/batch = 0.091\n",
            "20896/22300 (epoch 46), train_loss = 0.702, time/batch = 0.092\n",
            "20897/22300 (epoch 46), train_loss = 0.721, time/batch = 0.092\n",
            "20898/22300 (epoch 46), train_loss = 0.694, time/batch = 0.093\n",
            "20899/22300 (epoch 46), train_loss = 0.678, time/batch = 0.093\n",
            "20900/22300 (epoch 46), train_loss = 0.704, time/batch = 0.092\n",
            "20901/22300 (epoch 46), train_loss = 0.703, time/batch = 0.092\n",
            "20902/22300 (epoch 46), train_loss = 0.714, time/batch = 0.092\n",
            "20903/22300 (epoch 46), train_loss = 0.730, time/batch = 0.092\n",
            "20904/22300 (epoch 46), train_loss = 0.702, time/batch = 0.092\n",
            "20905/22300 (epoch 46), train_loss = 0.703, time/batch = 0.092\n",
            "20906/22300 (epoch 46), train_loss = 0.719, time/batch = 0.093\n",
            "20907/22300 (epoch 46), train_loss = 0.696, time/batch = 0.099\n",
            "20908/22300 (epoch 46), train_loss = 0.725, time/batch = 0.091\n",
            "20909/22300 (epoch 46), train_loss = 0.753, time/batch = 0.093\n",
            "20910/22300 (epoch 46), train_loss = 0.718, time/batch = 0.092\n",
            "20911/22300 (epoch 46), train_loss = 0.743, time/batch = 0.093\n",
            "20912/22300 (epoch 46), train_loss = 0.738, time/batch = 0.091\n",
            "20913/22300 (epoch 46), train_loss = 0.737, time/batch = 0.093\n",
            "20914/22300 (epoch 46), train_loss = 0.761, time/batch = 0.092\n",
            "20915/22300 (epoch 46), train_loss = 0.726, time/batch = 0.093\n",
            "20916/22300 (epoch 46), train_loss = 0.744, time/batch = 0.094\n",
            "20917/22300 (epoch 46), train_loss = 0.754, time/batch = 0.092\n",
            "20918/22300 (epoch 46), train_loss = 0.743, time/batch = 0.092\n",
            "20919/22300 (epoch 46), train_loss = 0.725, time/batch = 0.092\n",
            "20920/22300 (epoch 46), train_loss = 0.732, time/batch = 0.093\n",
            "20921/22300 (epoch 46), train_loss = 0.733, time/batch = 0.095\n",
            "20922/22300 (epoch 46), train_loss = 0.727, time/batch = 0.092\n",
            "20923/22300 (epoch 46), train_loss = 0.756, time/batch = 0.098\n",
            "20924/22300 (epoch 46), train_loss = 0.726, time/batch = 0.094\n",
            "20925/22300 (epoch 46), train_loss = 0.750, time/batch = 0.092\n",
            "20926/22300 (epoch 46), train_loss = 0.758, time/batch = 0.094\n",
            "20927/22300 (epoch 46), train_loss = 0.728, time/batch = 0.092\n",
            "20928/22300 (epoch 46), train_loss = 0.744, time/batch = 0.093\n",
            "20929/22300 (epoch 46), train_loss = 0.718, time/batch = 0.093\n",
            "20930/22300 (epoch 46), train_loss = 0.756, time/batch = 0.091\n",
            "20931/22300 (epoch 46), train_loss = 0.771, time/batch = 0.093\n",
            "20932/22300 (epoch 46), train_loss = 0.763, time/batch = 0.102\n",
            "20933/22300 (epoch 46), train_loss = 0.767, time/batch = 0.092\n",
            "20934/22300 (epoch 46), train_loss = 0.766, time/batch = 0.092\n",
            "20935/22300 (epoch 46), train_loss = 0.742, time/batch = 0.091\n",
            "20936/22300 (epoch 46), train_loss = 0.749, time/batch = 0.092\n",
            "20937/22300 (epoch 46), train_loss = 0.734, time/batch = 0.092\n",
            "20938/22300 (epoch 46), train_loss = 0.742, time/batch = 0.092\n",
            "20939/22300 (epoch 46), train_loss = 0.730, time/batch = 0.093\n",
            "20940/22300 (epoch 46), train_loss = 0.741, time/batch = 0.092\n",
            "20941/22300 (epoch 46), train_loss = 0.734, time/batch = 0.094\n",
            "20942/22300 (epoch 46), train_loss = 0.784, time/batch = 0.093\n",
            "20943/22300 (epoch 46), train_loss = 0.719, time/batch = 0.092\n",
            "20944/22300 (epoch 46), train_loss = 0.746, time/batch = 0.093\n",
            "20945/22300 (epoch 46), train_loss = 0.722, time/batch = 0.091\n",
            "20946/22300 (epoch 46), train_loss = 0.729, time/batch = 0.093\n",
            "20947/22300 (epoch 46), train_loss = 0.773, time/batch = 0.095\n",
            "20948/22300 (epoch 46), train_loss = 0.749, time/batch = 0.092\n",
            "20949/22300 (epoch 46), train_loss = 0.751, time/batch = 0.094\n",
            "20950/22300 (epoch 46), train_loss = 0.721, time/batch = 0.094\n",
            "20951/22300 (epoch 46), train_loss = 0.715, time/batch = 0.093\n",
            "20952/22300 (epoch 46), train_loss = 0.688, time/batch = 0.093\n",
            "20953/22300 (epoch 46), train_loss = 0.716, time/batch = 0.092\n",
            "20954/22300 (epoch 46), train_loss = 0.785, time/batch = 0.094\n",
            "20955/22300 (epoch 46), train_loss = 0.723, time/batch = 0.100\n",
            "20956/22300 (epoch 46), train_loss = 0.735, time/batch = 0.095\n",
            "20957/22300 (epoch 46), train_loss = 0.722, time/batch = 0.092\n",
            "20958/22300 (epoch 46), train_loss = 0.737, time/batch = 0.092\n",
            "20959/22300 (epoch 46), train_loss = 0.726, time/batch = 0.092\n",
            "20960/22300 (epoch 46), train_loss = 0.739, time/batch = 0.092\n",
            "20961/22300 (epoch 46), train_loss = 0.725, time/batch = 0.092\n",
            "20962/22300 (epoch 47), train_loss = 0.409, time/batch = 0.086\n",
            "20963/22300 (epoch 47), train_loss = 0.728, time/batch = 0.093\n",
            "20964/22300 (epoch 47), train_loss = 0.755, time/batch = 0.093\n",
            "20965/22300 (epoch 47), train_loss = 0.769, time/batch = 0.091\n",
            "20966/22300 (epoch 47), train_loss = 0.782, time/batch = 0.092\n",
            "20967/22300 (epoch 47), train_loss = 0.724, time/batch = 0.091\n",
            "20968/22300 (epoch 47), train_loss = 0.758, time/batch = 0.092\n",
            "20969/22300 (epoch 47), train_loss = 0.752, time/batch = 0.092\n",
            "20970/22300 (epoch 47), train_loss = 0.732, time/batch = 0.091\n",
            "20971/22300 (epoch 47), train_loss = 0.719, time/batch = 0.102\n",
            "20972/22300 (epoch 47), train_loss = 0.786, time/batch = 0.092\n",
            "20973/22300 (epoch 47), train_loss = 0.734, time/batch = 0.092\n",
            "20974/22300 (epoch 47), train_loss = 0.767, time/batch = 0.092\n",
            "20975/22300 (epoch 47), train_loss = 0.769, time/batch = 0.095\n",
            "20976/22300 (epoch 47), train_loss = 0.754, time/batch = 0.091\n",
            "20977/22300 (epoch 47), train_loss = 0.789, time/batch = 0.091\n",
            "20978/22300 (epoch 47), train_loss = 0.794, time/batch = 0.092\n",
            "20979/22300 (epoch 47), train_loss = 0.744, time/batch = 0.091\n",
            "20980/22300 (epoch 47), train_loss = 0.753, time/batch = 0.099\n",
            "20981/22300 (epoch 47), train_loss = 0.776, time/batch = 0.092\n",
            "20982/22300 (epoch 47), train_loss = 0.742, time/batch = 0.093\n",
            "20983/22300 (epoch 47), train_loss = 0.733, time/batch = 0.093\n",
            "20984/22300 (epoch 47), train_loss = 0.744, time/batch = 0.093\n",
            "20985/22300 (epoch 47), train_loss = 0.739, time/batch = 0.092\n",
            "20986/22300 (epoch 47), train_loss = 0.711, time/batch = 0.093\n",
            "20987/22300 (epoch 47), train_loss = 0.736, time/batch = 0.092\n",
            "20988/22300 (epoch 47), train_loss = 0.753, time/batch = 0.092\n",
            "20989/22300 (epoch 47), train_loss = 0.722, time/batch = 0.091\n",
            "20990/22300 (epoch 47), train_loss = 0.750, time/batch = 0.092\n",
            "20991/22300 (epoch 47), train_loss = 0.728, time/batch = 0.093\n",
            "20992/22300 (epoch 47), train_loss = 0.730, time/batch = 0.092\n",
            "20993/22300 (epoch 47), train_loss = 0.747, time/batch = 0.093\n",
            "20994/22300 (epoch 47), train_loss = 0.742, time/batch = 0.092\n",
            "20995/22300 (epoch 47), train_loss = 0.751, time/batch = 0.093\n",
            "20996/22300 (epoch 47), train_loss = 0.747, time/batch = 0.093\n",
            "20997/22300 (epoch 47), train_loss = 0.722, time/batch = 0.092\n",
            "20998/22300 (epoch 47), train_loss = 0.748, time/batch = 0.092\n",
            "20999/22300 (epoch 47), train_loss = 0.729, time/batch = 0.091\n",
            "21000/22300 (epoch 47), train_loss = 0.750, time/batch = 0.092\n",
            "model saved to save/model.ckpt\n",
            "21001/22300 (epoch 47), train_loss = 0.726, time/batch = 0.085\n",
            "21002/22300 (epoch 47), train_loss = 0.744, time/batch = 0.093\n",
            "21003/22300 (epoch 47), train_loss = 0.770, time/batch = 0.092\n",
            "21004/22300 (epoch 47), train_loss = 0.757, time/batch = 0.092\n",
            "21005/22300 (epoch 47), train_loss = 0.755, time/batch = 0.092\n",
            "21006/22300 (epoch 47), train_loss = 0.722, time/batch = 0.091\n",
            "21007/22300 (epoch 47), train_loss = 0.776, time/batch = 0.091\n",
            "21008/22300 (epoch 47), train_loss = 0.731, time/batch = 0.092\n",
            "21009/22300 (epoch 47), train_loss = 0.705, time/batch = 0.092\n",
            "21010/22300 (epoch 47), train_loss = 0.733, time/batch = 0.106\n",
            "21011/22300 (epoch 47), train_loss = 0.740, time/batch = 0.097\n",
            "21012/22300 (epoch 47), train_loss = 0.746, time/batch = 0.093\n",
            "21013/22300 (epoch 47), train_loss = 0.698, time/batch = 0.091\n",
            "21014/22300 (epoch 47), train_loss = 0.719, time/batch = 0.092\n",
            "21015/22300 (epoch 47), train_loss = 0.727, time/batch = 0.092\n",
            "21016/22300 (epoch 47), train_loss = 0.716, time/batch = 0.091\n",
            "21017/22300 (epoch 47), train_loss = 0.730, time/batch = 0.093\n",
            "21018/22300 (epoch 47), train_loss = 0.736, time/batch = 0.092\n",
            "21019/22300 (epoch 47), train_loss = 0.771, time/batch = 0.092\n",
            "21020/22300 (epoch 47), train_loss = 0.744, time/batch = 0.095\n",
            "21021/22300 (epoch 47), train_loss = 0.713, time/batch = 0.095\n",
            "21022/22300 (epoch 47), train_loss = 0.751, time/batch = 0.094\n",
            "21023/22300 (epoch 47), train_loss = 0.716, time/batch = 0.092\n",
            "21024/22300 (epoch 47), train_loss = 0.707, time/batch = 0.093\n",
            "21025/22300 (epoch 47), train_loss = 0.744, time/batch = 0.097\n",
            "21026/22300 (epoch 47), train_loss = 0.725, time/batch = 0.092\n",
            "21027/22300 (epoch 47), train_loss = 0.735, time/batch = 0.092\n",
            "21028/22300 (epoch 47), train_loss = 0.730, time/batch = 0.091\n",
            "21029/22300 (epoch 47), train_loss = 0.782, time/batch = 0.093\n",
            "21030/22300 (epoch 47), train_loss = 0.746, time/batch = 0.092\n",
            "21031/22300 (epoch 47), train_loss = 0.759, time/batch = 0.094\n",
            "21032/22300 (epoch 47), train_loss = 0.739, time/batch = 0.093\n",
            "21033/22300 (epoch 47), train_loss = 0.683, time/batch = 0.092\n",
            "21034/22300 (epoch 47), train_loss = 0.721, time/batch = 0.094\n",
            "21035/22300 (epoch 47), train_loss = 0.701, time/batch = 0.092\n",
            "21036/22300 (epoch 47), train_loss = 0.691, time/batch = 0.092\n",
            "21037/22300 (epoch 47), train_loss = 0.748, time/batch = 0.091\n",
            "21038/22300 (epoch 47), train_loss = 0.734, time/batch = 0.091\n",
            "21039/22300 (epoch 47), train_loss = 0.722, time/batch = 0.092\n",
            "21040/22300 (epoch 47), train_loss = 0.701, time/batch = 0.092\n",
            "21041/22300 (epoch 47), train_loss = 0.750, time/batch = 0.093\n",
            "21042/22300 (epoch 47), train_loss = 0.721, time/batch = 0.092\n",
            "21043/22300 (epoch 47), train_loss = 0.707, time/batch = 0.093\n",
            "21044/22300 (epoch 47), train_loss = 0.727, time/batch = 0.092\n",
            "21045/22300 (epoch 47), train_loss = 0.709, time/batch = 0.093\n",
            "21046/22300 (epoch 47), train_loss = 0.731, time/batch = 0.093\n",
            "21047/22300 (epoch 47), train_loss = 0.727, time/batch = 0.092\n",
            "21048/22300 (epoch 47), train_loss = 0.717, time/batch = 0.092\n",
            "21049/22300 (epoch 47), train_loss = 0.753, time/batch = 0.091\n",
            "21050/22300 (epoch 47), train_loss = 0.726, time/batch = 0.091\n",
            "21051/22300 (epoch 47), train_loss = 0.758, time/batch = 0.092\n",
            "21052/22300 (epoch 47), train_loss = 0.705, time/batch = 0.092\n",
            "21053/22300 (epoch 47), train_loss = 0.731, time/batch = 0.092\n",
            "21054/22300 (epoch 47), train_loss = 0.746, time/batch = 0.093\n",
            "21055/22300 (epoch 47), train_loss = 0.718, time/batch = 0.093\n",
            "21056/22300 (epoch 47), train_loss = 0.734, time/batch = 0.092\n",
            "21057/22300 (epoch 47), train_loss = 0.740, time/batch = 0.094\n",
            "21058/22300 (epoch 47), train_loss = 0.696, time/batch = 0.093\n",
            "21059/22300 (epoch 47), train_loss = 0.747, time/batch = 0.092\n",
            "21060/22300 (epoch 47), train_loss = 0.691, time/batch = 0.093\n",
            "21061/22300 (epoch 47), train_loss = 0.713, time/batch = 0.092\n",
            "21062/22300 (epoch 47), train_loss = 0.709, time/batch = 0.092\n",
            "21063/22300 (epoch 47), train_loss = 0.721, time/batch = 0.091\n",
            "21064/22300 (epoch 47), train_loss = 0.733, time/batch = 0.092\n",
            "21065/22300 (epoch 47), train_loss = 0.705, time/batch = 0.092\n",
            "21066/22300 (epoch 47), train_loss = 0.736, time/batch = 0.092\n",
            "21067/22300 (epoch 47), train_loss = 0.709, time/batch = 0.093\n",
            "21068/22300 (epoch 47), train_loss = 0.700, time/batch = 0.093\n",
            "21069/22300 (epoch 47), train_loss = 0.704, time/batch = 0.093\n",
            "21070/22300 (epoch 47), train_loss = 0.711, time/batch = 0.091\n",
            "21071/22300 (epoch 47), train_loss = 0.716, time/batch = 0.092\n",
            "21072/22300 (epoch 47), train_loss = 0.692, time/batch = 0.094\n",
            "21073/22300 (epoch 47), train_loss = 0.682, time/batch = 0.093\n",
            "21074/22300 (epoch 47), train_loss = 0.711, time/batch = 0.093\n",
            "21075/22300 (epoch 47), train_loss = 0.669, time/batch = 0.092\n",
            "21076/22300 (epoch 47), train_loss = 0.721, time/batch = 0.093\n",
            "21077/22300 (epoch 47), train_loss = 0.705, time/batch = 0.092\n",
            "21078/22300 (epoch 47), train_loss = 0.704, time/batch = 0.092\n",
            "21079/22300 (epoch 47), train_loss = 0.744, time/batch = 0.092\n",
            "21080/22300 (epoch 47), train_loss = 0.761, time/batch = 0.092\n",
            "21081/22300 (epoch 47), train_loss = 0.724, time/batch = 0.092\n",
            "21082/22300 (epoch 47), train_loss = 0.779, time/batch = 0.092\n",
            "21083/22300 (epoch 47), train_loss = 0.744, time/batch = 0.093\n",
            "21084/22300 (epoch 47), train_loss = 0.726, time/batch = 0.092\n",
            "21085/22300 (epoch 47), train_loss = 0.721, time/batch = 0.092\n",
            "21086/22300 (epoch 47), train_loss = 0.745, time/batch = 0.092\n",
            "21087/22300 (epoch 47), train_loss = 0.741, time/batch = 0.092\n",
            "21088/22300 (epoch 47), train_loss = 0.727, time/batch = 0.093\n",
            "21089/22300 (epoch 47), train_loss = 0.748, time/batch = 0.092\n",
            "21090/22300 (epoch 47), train_loss = 0.760, time/batch = 0.093\n",
            "21091/22300 (epoch 47), train_loss = 0.755, time/batch = 0.091\n",
            "21092/22300 (epoch 47), train_loss = 0.735, time/batch = 0.093\n",
            "21093/22300 (epoch 47), train_loss = 0.748, time/batch = 0.092\n",
            "21094/22300 (epoch 47), train_loss = 0.765, time/batch = 0.093\n",
            "21095/22300 (epoch 47), train_loss = 0.766, time/batch = 0.092\n",
            "21096/22300 (epoch 47), train_loss = 0.725, time/batch = 0.093\n",
            "21097/22300 (epoch 47), train_loss = 0.763, time/batch = 0.093\n",
            "21098/22300 (epoch 47), train_loss = 0.761, time/batch = 0.097\n",
            "21099/22300 (epoch 47), train_loss = 0.770, time/batch = 0.091\n",
            "21100/22300 (epoch 47), train_loss = 0.751, time/batch = 0.092\n",
            "21101/22300 (epoch 47), train_loss = 0.780, time/batch = 0.091\n",
            "21102/22300 (epoch 47), train_loss = 0.740, time/batch = 0.098\n",
            "21103/22300 (epoch 47), train_loss = 0.745, time/batch = 0.123\n",
            "21104/22300 (epoch 47), train_loss = 0.725, time/batch = 0.100\n",
            "21105/22300 (epoch 47), train_loss = 0.740, time/batch = 0.090\n",
            "21106/22300 (epoch 47), train_loss = 0.737, time/batch = 0.092\n",
            "21107/22300 (epoch 47), train_loss = 0.793, time/batch = 0.100\n",
            "21108/22300 (epoch 47), train_loss = 0.755, time/batch = 0.094\n",
            "21109/22300 (epoch 47), train_loss = 0.752, time/batch = 0.093\n",
            "21110/22300 (epoch 47), train_loss = 0.765, time/batch = 0.092\n",
            "21111/22300 (epoch 47), train_loss = 0.765, time/batch = 0.092\n",
            "21112/22300 (epoch 47), train_loss = 0.761, time/batch = 0.094\n",
            "21113/22300 (epoch 47), train_loss = 0.763, time/batch = 0.092\n",
            "21114/22300 (epoch 47), train_loss = 0.746, time/batch = 0.093\n",
            "21115/22300 (epoch 47), train_loss = 0.710, time/batch = 0.092\n",
            "21116/22300 (epoch 47), train_loss = 0.734, time/batch = 0.092\n",
            "21117/22300 (epoch 47), train_loss = 0.731, time/batch = 0.098\n",
            "21118/22300 (epoch 47), train_loss = 0.732, time/batch = 0.092\n",
            "21119/22300 (epoch 47), train_loss = 0.711, time/batch = 0.092\n",
            "21120/22300 (epoch 47), train_loss = 0.747, time/batch = 0.097\n",
            "21121/22300 (epoch 47), train_loss = 0.727, time/batch = 0.091\n",
            "21122/22300 (epoch 47), train_loss = 0.710, time/batch = 0.101\n",
            "21123/22300 (epoch 47), train_loss = 0.746, time/batch = 0.092\n",
            "21124/22300 (epoch 47), train_loss = 0.741, time/batch = 0.092\n",
            "21125/22300 (epoch 47), train_loss = 0.745, time/batch = 0.092\n",
            "21126/22300 (epoch 47), train_loss = 0.742, time/batch = 0.093\n",
            "21127/22300 (epoch 47), train_loss = 0.742, time/batch = 0.093\n",
            "21128/22300 (epoch 47), train_loss = 0.713, time/batch = 0.092\n",
            "21129/22300 (epoch 47), train_loss = 0.732, time/batch = 0.093\n",
            "21130/22300 (epoch 47), train_loss = 0.733, time/batch = 0.099\n",
            "21131/22300 (epoch 47), train_loss = 0.744, time/batch = 0.094\n",
            "21132/22300 (epoch 47), train_loss = 0.715, time/batch = 0.094\n",
            "21133/22300 (epoch 47), train_loss = 0.721, time/batch = 0.093\n",
            "21134/22300 (epoch 47), train_loss = 0.710, time/batch = 0.092\n",
            "21135/22300 (epoch 47), train_loss = 0.694, time/batch = 0.093\n",
            "21136/22300 (epoch 47), train_loss = 0.708, time/batch = 0.092\n",
            "21137/22300 (epoch 47), train_loss = 0.690, time/batch = 0.094\n",
            "21138/22300 (epoch 47), train_loss = 0.677, time/batch = 0.092\n",
            "21139/22300 (epoch 47), train_loss = 0.700, time/batch = 0.093\n",
            "21140/22300 (epoch 47), train_loss = 0.645, time/batch = 0.092\n",
            "21141/22300 (epoch 47), train_loss = 0.724, time/batch = 0.091\n",
            "21142/22300 (epoch 47), train_loss = 0.710, time/batch = 0.093\n",
            "21143/22300 (epoch 47), train_loss = 0.691, time/batch = 0.092\n",
            "21144/22300 (epoch 47), train_loss = 0.715, time/batch = 0.092\n",
            "21145/22300 (epoch 47), train_loss = 0.704, time/batch = 0.093\n",
            "21146/22300 (epoch 47), train_loss = 0.691, time/batch = 0.093\n",
            "21147/22300 (epoch 47), train_loss = 0.671, time/batch = 0.092\n",
            "21148/22300 (epoch 47), train_loss = 0.721, time/batch = 0.093\n",
            "21149/22300 (epoch 47), train_loss = 0.696, time/batch = 0.093\n",
            "21150/22300 (epoch 47), train_loss = 0.719, time/batch = 0.092\n",
            "21151/22300 (epoch 47), train_loss = 0.693, time/batch = 0.093\n",
            "21152/22300 (epoch 47), train_loss = 0.721, time/batch = 0.092\n",
            "21153/22300 (epoch 47), train_loss = 0.722, time/batch = 0.092\n",
            "21154/22300 (epoch 47), train_loss = 0.721, time/batch = 0.092\n",
            "21155/22300 (epoch 47), train_loss = 0.738, time/batch = 0.092\n",
            "21156/22300 (epoch 47), train_loss = 0.734, time/batch = 0.093\n",
            "21157/22300 (epoch 47), train_loss = 0.704, time/batch = 0.092\n",
            "21158/22300 (epoch 47), train_loss = 0.721, time/batch = 0.091\n",
            "21159/22300 (epoch 47), train_loss = 0.706, time/batch = 0.092\n",
            "21160/22300 (epoch 47), train_loss = 0.754, time/batch = 0.093\n",
            "21161/22300 (epoch 47), train_loss = 0.724, time/batch = 0.092\n",
            "21162/22300 (epoch 47), train_loss = 0.751, time/batch = 0.093\n",
            "21163/22300 (epoch 47), train_loss = 0.709, time/batch = 0.093\n",
            "21164/22300 (epoch 47), train_loss = 0.705, time/batch = 0.091\n",
            "21165/22300 (epoch 47), train_loss = 0.692, time/batch = 0.093\n",
            "21166/22300 (epoch 47), train_loss = 0.708, time/batch = 0.092\n",
            "21167/22300 (epoch 47), train_loss = 0.700, time/batch = 0.092\n",
            "21168/22300 (epoch 47), train_loss = 0.747, time/batch = 0.092\n",
            "21169/22300 (epoch 47), train_loss = 0.714, time/batch = 0.092\n",
            "21170/22300 (epoch 47), train_loss = 0.752, time/batch = 0.094\n",
            "21171/22300 (epoch 47), train_loss = 0.691, time/batch = 0.091\n",
            "21172/22300 (epoch 47), train_loss = 0.709, time/batch = 0.093\n",
            "21173/22300 (epoch 47), train_loss = 0.701, time/batch = 0.091\n",
            "21174/22300 (epoch 47), train_loss = 0.720, time/batch = 0.092\n",
            "21175/22300 (epoch 47), train_loss = 0.721, time/batch = 0.092\n",
            "21176/22300 (epoch 47), train_loss = 0.702, time/batch = 0.092\n",
            "21177/22300 (epoch 47), train_loss = 0.714, time/batch = 0.093\n",
            "21178/22300 (epoch 47), train_loss = 0.709, time/batch = 0.091\n",
            "21179/22300 (epoch 47), train_loss = 0.741, time/batch = 0.093\n",
            "21180/22300 (epoch 47), train_loss = 0.723, time/batch = 0.092\n",
            "21181/22300 (epoch 47), train_loss = 0.721, time/batch = 0.093\n",
            "21182/22300 (epoch 47), train_loss = 0.744, time/batch = 0.092\n",
            "21183/22300 (epoch 47), train_loss = 0.710, time/batch = 0.096\n",
            "21184/22300 (epoch 47), train_loss = 0.719, time/batch = 0.093\n",
            "21185/22300 (epoch 47), train_loss = 0.748, time/batch = 0.091\n",
            "21186/22300 (epoch 47), train_loss = 0.706, time/batch = 0.092\n",
            "21187/22300 (epoch 47), train_loss = 0.742, time/batch = 0.092\n",
            "21188/22300 (epoch 47), train_loss = 0.770, time/batch = 0.092\n",
            "21189/22300 (epoch 47), train_loss = 0.749, time/batch = 0.092\n",
            "21190/22300 (epoch 47), train_loss = 0.757, time/batch = 0.093\n",
            "21191/22300 (epoch 47), train_loss = 0.756, time/batch = 0.093\n",
            "21192/22300 (epoch 47), train_loss = 0.745, time/batch = 0.094\n",
            "21193/22300 (epoch 47), train_loss = 0.744, time/batch = 0.093\n",
            "21194/22300 (epoch 47), train_loss = 0.737, time/batch = 0.093\n",
            "21195/22300 (epoch 47), train_loss = 0.735, time/batch = 0.091\n",
            "21196/22300 (epoch 47), train_loss = 0.760, time/batch = 0.097\n",
            "21197/22300 (epoch 47), train_loss = 0.724, time/batch = 0.098\n",
            "21198/22300 (epoch 47), train_loss = 0.742, time/batch = 0.091\n",
            "21199/22300 (epoch 47), train_loss = 0.713, time/batch = 0.092\n",
            "21200/22300 (epoch 47), train_loss = 0.741, time/batch = 0.091\n",
            "21201/22300 (epoch 47), train_loss = 0.737, time/batch = 0.091\n",
            "21202/22300 (epoch 47), train_loss = 0.760, time/batch = 0.092\n",
            "21203/22300 (epoch 47), train_loss = 0.724, time/batch = 0.092\n",
            "21204/22300 (epoch 47), train_loss = 0.758, time/batch = 0.093\n",
            "21205/22300 (epoch 47), train_loss = 0.761, time/batch = 0.092\n",
            "21206/22300 (epoch 47), train_loss = 0.720, time/batch = 0.094\n",
            "21207/22300 (epoch 47), train_loss = 0.754, time/batch = 0.092\n",
            "21208/22300 (epoch 47), train_loss = 0.756, time/batch = 0.093\n",
            "21209/22300 (epoch 47), train_loss = 0.703, time/batch = 0.092\n",
            "21210/22300 (epoch 47), train_loss = 0.753, time/batch = 0.091\n",
            "21211/22300 (epoch 47), train_loss = 0.754, time/batch = 0.093\n",
            "21212/22300 (epoch 47), train_loss = 0.764, time/batch = 0.096\n",
            "21213/22300 (epoch 47), train_loss = 0.752, time/batch = 0.092\n",
            "21214/22300 (epoch 47), train_loss = 0.774, time/batch = 0.110\n",
            "21215/22300 (epoch 47), train_loss = 0.772, time/batch = 0.093\n",
            "21216/22300 (epoch 47), train_loss = 0.750, time/batch = 0.102\n",
            "21217/22300 (epoch 47), train_loss = 0.761, time/batch = 0.098\n",
            "21218/22300 (epoch 47), train_loss = 0.728, time/batch = 0.092\n",
            "21219/22300 (epoch 47), train_loss = 0.747, time/batch = 0.091\n",
            "21220/22300 (epoch 47), train_loss = 0.731, time/batch = 0.092\n",
            "21221/22300 (epoch 47), train_loss = 0.754, time/batch = 0.093\n",
            "21222/22300 (epoch 47), train_loss = 0.716, time/batch = 0.092\n",
            "21223/22300 (epoch 47), train_loss = 0.776, time/batch = 0.094\n",
            "21224/22300 (epoch 47), train_loss = 0.712, time/batch = 0.093\n",
            "21225/22300 (epoch 47), train_loss = 0.764, time/batch = 0.092\n",
            "21226/22300 (epoch 47), train_loss = 0.739, time/batch = 0.093\n",
            "21227/22300 (epoch 47), train_loss = 0.757, time/batch = 0.095\n",
            "21228/22300 (epoch 47), train_loss = 0.739, time/batch = 0.092\n",
            "21229/22300 (epoch 47), train_loss = 0.775, time/batch = 0.092\n",
            "21230/22300 (epoch 47), train_loss = 0.763, time/batch = 0.104\n",
            "21231/22300 (epoch 47), train_loss = 0.744, time/batch = 0.095\n",
            "21232/22300 (epoch 47), train_loss = 0.774, time/batch = 0.094\n",
            "21233/22300 (epoch 47), train_loss = 0.766, time/batch = 0.093\n",
            "21234/22300 (epoch 47), train_loss = 0.783, time/batch = 0.096\n",
            "21235/22300 (epoch 47), train_loss = 0.804, time/batch = 0.091\n",
            "21236/22300 (epoch 47), train_loss = 0.778, time/batch = 0.094\n",
            "21237/22300 (epoch 47), train_loss = 0.777, time/batch = 0.093\n",
            "21238/22300 (epoch 47), train_loss = 0.798, time/batch = 0.092\n",
            "21239/22300 (epoch 47), train_loss = 0.751, time/batch = 0.093\n",
            "21240/22300 (epoch 47), train_loss = 0.735, time/batch = 0.094\n",
            "21241/22300 (epoch 47), train_loss = 0.738, time/batch = 0.093\n",
            "21242/22300 (epoch 47), train_loss = 0.739, time/batch = 0.092\n",
            "21243/22300 (epoch 47), train_loss = 0.779, time/batch = 0.092\n",
            "21244/22300 (epoch 47), train_loss = 0.760, time/batch = 0.093\n",
            "21245/22300 (epoch 47), train_loss = 0.735, time/batch = 0.092\n",
            "21246/22300 (epoch 47), train_loss = 0.737, time/batch = 0.093\n",
            "21247/22300 (epoch 47), train_loss = 0.745, time/batch = 0.092\n",
            "21248/22300 (epoch 47), train_loss = 0.766, time/batch = 0.093\n",
            "21249/22300 (epoch 47), train_loss = 0.776, time/batch = 0.093\n",
            "21250/22300 (epoch 47), train_loss = 0.752, time/batch = 0.092\n",
            "21251/22300 (epoch 47), train_loss = 0.761, time/batch = 0.092\n",
            "21252/22300 (epoch 47), train_loss = 0.724, time/batch = 0.092\n",
            "21253/22300 (epoch 47), train_loss = 0.739, time/batch = 0.092\n",
            "21254/22300 (epoch 47), train_loss = 0.758, time/batch = 0.096\n",
            "21255/22300 (epoch 47), train_loss = 0.744, time/batch = 0.091\n",
            "21256/22300 (epoch 47), train_loss = 0.752, time/batch = 0.093\n",
            "21257/22300 (epoch 47), train_loss = 0.745, time/batch = 0.091\n",
            "21258/22300 (epoch 47), train_loss = 0.715, time/batch = 0.091\n",
            "21259/22300 (epoch 47), train_loss = 0.733, time/batch = 0.092\n",
            "21260/22300 (epoch 47), train_loss = 0.735, time/batch = 0.099\n",
            "21261/22300 (epoch 47), train_loss = 0.765, time/batch = 0.094\n",
            "21262/22300 (epoch 47), train_loss = 0.722, time/batch = 0.091\n",
            "21263/22300 (epoch 47), train_loss = 0.752, time/batch = 0.093\n",
            "21264/22300 (epoch 47), train_loss = 0.702, time/batch = 0.092\n",
            "21265/22300 (epoch 47), train_loss = 0.724, time/batch = 0.092\n",
            "21266/22300 (epoch 47), train_loss = 0.739, time/batch = 0.093\n",
            "21267/22300 (epoch 47), train_loss = 0.735, time/batch = 0.091\n",
            "21268/22300 (epoch 47), train_loss = 0.718, time/batch = 0.092\n",
            "21269/22300 (epoch 47), train_loss = 0.766, time/batch = 0.092\n",
            "21270/22300 (epoch 47), train_loss = 0.750, time/batch = 0.093\n",
            "21271/22300 (epoch 47), train_loss = 0.735, time/batch = 0.093\n",
            "21272/22300 (epoch 47), train_loss = 0.711, time/batch = 0.098\n",
            "21273/22300 (epoch 47), train_loss = 0.702, time/batch = 0.092\n",
            "21274/22300 (epoch 47), train_loss = 0.712, time/batch = 0.092\n",
            "21275/22300 (epoch 47), train_loss = 0.723, time/batch = 0.092\n",
            "21276/22300 (epoch 47), train_loss = 0.717, time/batch = 0.093\n",
            "21277/22300 (epoch 47), train_loss = 0.718, time/batch = 0.092\n",
            "21278/22300 (epoch 47), train_loss = 0.706, time/batch = 0.093\n",
            "21279/22300 (epoch 47), train_loss = 0.746, time/batch = 0.092\n",
            "21280/22300 (epoch 47), train_loss = 0.714, time/batch = 0.092\n",
            "21281/22300 (epoch 47), train_loss = 0.707, time/batch = 0.092\n",
            "21282/22300 (epoch 47), train_loss = 0.710, time/batch = 0.093\n",
            "21283/22300 (epoch 47), train_loss = 0.743, time/batch = 0.098\n",
            "21284/22300 (epoch 47), train_loss = 0.728, time/batch = 0.092\n",
            "21285/22300 (epoch 47), train_loss = 0.695, time/batch = 0.092\n",
            "21286/22300 (epoch 47), train_loss = 0.697, time/batch = 0.092\n",
            "21287/22300 (epoch 47), train_loss = 0.725, time/batch = 0.091\n",
            "21288/22300 (epoch 47), train_loss = 0.743, time/batch = 0.093\n",
            "21289/22300 (epoch 47), train_loss = 0.723, time/batch = 0.092\n",
            "21290/22300 (epoch 47), train_loss = 0.759, time/batch = 0.092\n",
            "21291/22300 (epoch 47), train_loss = 0.723, time/batch = 0.091\n",
            "21292/22300 (epoch 47), train_loss = 0.701, time/batch = 0.099\n",
            "21293/22300 (epoch 47), train_loss = 0.722, time/batch = 0.092\n",
            "21294/22300 (epoch 47), train_loss = 0.716, time/batch = 0.095\n",
            "21295/22300 (epoch 47), train_loss = 0.678, time/batch = 0.091\n",
            "21296/22300 (epoch 47), train_loss = 0.710, time/batch = 0.092\n",
            "21297/22300 (epoch 47), train_loss = 0.722, time/batch = 0.092\n",
            "21298/22300 (epoch 47), train_loss = 0.738, time/batch = 0.093\n",
            "21299/22300 (epoch 47), train_loss = 0.706, time/batch = 0.091\n",
            "21300/22300 (epoch 47), train_loss = 0.714, time/batch = 0.093\n",
            "21301/22300 (epoch 47), train_loss = 0.729, time/batch = 0.092\n",
            "21302/22300 (epoch 47), train_loss = 0.751, time/batch = 0.092\n",
            "21303/22300 (epoch 47), train_loss = 0.713, time/batch = 0.094\n",
            "21304/22300 (epoch 47), train_loss = 0.706, time/batch = 0.092\n",
            "21305/22300 (epoch 47), train_loss = 0.734, time/batch = 0.092\n",
            "21306/22300 (epoch 47), train_loss = 0.725, time/batch = 0.093\n",
            "21307/22300 (epoch 47), train_loss = 0.730, time/batch = 0.099\n",
            "21308/22300 (epoch 47), train_loss = 0.704, time/batch = 0.092\n",
            "21309/22300 (epoch 47), train_loss = 0.744, time/batch = 0.091\n",
            "21310/22300 (epoch 47), train_loss = 0.685, time/batch = 0.092\n",
            "21311/22300 (epoch 47), train_loss = 0.711, time/batch = 0.092\n",
            "21312/22300 (epoch 47), train_loss = 0.707, time/batch = 0.092\n",
            "21313/22300 (epoch 47), train_loss = 0.716, time/batch = 0.094\n",
            "21314/22300 (epoch 47), train_loss = 0.709, time/batch = 0.092\n",
            "21315/22300 (epoch 47), train_loss = 0.714, time/batch = 0.094\n",
            "21316/22300 (epoch 47), train_loss = 0.716, time/batch = 0.091\n",
            "21317/22300 (epoch 47), train_loss = 0.686, time/batch = 0.092\n",
            "21318/22300 (epoch 47), train_loss = 0.732, time/batch = 0.093\n",
            "21319/22300 (epoch 47), train_loss = 0.735, time/batch = 0.091\n",
            "21320/22300 (epoch 47), train_loss = 0.719, time/batch = 0.093\n",
            "21321/22300 (epoch 47), train_loss = 0.733, time/batch = 0.091\n",
            "21322/22300 (epoch 47), train_loss = 0.736, time/batch = 0.092\n",
            "21323/22300 (epoch 47), train_loss = 0.724, time/batch = 0.092\n",
            "21324/22300 (epoch 47), train_loss = 0.736, time/batch = 0.097\n",
            "21325/22300 (epoch 47), train_loss = 0.722, time/batch = 0.093\n",
            "21326/22300 (epoch 47), train_loss = 0.741, time/batch = 0.092\n",
            "21327/22300 (epoch 47), train_loss = 0.722, time/batch = 0.092\n",
            "21328/22300 (epoch 47), train_loss = 0.714, time/batch = 0.092\n",
            "21329/22300 (epoch 47), train_loss = 0.682, time/batch = 0.092\n",
            "21330/22300 (epoch 47), train_loss = 0.685, time/batch = 0.092\n",
            "21331/22300 (epoch 47), train_loss = 0.702, time/batch = 0.092\n",
            "21332/22300 (epoch 47), train_loss = 0.728, time/batch = 0.092\n",
            "21333/22300 (epoch 47), train_loss = 0.679, time/batch = 0.099\n",
            "21334/22300 (epoch 47), train_loss = 0.730, time/batch = 0.091\n",
            "21335/22300 (epoch 47), train_loss = 0.698, time/batch = 0.094\n",
            "21336/22300 (epoch 47), train_loss = 0.730, time/batch = 0.093\n",
            "21337/22300 (epoch 47), train_loss = 0.705, time/batch = 0.093\n",
            "21338/22300 (epoch 47), train_loss = 0.710, time/batch = 0.092\n",
            "21339/22300 (epoch 47), train_loss = 0.736, time/batch = 0.092\n",
            "21340/22300 (epoch 47), train_loss = 0.711, time/batch = 0.092\n",
            "21341/22300 (epoch 47), train_loss = 0.679, time/batch = 0.092\n",
            "21342/22300 (epoch 47), train_loss = 0.699, time/batch = 0.093\n",
            "21343/22300 (epoch 47), train_loss = 0.713, time/batch = 0.092\n",
            "21344/22300 (epoch 47), train_loss = 0.687, time/batch = 0.093\n",
            "21345/22300 (epoch 47), train_loss = 0.667, time/batch = 0.092\n",
            "21346/22300 (epoch 47), train_loss = 0.685, time/batch = 0.097\n",
            "21347/22300 (epoch 47), train_loss = 0.689, time/batch = 0.092\n",
            "21348/22300 (epoch 47), train_loss = 0.688, time/batch = 0.091\n",
            "21349/22300 (epoch 47), train_loss = 0.705, time/batch = 0.093\n",
            "21350/22300 (epoch 47), train_loss = 0.678, time/batch = 0.095\n",
            "21351/22300 (epoch 47), train_loss = 0.678, time/batch = 0.092\n",
            "21352/22300 (epoch 47), train_loss = 0.685, time/batch = 0.095\n",
            "21353/22300 (epoch 47), train_loss = 0.676, time/batch = 0.091\n",
            "21354/22300 (epoch 47), train_loss = 0.702, time/batch = 0.092\n",
            "21355/22300 (epoch 47), train_loss = 0.720, time/batch = 0.092\n",
            "21356/22300 (epoch 47), train_loss = 0.689, time/batch = 0.092\n",
            "21357/22300 (epoch 47), train_loss = 0.716, time/batch = 0.094\n",
            "21358/22300 (epoch 47), train_loss = 0.710, time/batch = 0.091\n",
            "21359/22300 (epoch 47), train_loss = 0.707, time/batch = 0.094\n",
            "21360/22300 (epoch 47), train_loss = 0.721, time/batch = 0.092\n",
            "21361/22300 (epoch 47), train_loss = 0.686, time/batch = 0.091\n",
            "21362/22300 (epoch 47), train_loss = 0.709, time/batch = 0.094\n",
            "21363/22300 (epoch 47), train_loss = 0.732, time/batch = 0.093\n",
            "21364/22300 (epoch 47), train_loss = 0.733, time/batch = 0.094\n",
            "21365/22300 (epoch 47), train_loss = 0.712, time/batch = 0.093\n",
            "21366/22300 (epoch 47), train_loss = 0.720, time/batch = 0.099\n",
            "21367/22300 (epoch 47), train_loss = 0.720, time/batch = 0.100\n",
            "21368/22300 (epoch 47), train_loss = 0.714, time/batch = 0.093\n",
            "21369/22300 (epoch 47), train_loss = 0.738, time/batch = 0.092\n",
            "21370/22300 (epoch 47), train_loss = 0.707, time/batch = 0.092\n",
            "21371/22300 (epoch 47), train_loss = 0.739, time/batch = 0.092\n",
            "21372/22300 (epoch 47), train_loss = 0.730, time/batch = 0.092\n",
            "21373/22300 (epoch 47), train_loss = 0.692, time/batch = 0.094\n",
            "21374/22300 (epoch 47), train_loss = 0.715, time/batch = 0.092\n",
            "21375/22300 (epoch 47), train_loss = 0.685, time/batch = 0.092\n",
            "21376/22300 (epoch 47), train_loss = 0.737, time/batch = 0.092\n",
            "21377/22300 (epoch 47), train_loss = 0.752, time/batch = 0.093\n",
            "21378/22300 (epoch 47), train_loss = 0.741, time/batch = 0.094\n",
            "21379/22300 (epoch 47), train_loss = 0.747, time/batch = 0.092\n",
            "21380/22300 (epoch 47), train_loss = 0.740, time/batch = 0.092\n",
            "21381/22300 (epoch 47), train_loss = 0.726, time/batch = 0.091\n",
            "21382/22300 (epoch 47), train_loss = 0.733, time/batch = 0.092\n",
            "21383/22300 (epoch 47), train_loss = 0.713, time/batch = 0.092\n",
            "21384/22300 (epoch 47), train_loss = 0.716, time/batch = 0.092\n",
            "21385/22300 (epoch 47), train_loss = 0.712, time/batch = 0.093\n",
            "21386/22300 (epoch 47), train_loss = 0.732, time/batch = 0.092\n",
            "21387/22300 (epoch 47), train_loss = 0.714, time/batch = 0.093\n",
            "21388/22300 (epoch 47), train_loss = 0.771, time/batch = 0.092\n",
            "21389/22300 (epoch 47), train_loss = 0.702, time/batch = 0.092\n",
            "21390/22300 (epoch 47), train_loss = 0.736, time/batch = 0.092\n",
            "21391/22300 (epoch 47), train_loss = 0.716, time/batch = 0.093\n",
            "21392/22300 (epoch 47), train_loss = 0.708, time/batch = 0.092\n",
            "21393/22300 (epoch 47), train_loss = 0.760, time/batch = 0.092\n",
            "21394/22300 (epoch 47), train_loss = 0.735, time/batch = 0.093\n",
            "21395/22300 (epoch 47), train_loss = 0.744, time/batch = 0.091\n",
            "21396/22300 (epoch 47), train_loss = 0.717, time/batch = 0.092\n",
            "21397/22300 (epoch 47), train_loss = 0.703, time/batch = 0.092\n",
            "21398/22300 (epoch 47), train_loss = 0.684, time/batch = 0.093\n",
            "21399/22300 (epoch 47), train_loss = 0.710, time/batch = 0.094\n",
            "21400/22300 (epoch 47), train_loss = 0.778, time/batch = 0.093\n",
            "21401/22300 (epoch 47), train_loss = 0.712, time/batch = 0.093\n",
            "21402/22300 (epoch 47), train_loss = 0.719, time/batch = 0.092\n",
            "21403/22300 (epoch 47), train_loss = 0.699, time/batch = 0.092\n",
            "21404/22300 (epoch 47), train_loss = 0.725, time/batch = 0.092\n",
            "21405/22300 (epoch 47), train_loss = 0.717, time/batch = 0.092\n",
            "21406/22300 (epoch 47), train_loss = 0.723, time/batch = 0.092\n",
            "21407/22300 (epoch 47), train_loss = 0.713, time/batch = 0.092\n",
            "21408/22300 (epoch 48), train_loss = 0.448, time/batch = 0.085\n",
            "21409/22300 (epoch 48), train_loss = 0.754, time/batch = 0.093\n",
            "21410/22300 (epoch 48), train_loss = 0.792, time/batch = 0.092\n",
            "21411/22300 (epoch 48), train_loss = 0.781, time/batch = 0.092\n",
            "21412/22300 (epoch 48), train_loss = 0.791, time/batch = 0.091\n",
            "21413/22300 (epoch 48), train_loss = 0.730, time/batch = 0.091\n",
            "21414/22300 (epoch 48), train_loss = 0.758, time/batch = 0.092\n",
            "21415/22300 (epoch 48), train_loss = 0.749, time/batch = 0.092\n",
            "21416/22300 (epoch 48), train_loss = 0.722, time/batch = 0.092\n",
            "21417/22300 (epoch 48), train_loss = 0.720, time/batch = 0.099\n",
            "21418/22300 (epoch 48), train_loss = 0.762, time/batch = 0.094\n",
            "21419/22300 (epoch 48), train_loss = 0.713, time/batch = 0.091\n",
            "21420/22300 (epoch 48), train_loss = 0.739, time/batch = 0.091\n",
            "21421/22300 (epoch 48), train_loss = 0.749, time/batch = 0.091\n",
            "21422/22300 (epoch 48), train_loss = 0.742, time/batch = 0.093\n",
            "21423/22300 (epoch 48), train_loss = 0.776, time/batch = 0.091\n",
            "21424/22300 (epoch 48), train_loss = 0.775, time/batch = 0.091\n",
            "21425/22300 (epoch 48), train_loss = 0.743, time/batch = 0.093\n",
            "21426/22300 (epoch 48), train_loss = 0.746, time/batch = 0.091\n",
            "21427/22300 (epoch 48), train_loss = 0.777, time/batch = 0.095\n",
            "21428/22300 (epoch 48), train_loss = 0.746, time/batch = 0.093\n",
            "21429/22300 (epoch 48), train_loss = 0.724, time/batch = 0.094\n",
            "21430/22300 (epoch 48), train_loss = 0.741, time/batch = 0.093\n",
            "21431/22300 (epoch 48), train_loss = 0.723, time/batch = 0.092\n",
            "21432/22300 (epoch 48), train_loss = 0.700, time/batch = 0.092\n",
            "21433/22300 (epoch 48), train_loss = 0.725, time/batch = 0.091\n",
            "21434/22300 (epoch 48), train_loss = 0.749, time/batch = 0.093\n",
            "21435/22300 (epoch 48), train_loss = 0.715, time/batch = 0.091\n",
            "21436/22300 (epoch 48), train_loss = 0.745, time/batch = 0.091\n",
            "21437/22300 (epoch 48), train_loss = 0.714, time/batch = 0.093\n",
            "21438/22300 (epoch 48), train_loss = 0.720, time/batch = 0.093\n",
            "21439/22300 (epoch 48), train_loss = 0.741, time/batch = 0.093\n",
            "21440/22300 (epoch 48), train_loss = 0.732, time/batch = 0.092\n",
            "21441/22300 (epoch 48), train_loss = 0.748, time/batch = 0.092\n",
            "21442/22300 (epoch 48), train_loss = 0.744, time/batch = 0.092\n",
            "21443/22300 (epoch 48), train_loss = 0.722, time/batch = 0.092\n",
            "21444/22300 (epoch 48), train_loss = 0.738, time/batch = 0.093\n",
            "21445/22300 (epoch 48), train_loss = 0.718, time/batch = 0.091\n",
            "21446/22300 (epoch 48), train_loss = 0.734, time/batch = 0.093\n",
            "21447/22300 (epoch 48), train_loss = 0.708, time/batch = 0.091\n",
            "21448/22300 (epoch 48), train_loss = 0.720, time/batch = 0.093\n",
            "21449/22300 (epoch 48), train_loss = 0.744, time/batch = 0.092\n",
            "21450/22300 (epoch 48), train_loss = 0.744, time/batch = 0.092\n",
            "21451/22300 (epoch 48), train_loss = 0.737, time/batch = 0.093\n",
            "21452/22300 (epoch 48), train_loss = 0.705, time/batch = 0.092\n",
            "21453/22300 (epoch 48), train_loss = 0.768, time/batch = 0.093\n",
            "21454/22300 (epoch 48), train_loss = 0.732, time/batch = 0.091\n",
            "21455/22300 (epoch 48), train_loss = 0.699, time/batch = 0.092\n",
            "21456/22300 (epoch 48), train_loss = 0.736, time/batch = 0.091\n",
            "21457/22300 (epoch 48), train_loss = 0.737, time/batch = 0.092\n",
            "21458/22300 (epoch 48), train_loss = 0.747, time/batch = 0.099\n",
            "21459/22300 (epoch 48), train_loss = 0.695, time/batch = 0.093\n",
            "21460/22300 (epoch 48), train_loss = 0.701, time/batch = 0.092\n",
            "21461/22300 (epoch 48), train_loss = 0.718, time/batch = 0.097\n",
            "21462/22300 (epoch 48), train_loss = 0.696, time/batch = 0.092\n",
            "21463/22300 (epoch 48), train_loss = 0.708, time/batch = 0.093\n",
            "21464/22300 (epoch 48), train_loss = 0.709, time/batch = 0.092\n",
            "21465/22300 (epoch 48), train_loss = 0.750, time/batch = 0.092\n",
            "21466/22300 (epoch 48), train_loss = 0.732, time/batch = 0.093\n",
            "21467/22300 (epoch 48), train_loss = 0.703, time/batch = 0.091\n",
            "21468/22300 (epoch 48), train_loss = 0.752, time/batch = 0.094\n",
            "21469/22300 (epoch 48), train_loss = 0.719, time/batch = 0.092\n",
            "21470/22300 (epoch 48), train_loss = 0.706, time/batch = 0.092\n",
            "21471/22300 (epoch 48), train_loss = 0.740, time/batch = 0.096\n",
            "21472/22300 (epoch 48), train_loss = 0.714, time/batch = 0.094\n",
            "21473/22300 (epoch 48), train_loss = 0.727, time/batch = 0.092\n",
            "21474/22300 (epoch 48), train_loss = 0.720, time/batch = 0.091\n",
            "21475/22300 (epoch 48), train_loss = 0.771, time/batch = 0.092\n",
            "21476/22300 (epoch 48), train_loss = 0.733, time/batch = 0.092\n",
            "21477/22300 (epoch 48), train_loss = 0.744, time/batch = 0.092\n",
            "21478/22300 (epoch 48), train_loss = 0.730, time/batch = 0.093\n",
            "21479/22300 (epoch 48), train_loss = 0.684, time/batch = 0.092\n",
            "21480/22300 (epoch 48), train_loss = 0.716, time/batch = 0.093\n",
            "21481/22300 (epoch 48), train_loss = 0.705, time/batch = 0.093\n",
            "21482/22300 (epoch 48), train_loss = 0.694, time/batch = 0.093\n",
            "21483/22300 (epoch 48), train_loss = 0.741, time/batch = 0.091\n",
            "21484/22300 (epoch 48), train_loss = 0.725, time/batch = 0.092\n",
            "21485/22300 (epoch 48), train_loss = 0.715, time/batch = 0.093\n",
            "21486/22300 (epoch 48), train_loss = 0.694, time/batch = 0.092\n",
            "21487/22300 (epoch 48), train_loss = 0.739, time/batch = 0.092\n",
            "21488/22300 (epoch 48), train_loss = 0.718, time/batch = 0.092\n",
            "21489/22300 (epoch 48), train_loss = 0.696, time/batch = 0.093\n",
            "21490/22300 (epoch 48), train_loss = 0.708, time/batch = 0.091\n",
            "21491/22300 (epoch 48), train_loss = 0.697, time/batch = 0.092\n",
            "21492/22300 (epoch 48), train_loss = 0.714, time/batch = 0.092\n",
            "21493/22300 (epoch 48), train_loss = 0.713, time/batch = 0.092\n",
            "21494/22300 (epoch 48), train_loss = 0.698, time/batch = 0.092\n",
            "21495/22300 (epoch 48), train_loss = 0.735, time/batch = 0.092\n",
            "21496/22300 (epoch 48), train_loss = 0.706, time/batch = 0.092\n",
            "21497/22300 (epoch 48), train_loss = 0.743, time/batch = 0.092\n",
            "21498/22300 (epoch 48), train_loss = 0.696, time/batch = 0.092\n",
            "21499/22300 (epoch 48), train_loss = 0.725, time/batch = 0.092\n",
            "21500/22300 (epoch 48), train_loss = 0.742, time/batch = 0.092\n",
            "21501/22300 (epoch 48), train_loss = 0.712, time/batch = 0.092\n",
            "21502/22300 (epoch 48), train_loss = 0.724, time/batch = 0.092\n",
            "21503/22300 (epoch 48), train_loss = 0.732, time/batch = 0.093\n",
            "21504/22300 (epoch 48), train_loss = 0.689, time/batch = 0.092\n",
            "21505/22300 (epoch 48), train_loss = 0.733, time/batch = 0.092\n",
            "21506/22300 (epoch 48), train_loss = 0.680, time/batch = 0.091\n",
            "21507/22300 (epoch 48), train_loss = 0.699, time/batch = 0.093\n",
            "21508/22300 (epoch 48), train_loss = 0.697, time/batch = 0.092\n",
            "21509/22300 (epoch 48), train_loss = 0.709, time/batch = 0.094\n",
            "21510/22300 (epoch 48), train_loss = 0.713, time/batch = 0.095\n",
            "21511/22300 (epoch 48), train_loss = 0.686, time/batch = 0.092\n",
            "21512/22300 (epoch 48), train_loss = 0.719, time/batch = 0.092\n",
            "21513/22300 (epoch 48), train_loss = 0.693, time/batch = 0.092\n",
            "21514/22300 (epoch 48), train_loss = 0.689, time/batch = 0.092\n",
            "21515/22300 (epoch 48), train_loss = 0.692, time/batch = 0.092\n",
            "21516/22300 (epoch 48), train_loss = 0.709, time/batch = 0.092\n",
            "21517/22300 (epoch 48), train_loss = 0.701, time/batch = 0.093\n",
            "21518/22300 (epoch 48), train_loss = 0.686, time/batch = 0.092\n",
            "21519/22300 (epoch 48), train_loss = 0.684, time/batch = 0.092\n",
            "21520/22300 (epoch 48), train_loss = 0.704, time/batch = 0.092\n",
            "21521/22300 (epoch 48), train_loss = 0.664, time/batch = 0.092\n",
            "21522/22300 (epoch 48), train_loss = 0.703, time/batch = 0.092\n",
            "21523/22300 (epoch 48), train_loss = 0.685, time/batch = 0.093\n",
            "21524/22300 (epoch 48), train_loss = 0.677, time/batch = 0.093\n",
            "21525/22300 (epoch 48), train_loss = 0.721, time/batch = 0.091\n",
            "21526/22300 (epoch 48), train_loss = 0.729, time/batch = 0.092\n",
            "21527/22300 (epoch 48), train_loss = 0.703, time/batch = 0.092\n",
            "21528/22300 (epoch 48), train_loss = 0.765, time/batch = 0.093\n",
            "21529/22300 (epoch 48), train_loss = 0.736, time/batch = 0.092\n",
            "21530/22300 (epoch 48), train_loss = 0.731, time/batch = 0.093\n",
            "21531/22300 (epoch 48), train_loss = 0.719, time/batch = 0.092\n",
            "21532/22300 (epoch 48), train_loss = 0.754, time/batch = 0.092\n",
            "21533/22300 (epoch 48), train_loss = 0.735, time/batch = 0.092\n",
            "21534/22300 (epoch 48), train_loss = 0.716, time/batch = 0.094\n",
            "21535/22300 (epoch 48), train_loss = 0.725, time/batch = 0.094\n",
            "21536/22300 (epoch 48), train_loss = 0.734, time/batch = 0.091\n",
            "21537/22300 (epoch 48), train_loss = 0.730, time/batch = 0.092\n",
            "21538/22300 (epoch 48), train_loss = 0.708, time/batch = 0.096\n",
            "21539/22300 (epoch 48), train_loss = 0.731, time/batch = 0.093\n",
            "21540/22300 (epoch 48), train_loss = 0.756, time/batch = 0.092\n",
            "21541/22300 (epoch 48), train_loss = 0.770, time/batch = 0.091\n",
            "21542/22300 (epoch 48), train_loss = 0.721, time/batch = 0.091\n",
            "21543/22300 (epoch 48), train_loss = 0.757, time/batch = 0.091\n",
            "21544/22300 (epoch 48), train_loss = 0.757, time/batch = 0.092\n",
            "21545/22300 (epoch 48), train_loss = 0.758, time/batch = 0.093\n",
            "21546/22300 (epoch 48), train_loss = 0.741, time/batch = 0.092\n",
            "21547/22300 (epoch 48), train_loss = 0.773, time/batch = 0.092\n",
            "21548/22300 (epoch 48), train_loss = 0.727, time/batch = 0.092\n",
            "21549/22300 (epoch 48), train_loss = 0.740, time/batch = 0.093\n",
            "21550/22300 (epoch 48), train_loss = 0.717, time/batch = 0.091\n",
            "21551/22300 (epoch 48), train_loss = 0.733, time/batch = 0.092\n",
            "21552/22300 (epoch 48), train_loss = 0.731, time/batch = 0.092\n",
            "21553/22300 (epoch 48), train_loss = 0.782, time/batch = 0.091\n",
            "21554/22300 (epoch 48), train_loss = 0.739, time/batch = 0.091\n",
            "21555/22300 (epoch 48), train_loss = 0.735, time/batch = 0.092\n",
            "21556/22300 (epoch 48), train_loss = 0.758, time/batch = 0.092\n",
            "21557/22300 (epoch 48), train_loss = 0.749, time/batch = 0.092\n",
            "21558/22300 (epoch 48), train_loss = 0.758, time/batch = 0.094\n",
            "21559/22300 (epoch 48), train_loss = 0.761, time/batch = 0.100\n",
            "21560/22300 (epoch 48), train_loss = 0.754, time/batch = 0.092\n",
            "21561/22300 (epoch 48), train_loss = 0.711, time/batch = 0.093\n",
            "21562/22300 (epoch 48), train_loss = 0.732, time/batch = 0.091\n",
            "21563/22300 (epoch 48), train_loss = 0.724, time/batch = 0.093\n",
            "21564/22300 (epoch 48), train_loss = 0.727, time/batch = 0.091\n",
            "21565/22300 (epoch 48), train_loss = 0.707, time/batch = 0.092\n",
            "21566/22300 (epoch 48), train_loss = 0.745, time/batch = 0.093\n",
            "21567/22300 (epoch 48), train_loss = 0.718, time/batch = 0.096\n",
            "21568/22300 (epoch 48), train_loss = 0.694, time/batch = 0.093\n",
            "21569/22300 (epoch 48), train_loss = 0.729, time/batch = 0.094\n",
            "21570/22300 (epoch 48), train_loss = 0.729, time/batch = 0.091\n",
            "21571/22300 (epoch 48), train_loss = 0.733, time/batch = 0.092\n",
            "21572/22300 (epoch 48), train_loss = 0.727, time/batch = 0.091\n",
            "21573/22300 (epoch 48), train_loss = 0.721, time/batch = 0.095\n",
            "21574/22300 (epoch 48), train_loss = 0.696, time/batch = 0.092\n",
            "21575/22300 (epoch 48), train_loss = 0.712, time/batch = 0.096\n",
            "21576/22300 (epoch 48), train_loss = 0.721, time/batch = 0.093\n",
            "21577/22300 (epoch 48), train_loss = 0.741, time/batch = 0.094\n",
            "21578/22300 (epoch 48), train_loss = 0.712, time/batch = 0.094\n",
            "21579/22300 (epoch 48), train_loss = 0.709, time/batch = 0.094\n",
            "21580/22300 (epoch 48), train_loss = 0.703, time/batch = 0.092\n",
            "21581/22300 (epoch 48), train_loss = 0.686, time/batch = 0.093\n",
            "21582/22300 (epoch 48), train_loss = 0.693, time/batch = 0.092\n",
            "21583/22300 (epoch 48), train_loss = 0.668, time/batch = 0.092\n",
            "21584/22300 (epoch 48), train_loss = 0.664, time/batch = 0.093\n",
            "21585/22300 (epoch 48), train_loss = 0.686, time/batch = 0.096\n",
            "21586/22300 (epoch 48), train_loss = 0.640, time/batch = 0.092\n",
            "21587/22300 (epoch 48), train_loss = 0.717, time/batch = 0.093\n",
            "21588/22300 (epoch 48), train_loss = 0.693, time/batch = 0.096\n",
            "21589/22300 (epoch 48), train_loss = 0.676, time/batch = 0.093\n",
            "21590/22300 (epoch 48), train_loss = 0.709, time/batch = 0.092\n",
            "21591/22300 (epoch 48), train_loss = 0.693, time/batch = 0.093\n",
            "21592/22300 (epoch 48), train_loss = 0.689, time/batch = 0.093\n",
            "21593/22300 (epoch 48), train_loss = 0.654, time/batch = 0.091\n",
            "21594/22300 (epoch 48), train_loss = 0.711, time/batch = 0.093\n",
            "21595/22300 (epoch 48), train_loss = 0.681, time/batch = 0.092\n",
            "21596/22300 (epoch 48), train_loss = 0.709, time/batch = 0.097\n",
            "21597/22300 (epoch 48), train_loss = 0.680, time/batch = 0.092\n",
            "21598/22300 (epoch 48), train_loss = 0.702, time/batch = 0.094\n",
            "21599/22300 (epoch 48), train_loss = 0.702, time/batch = 0.091\n",
            "21600/22300 (epoch 48), train_loss = 0.705, time/batch = 0.093\n",
            "21601/22300 (epoch 48), train_loss = 0.723, time/batch = 0.092\n",
            "21602/22300 (epoch 48), train_loss = 0.711, time/batch = 0.093\n",
            "21603/22300 (epoch 48), train_loss = 0.685, time/batch = 0.091\n",
            "21604/22300 (epoch 48), train_loss = 0.703, time/batch = 0.093\n",
            "21605/22300 (epoch 48), train_loss = 0.691, time/batch = 0.099\n",
            "21606/22300 (epoch 48), train_loss = 0.741, time/batch = 0.091\n",
            "21607/22300 (epoch 48), train_loss = 0.705, time/batch = 0.092\n",
            "21608/22300 (epoch 48), train_loss = 0.742, time/batch = 0.091\n",
            "21609/22300 (epoch 48), train_loss = 0.696, time/batch = 0.092\n",
            "21610/22300 (epoch 48), train_loss = 0.684, time/batch = 0.092\n",
            "21611/22300 (epoch 48), train_loss = 0.676, time/batch = 0.096\n",
            "21612/22300 (epoch 48), train_loss = 0.693, time/batch = 0.093\n",
            "21613/22300 (epoch 48), train_loss = 0.685, time/batch = 0.092\n",
            "21614/22300 (epoch 48), train_loss = 0.739, time/batch = 0.094\n",
            "21615/22300 (epoch 48), train_loss = 0.704, time/batch = 0.095\n",
            "21616/22300 (epoch 48), train_loss = 0.742, time/batch = 0.091\n",
            "21617/22300 (epoch 48), train_loss = 0.684, time/batch = 0.092\n",
            "21618/22300 (epoch 48), train_loss = 0.700, time/batch = 0.091\n",
            "21619/22300 (epoch 48), train_loss = 0.693, time/batch = 0.092\n",
            "21620/22300 (epoch 48), train_loss = 0.701, time/batch = 0.092\n",
            "21621/22300 (epoch 48), train_loss = 0.689, time/batch = 0.092\n",
            "21622/22300 (epoch 48), train_loss = 0.676, time/batch = 0.093\n",
            "21623/22300 (epoch 48), train_loss = 0.689, time/batch = 0.092\n",
            "21624/22300 (epoch 48), train_loss = 0.687, time/batch = 0.093\n",
            "21625/22300 (epoch 48), train_loss = 0.733, time/batch = 0.101\n",
            "21626/22300 (epoch 48), train_loss = 0.709, time/batch = 0.089\n",
            "21627/22300 (epoch 48), train_loss = 0.711, time/batch = 0.093\n",
            "21628/22300 (epoch 48), train_loss = 0.750, time/batch = 0.092\n",
            "21629/22300 (epoch 48), train_loss = 0.710, time/batch = 0.092\n",
            "21630/22300 (epoch 48), train_loss = 0.723, time/batch = 0.091\n",
            "21631/22300 (epoch 48), train_loss = 0.750, time/batch = 0.092\n",
            "21632/22300 (epoch 48), train_loss = 0.684, time/batch = 0.091\n",
            "21633/22300 (epoch 48), train_loss = 0.726, time/batch = 0.092\n",
            "21634/22300 (epoch 48), train_loss = 0.748, time/batch = 0.093\n",
            "21635/22300 (epoch 48), train_loss = 0.719, time/batch = 0.095\n",
            "21636/22300 (epoch 48), train_loss = 0.733, time/batch = 0.093\n",
            "21637/22300 (epoch 48), train_loss = 0.742, time/batch = 0.092\n",
            "21638/22300 (epoch 48), train_loss = 0.733, time/batch = 0.095\n",
            "21639/22300 (epoch 48), train_loss = 0.741, time/batch = 0.096\n",
            "21640/22300 (epoch 48), train_loss = 0.738, time/batch = 0.092\n",
            "21641/22300 (epoch 48), train_loss = 0.734, time/batch = 0.093\n",
            "21642/22300 (epoch 48), train_loss = 0.763, time/batch = 0.100\n",
            "21643/22300 (epoch 48), train_loss = 0.717, time/batch = 0.091\n",
            "21644/22300 (epoch 48), train_loss = 0.720, time/batch = 0.096\n",
            "21645/22300 (epoch 48), train_loss = 0.688, time/batch = 0.094\n",
            "21646/22300 (epoch 48), train_loss = 0.717, time/batch = 0.092\n",
            "21647/22300 (epoch 48), train_loss = 0.720, time/batch = 0.093\n",
            "21648/22300 (epoch 48), train_loss = 0.739, time/batch = 0.092\n",
            "21649/22300 (epoch 48), train_loss = 0.707, time/batch = 0.092\n",
            "21650/22300 (epoch 48), train_loss = 0.740, time/batch = 0.093\n",
            "21651/22300 (epoch 48), train_loss = 0.753, time/batch = 0.092\n",
            "21652/22300 (epoch 48), train_loss = 0.715, time/batch = 0.093\n",
            "21653/22300 (epoch 48), train_loss = 0.750, time/batch = 0.091\n",
            "21654/22300 (epoch 48), train_loss = 0.751, time/batch = 0.092\n",
            "21655/22300 (epoch 48), train_loss = 0.706, time/batch = 0.093\n",
            "21656/22300 (epoch 48), train_loss = 0.756, time/batch = 0.092\n",
            "21657/22300 (epoch 48), train_loss = 0.737, time/batch = 0.099\n",
            "21658/22300 (epoch 48), train_loss = 0.743, time/batch = 0.093\n",
            "21659/22300 (epoch 48), train_loss = 0.726, time/batch = 0.092\n",
            "21660/22300 (epoch 48), train_loss = 0.748, time/batch = 0.093\n",
            "21661/22300 (epoch 48), train_loss = 0.766, time/batch = 0.091\n",
            "21662/22300 (epoch 48), train_loss = 0.749, time/batch = 0.094\n",
            "21663/22300 (epoch 48), train_loss = 0.765, time/batch = 0.091\n",
            "21664/22300 (epoch 48), train_loss = 0.730, time/batch = 0.091\n",
            "21665/22300 (epoch 48), train_loss = 0.764, time/batch = 0.091\n",
            "21666/22300 (epoch 48), train_loss = 0.740, time/batch = 0.095\n",
            "21667/22300 (epoch 48), train_loss = 0.762, time/batch = 0.095\n",
            "21668/22300 (epoch 48), train_loss = 0.713, time/batch = 0.091\n",
            "21669/22300 (epoch 48), train_loss = 0.767, time/batch = 0.095\n",
            "21670/22300 (epoch 48), train_loss = 0.700, time/batch = 0.092\n",
            "21671/22300 (epoch 48), train_loss = 0.745, time/batch = 0.098\n",
            "21672/22300 (epoch 48), train_loss = 0.715, time/batch = 0.093\n",
            "21673/22300 (epoch 48), train_loss = 0.740, time/batch = 0.095\n",
            "21674/22300 (epoch 48), train_loss = 0.718, time/batch = 0.093\n",
            "21675/22300 (epoch 48), train_loss = 0.775, time/batch = 0.094\n",
            "21676/22300 (epoch 48), train_loss = 0.754, time/batch = 0.093\n",
            "21677/22300 (epoch 48), train_loss = 0.749, time/batch = 0.094\n",
            "21678/22300 (epoch 48), train_loss = 0.775, time/batch = 0.092\n",
            "21679/22300 (epoch 48), train_loss = 0.761, time/batch = 0.091\n",
            "21680/22300 (epoch 48), train_loss = 0.778, time/batch = 0.092\n",
            "21681/22300 (epoch 48), train_loss = 0.809, time/batch = 0.098\n",
            "21682/22300 (epoch 48), train_loss = 0.793, time/batch = 0.092\n",
            "21683/22300 (epoch 48), train_loss = 0.781, time/batch = 0.093\n",
            "21684/22300 (epoch 48), train_loss = 0.800, time/batch = 0.093\n",
            "21685/22300 (epoch 48), train_loss = 0.763, time/batch = 0.092\n",
            "21686/22300 (epoch 48), train_loss = 0.738, time/batch = 0.094\n",
            "21687/22300 (epoch 48), train_loss = 0.747, time/batch = 0.091\n",
            "21688/22300 (epoch 48), train_loss = 0.742, time/batch = 0.092\n",
            "21689/22300 (epoch 48), train_loss = 0.777, time/batch = 0.091\n",
            "21690/22300 (epoch 48), train_loss = 0.748, time/batch = 0.092\n",
            "21691/22300 (epoch 48), train_loss = 0.729, time/batch = 0.092\n",
            "21692/22300 (epoch 48), train_loss = 0.731, time/batch = 0.092\n",
            "21693/22300 (epoch 48), train_loss = 0.726, time/batch = 0.092\n",
            "21694/22300 (epoch 48), train_loss = 0.755, time/batch = 0.091\n",
            "21695/22300 (epoch 48), train_loss = 0.751, time/batch = 0.092\n",
            "21696/22300 (epoch 48), train_loss = 0.745, time/batch = 0.092\n",
            "21697/22300 (epoch 48), train_loss = 0.758, time/batch = 0.094\n",
            "21698/22300 (epoch 48), train_loss = 0.725, time/batch = 0.091\n",
            "21699/22300 (epoch 48), train_loss = 0.750, time/batch = 0.097\n",
            "21700/22300 (epoch 48), train_loss = 0.773, time/batch = 0.093\n",
            "21701/22300 (epoch 48), train_loss = 0.747, time/batch = 0.092\n",
            "21702/22300 (epoch 48), train_loss = 0.756, time/batch = 0.092\n",
            "21703/22300 (epoch 48), train_loss = 0.741, time/batch = 0.092\n",
            "21704/22300 (epoch 48), train_loss = 0.712, time/batch = 0.092\n",
            "21705/22300 (epoch 48), train_loss = 0.729, time/batch = 0.092\n",
            "21706/22300 (epoch 48), train_loss = 0.716, time/batch = 0.093\n",
            "21707/22300 (epoch 48), train_loss = 0.757, time/batch = 0.094\n",
            "21708/22300 (epoch 48), train_loss = 0.705, time/batch = 0.101\n",
            "21709/22300 (epoch 48), train_loss = 0.737, time/batch = 0.092\n",
            "21710/22300 (epoch 48), train_loss = 0.686, time/batch = 0.093\n",
            "21711/22300 (epoch 48), train_loss = 0.706, time/batch = 0.094\n",
            "21712/22300 (epoch 48), train_loss = 0.714, time/batch = 0.092\n",
            "21713/22300 (epoch 48), train_loss = 0.718, time/batch = 0.092\n",
            "21714/22300 (epoch 48), train_loss = 0.712, time/batch = 0.092\n",
            "21715/22300 (epoch 48), train_loss = 0.749, time/batch = 0.093\n",
            "21716/22300 (epoch 48), train_loss = 0.748, time/batch = 0.092\n",
            "21717/22300 (epoch 48), train_loss = 0.734, time/batch = 0.093\n",
            "21718/22300 (epoch 48), train_loss = 0.710, time/batch = 0.093\n",
            "21719/22300 (epoch 48), train_loss = 0.711, time/batch = 0.094\n",
            "21720/22300 (epoch 48), train_loss = 0.727, time/batch = 0.092\n",
            "21721/22300 (epoch 48), train_loss = 0.716, time/batch = 0.092\n",
            "21722/22300 (epoch 48), train_loss = 0.709, time/batch = 0.093\n",
            "21723/22300 (epoch 48), train_loss = 0.698, time/batch = 0.092\n",
            "21724/22300 (epoch 48), train_loss = 0.688, time/batch = 0.092\n",
            "21725/22300 (epoch 48), train_loss = 0.712, time/batch = 0.092\n",
            "21726/22300 (epoch 48), train_loss = 0.702, time/batch = 0.091\n",
            "21727/22300 (epoch 48), train_loss = 0.693, time/batch = 0.092\n",
            "21728/22300 (epoch 48), train_loss = 0.703, time/batch = 0.093\n",
            "21729/22300 (epoch 48), train_loss = 0.722, time/batch = 0.094\n",
            "21730/22300 (epoch 48), train_loss = 0.732, time/batch = 0.093\n",
            "21731/22300 (epoch 48), train_loss = 0.690, time/batch = 0.097\n",
            "21732/22300 (epoch 48), train_loss = 0.695, time/batch = 0.091\n",
            "21733/22300 (epoch 48), train_loss = 0.728, time/batch = 0.092\n",
            "21734/22300 (epoch 48), train_loss = 0.738, time/batch = 0.091\n",
            "21735/22300 (epoch 48), train_loss = 0.726, time/batch = 0.093\n",
            "21736/22300 (epoch 48), train_loss = 0.751, time/batch = 0.092\n",
            "21737/22300 (epoch 48), train_loss = 0.711, time/batch = 0.093\n",
            "21738/22300 (epoch 48), train_loss = 0.692, time/batch = 0.111\n",
            "21739/22300 (epoch 48), train_loss = 0.709, time/batch = 0.094\n",
            "21740/22300 (epoch 48), train_loss = 0.705, time/batch = 0.093\n",
            "21741/22300 (epoch 48), train_loss = 0.669, time/batch = 0.096\n",
            "21742/22300 (epoch 48), train_loss = 0.699, time/batch = 0.091\n",
            "21743/22300 (epoch 48), train_loss = 0.707, time/batch = 0.091\n",
            "21744/22300 (epoch 48), train_loss = 0.726, time/batch = 0.091\n",
            "21745/22300 (epoch 48), train_loss = 0.691, time/batch = 0.092\n",
            "21746/22300 (epoch 48), train_loss = 0.704, time/batch = 0.092\n",
            "21747/22300 (epoch 48), train_loss = 0.719, time/batch = 0.094\n",
            "21748/22300 (epoch 48), train_loss = 0.739, time/batch = 0.093\n",
            "21749/22300 (epoch 48), train_loss = 0.702, time/batch = 0.092\n",
            "21750/22300 (epoch 48), train_loss = 0.691, time/batch = 0.093\n",
            "21751/22300 (epoch 48), train_loss = 0.726, time/batch = 0.093\n",
            "21752/22300 (epoch 48), train_loss = 0.718, time/batch = 0.092\n",
            "21753/22300 (epoch 48), train_loss = 0.719, time/batch = 0.092\n",
            "21754/22300 (epoch 48), train_loss = 0.696, time/batch = 0.091\n",
            "21755/22300 (epoch 48), train_loss = 0.745, time/batch = 0.091\n",
            "21756/22300 (epoch 48), train_loss = 0.677, time/batch = 0.092\n",
            "21757/22300 (epoch 48), train_loss = 0.708, time/batch = 0.092\n",
            "21758/22300 (epoch 48), train_loss = 0.697, time/batch = 0.093\n",
            "21759/22300 (epoch 48), train_loss = 0.707, time/batch = 0.093\n",
            "21760/22300 (epoch 48), train_loss = 0.690, time/batch = 0.093\n",
            "21761/22300 (epoch 48), train_loss = 0.700, time/batch = 0.092\n",
            "21762/22300 (epoch 48), train_loss = 0.701, time/batch = 0.091\n",
            "21763/22300 (epoch 48), train_loss = 0.671, time/batch = 0.092\n",
            "21764/22300 (epoch 48), train_loss = 0.718, time/batch = 0.092\n",
            "21765/22300 (epoch 48), train_loss = 0.711, time/batch = 0.092\n",
            "21766/22300 (epoch 48), train_loss = 0.699, time/batch = 0.092\n",
            "21767/22300 (epoch 48), train_loss = 0.714, time/batch = 0.091\n",
            "21768/22300 (epoch 48), train_loss = 0.711, time/batch = 0.094\n",
            "21769/22300 (epoch 48), train_loss = 0.713, time/batch = 0.092\n",
            "21770/22300 (epoch 48), train_loss = 0.719, time/batch = 0.094\n",
            "21771/22300 (epoch 48), train_loss = 0.708, time/batch = 0.094\n",
            "21772/22300 (epoch 48), train_loss = 0.727, time/batch = 0.091\n",
            "21773/22300 (epoch 48), train_loss = 0.698, time/batch = 0.093\n",
            "21774/22300 (epoch 48), train_loss = 0.697, time/batch = 0.092\n",
            "21775/22300 (epoch 48), train_loss = 0.663, time/batch = 0.092\n",
            "21776/22300 (epoch 48), train_loss = 0.668, time/batch = 0.092\n",
            "21777/22300 (epoch 48), train_loss = 0.685, time/batch = 0.092\n",
            "21778/22300 (epoch 48), train_loss = 0.710, time/batch = 0.093\n",
            "21779/22300 (epoch 48), train_loss = 0.667, time/batch = 0.096\n",
            "21780/22300 (epoch 48), train_loss = 0.721, time/batch = 0.091\n",
            "21781/22300 (epoch 48), train_loss = 0.685, time/batch = 0.093\n",
            "21782/22300 (epoch 48), train_loss = 0.716, time/batch = 0.093\n",
            "21783/22300 (epoch 48), train_loss = 0.693, time/batch = 0.093\n",
            "21784/22300 (epoch 48), train_loss = 0.687, time/batch = 0.092\n",
            "21785/22300 (epoch 48), train_loss = 0.704, time/batch = 0.092\n",
            "21786/22300 (epoch 48), train_loss = 0.694, time/batch = 0.092\n",
            "21787/22300 (epoch 48), train_loss = 0.653, time/batch = 0.092\n",
            "21788/22300 (epoch 48), train_loss = 0.678, time/batch = 0.097\n",
            "21789/22300 (epoch 48), train_loss = 0.696, time/batch = 0.093\n",
            "21790/22300 (epoch 48), train_loss = 0.675, time/batch = 0.093\n",
            "21791/22300 (epoch 48), train_loss = 0.662, time/batch = 0.093\n",
            "21792/22300 (epoch 48), train_loss = 0.693, time/batch = 0.092\n",
            "21793/22300 (epoch 48), train_loss = 0.689, time/batch = 0.093\n",
            "21794/22300 (epoch 48), train_loss = 0.684, time/batch = 0.092\n",
            "21795/22300 (epoch 48), train_loss = 0.696, time/batch = 0.092\n",
            "21796/22300 (epoch 48), train_loss = 0.658, time/batch = 0.092\n",
            "21797/22300 (epoch 48), train_loss = 0.666, time/batch = 0.093\n",
            "21798/22300 (epoch 48), train_loss = 0.659, time/batch = 0.092\n",
            "21799/22300 (epoch 48), train_loss = 0.662, time/batch = 0.092\n",
            "21800/22300 (epoch 48), train_loss = 0.684, time/batch = 0.093\n",
            "21801/22300 (epoch 48), train_loss = 0.707, time/batch = 0.092\n",
            "21802/22300 (epoch 48), train_loss = 0.677, time/batch = 0.093\n",
            "21803/22300 (epoch 48), train_loss = 0.706, time/batch = 0.091\n",
            "21804/22300 (epoch 48), train_loss = 0.692, time/batch = 0.092\n",
            "21805/22300 (epoch 48), train_loss = 0.692, time/batch = 0.092\n",
            "21806/22300 (epoch 48), train_loss = 0.690, time/batch = 0.096\n",
            "21807/22300 (epoch 48), train_loss = 0.666, time/batch = 0.093\n",
            "21808/22300 (epoch 48), train_loss = 0.695, time/batch = 0.091\n",
            "21809/22300 (epoch 48), train_loss = 0.713, time/batch = 0.092\n",
            "21810/22300 (epoch 48), train_loss = 0.710, time/batch = 0.091\n",
            "21811/22300 (epoch 48), train_loss = 0.699, time/batch = 0.093\n",
            "21812/22300 (epoch 48), train_loss = 0.705, time/batch = 0.102\n",
            "21813/22300 (epoch 48), train_loss = 0.703, time/batch = 0.090\n",
            "21814/22300 (epoch 48), train_loss = 0.705, time/batch = 0.093\n",
            "21815/22300 (epoch 48), train_loss = 0.720, time/batch = 0.093\n",
            "21816/22300 (epoch 48), train_loss = 0.700, time/batch = 0.099\n",
            "21817/22300 (epoch 48), train_loss = 0.735, time/batch = 0.092\n",
            "21818/22300 (epoch 48), train_loss = 0.724, time/batch = 0.092\n",
            "21819/22300 (epoch 48), train_loss = 0.680, time/batch = 0.091\n",
            "21820/22300 (epoch 48), train_loss = 0.699, time/batch = 0.091\n",
            "21821/22300 (epoch 48), train_loss = 0.675, time/batch = 0.092\n",
            "21822/22300 (epoch 48), train_loss = 0.714, time/batch = 0.095\n",
            "21823/22300 (epoch 48), train_loss = 0.729, time/batch = 0.093\n",
            "21824/22300 (epoch 48), train_loss = 0.721, time/batch = 0.091\n",
            "21825/22300 (epoch 48), train_loss = 0.724, time/batch = 0.094\n",
            "21826/22300 (epoch 48), train_loss = 0.721, time/batch = 0.093\n",
            "21827/22300 (epoch 48), train_loss = 0.714, time/batch = 0.093\n",
            "21828/22300 (epoch 48), train_loss = 0.717, time/batch = 0.092\n",
            "21829/22300 (epoch 48), train_loss = 0.706, time/batch = 0.096\n",
            "21830/22300 (epoch 48), train_loss = 0.705, time/batch = 0.092\n",
            "21831/22300 (epoch 48), train_loss = 0.702, time/batch = 0.093\n",
            "21832/22300 (epoch 48), train_loss = 0.712, time/batch = 0.092\n",
            "21833/22300 (epoch 48), train_loss = 0.704, time/batch = 0.094\n",
            "21834/22300 (epoch 48), train_loss = 0.748, time/batch = 0.092\n",
            "21835/22300 (epoch 48), train_loss = 0.680, time/batch = 0.092\n",
            "21836/22300 (epoch 48), train_loss = 0.710, time/batch = 0.091\n",
            "21837/22300 (epoch 48), train_loss = 0.687, time/batch = 0.092\n",
            "21838/22300 (epoch 48), train_loss = 0.684, time/batch = 0.092\n",
            "21839/22300 (epoch 48), train_loss = 0.742, time/batch = 0.093\n",
            "21840/22300 (epoch 48), train_loss = 0.709, time/batch = 0.094\n",
            "21841/22300 (epoch 48), train_loss = 0.741, time/batch = 0.098\n",
            "21842/22300 (epoch 48), train_loss = 0.706, time/batch = 0.092\n",
            "21843/22300 (epoch 48), train_loss = 0.703, time/batch = 0.094\n",
            "21844/22300 (epoch 48), train_loss = 0.682, time/batch = 0.092\n",
            "21845/22300 (epoch 48), train_loss = 0.701, time/batch = 0.092\n",
            "21846/22300 (epoch 48), train_loss = 0.768, time/batch = 0.093\n",
            "21847/22300 (epoch 48), train_loss = 0.701, time/batch = 0.092\n",
            "21848/22300 (epoch 48), train_loss = 0.712, time/batch = 0.093\n",
            "21849/22300 (epoch 48), train_loss = 0.690, time/batch = 0.092\n",
            "21850/22300 (epoch 48), train_loss = 0.700, time/batch = 0.093\n",
            "21851/22300 (epoch 48), train_loss = 0.701, time/batch = 0.092\n",
            "21852/22300 (epoch 48), train_loss = 0.702, time/batch = 0.092\n",
            "21853/22300 (epoch 48), train_loss = 0.695, time/batch = 0.093\n",
            "21854/22300 (epoch 49), train_loss = 0.442, time/batch = 0.086\n",
            "21855/22300 (epoch 49), train_loss = 0.771, time/batch = 0.093\n",
            "21856/22300 (epoch 49), train_loss = 0.798, time/batch = 0.091\n",
            "21857/22300 (epoch 49), train_loss = 0.781, time/batch = 0.093\n",
            "21858/22300 (epoch 49), train_loss = 0.787, time/batch = 0.091\n",
            "21859/22300 (epoch 49), train_loss = 0.730, time/batch = 0.092\n",
            "21860/22300 (epoch 49), train_loss = 0.758, time/batch = 0.092\n",
            "21861/22300 (epoch 49), train_loss = 0.759, time/batch = 0.092\n",
            "21862/22300 (epoch 49), train_loss = 0.728, time/batch = 0.092\n",
            "21863/22300 (epoch 49), train_loss = 0.725, time/batch = 0.098\n",
            "21864/22300 (epoch 49), train_loss = 0.774, time/batch = 0.093\n",
            "21865/22300 (epoch 49), train_loss = 0.717, time/batch = 0.092\n",
            "21866/22300 (epoch 49), train_loss = 0.742, time/batch = 0.092\n",
            "21867/22300 (epoch 49), train_loss = 0.745, time/batch = 0.094\n",
            "21868/22300 (epoch 49), train_loss = 0.752, time/batch = 0.091\n",
            "21869/22300 (epoch 49), train_loss = 0.770, time/batch = 0.092\n",
            "21870/22300 (epoch 49), train_loss = 0.772, time/batch = 0.091\n",
            "21871/22300 (epoch 49), train_loss = 0.733, time/batch = 0.095\n",
            "21872/22300 (epoch 49), train_loss = 0.739, time/batch = 0.091\n",
            "21873/22300 (epoch 49), train_loss = 0.765, time/batch = 0.094\n",
            "21874/22300 (epoch 49), train_loss = 0.732, time/batch = 0.095\n",
            "21875/22300 (epoch 49), train_loss = 0.717, time/batch = 0.093\n",
            "21876/22300 (epoch 49), train_loss = 0.728, time/batch = 0.092\n",
            "21877/22300 (epoch 49), train_loss = 0.715, time/batch = 0.093\n",
            "21878/22300 (epoch 49), train_loss = 0.692, time/batch = 0.091\n",
            "21879/22300 (epoch 49), train_loss = 0.727, time/batch = 0.094\n",
            "21880/22300 (epoch 49), train_loss = 0.740, time/batch = 0.091\n",
            "21881/22300 (epoch 49), train_loss = 0.702, time/batch = 0.093\n",
            "21882/22300 (epoch 49), train_loss = 0.744, time/batch = 0.093\n",
            "21883/22300 (epoch 49), train_loss = 0.714, time/batch = 0.092\n",
            "21884/22300 (epoch 49), train_loss = 0.703, time/batch = 0.094\n",
            "21885/22300 (epoch 49), train_loss = 0.733, time/batch = 0.094\n",
            "21886/22300 (epoch 49), train_loss = 0.712, time/batch = 0.092\n",
            "21887/22300 (epoch 49), train_loss = 0.729, time/batch = 0.093\n",
            "21888/22300 (epoch 49), train_loss = 0.726, time/batch = 0.108\n",
            "21889/22300 (epoch 49), train_loss = 0.722, time/batch = 0.093\n",
            "21890/22300 (epoch 49), train_loss = 0.736, time/batch = 0.091\n",
            "21891/22300 (epoch 49), train_loss = 0.718, time/batch = 0.093\n",
            "21892/22300 (epoch 49), train_loss = 0.744, time/batch = 0.091\n",
            "21893/22300 (epoch 49), train_loss = 0.697, time/batch = 0.092\n",
            "21894/22300 (epoch 49), train_loss = 0.705, time/batch = 0.092\n",
            "21895/22300 (epoch 49), train_loss = 0.722, time/batch = 0.095\n",
            "21896/22300 (epoch 49), train_loss = 0.722, time/batch = 0.093\n",
            "21897/22300 (epoch 49), train_loss = 0.716, time/batch = 0.095\n",
            "21898/22300 (epoch 49), train_loss = 0.685, time/batch = 0.093\n",
            "21899/22300 (epoch 49), train_loss = 0.746, time/batch = 0.092\n",
            "21900/22300 (epoch 49), train_loss = 0.701, time/batch = 0.092\n",
            "21901/22300 (epoch 49), train_loss = 0.680, time/batch = 0.093\n",
            "21902/22300 (epoch 49), train_loss = 0.716, time/batch = 0.091\n",
            "21903/22300 (epoch 49), train_loss = 0.721, time/batch = 0.097\n",
            "21904/22300 (epoch 49), train_loss = 0.737, time/batch = 0.092\n",
            "21905/22300 (epoch 49), train_loss = 0.678, time/batch = 0.092\n",
            "21906/22300 (epoch 49), train_loss = 0.698, time/batch = 0.093\n",
            "21907/22300 (epoch 49), train_loss = 0.717, time/batch = 0.092\n",
            "21908/22300 (epoch 49), train_loss = 0.691, time/batch = 0.093\n",
            "21909/22300 (epoch 49), train_loss = 0.699, time/batch = 0.092\n",
            "21910/22300 (epoch 49), train_loss = 0.696, time/batch = 0.093\n",
            "21911/22300 (epoch 49), train_loss = 0.729, time/batch = 0.094\n",
            "21912/22300 (epoch 49), train_loss = 0.703, time/batch = 0.091\n",
            "21913/22300 (epoch 49), train_loss = 0.683, time/batch = 0.094\n",
            "21914/22300 (epoch 49), train_loss = 0.727, time/batch = 0.092\n",
            "21915/22300 (epoch 49), train_loss = 0.700, time/batch = 0.092\n",
            "21916/22300 (epoch 49), train_loss = 0.694, time/batch = 0.092\n",
            "21917/22300 (epoch 49), train_loss = 0.736, time/batch = 0.093\n",
            "21918/22300 (epoch 49), train_loss = 0.718, time/batch = 0.096\n",
            "21919/22300 (epoch 49), train_loss = 0.725, time/batch = 0.092\n",
            "21920/22300 (epoch 49), train_loss = 0.714, time/batch = 0.092\n",
            "21921/22300 (epoch 49), train_loss = 0.768, time/batch = 0.093\n",
            "21922/22300 (epoch 49), train_loss = 0.729, time/batch = 0.093\n",
            "21923/22300 (epoch 49), train_loss = 0.732, time/batch = 0.092\n",
            "21924/22300 (epoch 49), train_loss = 0.715, time/batch = 0.092\n",
            "21925/22300 (epoch 49), train_loss = 0.675, time/batch = 0.092\n",
            "21926/22300 (epoch 49), train_loss = 0.703, time/batch = 0.092\n",
            "21927/22300 (epoch 49), train_loss = 0.685, time/batch = 0.092\n",
            "21928/22300 (epoch 49), train_loss = 0.685, time/batch = 0.094\n",
            "21929/22300 (epoch 49), train_loss = 0.729, time/batch = 0.096\n",
            "21930/22300 (epoch 49), train_loss = 0.721, time/batch = 0.092\n",
            "21931/22300 (epoch 49), train_loss = 0.719, time/batch = 0.092\n",
            "21932/22300 (epoch 49), train_loss = 0.692, time/batch = 0.092\n",
            "21933/22300 (epoch 49), train_loss = 0.747, time/batch = 0.093\n",
            "21934/22300 (epoch 49), train_loss = 0.720, time/batch = 0.092\n",
            "21935/22300 (epoch 49), train_loss = 0.688, time/batch = 0.093\n",
            "21936/22300 (epoch 49), train_loss = 0.707, time/batch = 0.091\n",
            "21937/22300 (epoch 49), train_loss = 0.687, time/batch = 0.092\n",
            "21938/22300 (epoch 49), train_loss = 0.709, time/batch = 0.094\n",
            "21939/22300 (epoch 49), train_loss = 0.705, time/batch = 0.092\n",
            "21940/22300 (epoch 49), train_loss = 0.691, time/batch = 0.093\n",
            "21941/22300 (epoch 49), train_loss = 0.723, time/batch = 0.095\n",
            "21942/22300 (epoch 49), train_loss = 0.701, time/batch = 0.092\n",
            "21943/22300 (epoch 49), train_loss = 0.737, time/batch = 0.092\n",
            "21944/22300 (epoch 49), train_loss = 0.686, time/batch = 0.092\n",
            "21945/22300 (epoch 49), train_loss = 0.717, time/batch = 0.093\n",
            "21946/22300 (epoch 49), train_loss = 0.730, time/batch = 0.095\n",
            "21947/22300 (epoch 49), train_loss = 0.707, time/batch = 0.091\n",
            "21948/22300 (epoch 49), train_loss = 0.705, time/batch = 0.093\n",
            "21949/22300 (epoch 49), train_loss = 0.726, time/batch = 0.092\n",
            "21950/22300 (epoch 49), train_loss = 0.682, time/batch = 0.094\n",
            "21951/22300 (epoch 49), train_loss = 0.731, time/batch = 0.092\n",
            "21952/22300 (epoch 49), train_loss = 0.682, time/batch = 0.092\n",
            "21953/22300 (epoch 49), train_loss = 0.688, time/batch = 0.095\n",
            "21954/22300 (epoch 49), train_loss = 0.688, time/batch = 0.098\n",
            "21955/22300 (epoch 49), train_loss = 0.695, time/batch = 0.092\n",
            "21956/22300 (epoch 49), train_loss = 0.708, time/batch = 0.093\n",
            "21957/22300 (epoch 49), train_loss = 0.681, time/batch = 0.092\n",
            "21958/22300 (epoch 49), train_loss = 0.705, time/batch = 0.093\n",
            "21959/22300 (epoch 49), train_loss = 0.672, time/batch = 0.092\n",
            "21960/22300 (epoch 49), train_loss = 0.681, time/batch = 0.093\n",
            "21961/22300 (epoch 49), train_loss = 0.682, time/batch = 0.091\n",
            "21962/22300 (epoch 49), train_loss = 0.693, time/batch = 0.092\n",
            "21963/22300 (epoch 49), train_loss = 0.684, time/batch = 0.093\n",
            "21964/22300 (epoch 49), train_loss = 0.670, time/batch = 0.092\n",
            "21965/22300 (epoch 49), train_loss = 0.671, time/batch = 0.093\n",
            "21966/22300 (epoch 49), train_loss = 0.691, time/batch = 0.094\n",
            "21967/22300 (epoch 49), train_loss = 0.667, time/batch = 0.093\n",
            "21968/22300 (epoch 49), train_loss = 0.706, time/batch = 0.094\n",
            "21969/22300 (epoch 49), train_loss = 0.674, time/batch = 0.092\n",
            "21970/22300 (epoch 49), train_loss = 0.666, time/batch = 0.093\n",
            "21971/22300 (epoch 49), train_loss = 0.708, time/batch = 0.092\n",
            "21972/22300 (epoch 49), train_loss = 0.705, time/batch = 0.092\n",
            "21973/22300 (epoch 49), train_loss = 0.685, time/batch = 0.092\n",
            "21974/22300 (epoch 49), train_loss = 0.735, time/batch = 0.092\n",
            "21975/22300 (epoch 49), train_loss = 0.713, time/batch = 0.093\n",
            "21976/22300 (epoch 49), train_loss = 0.717, time/batch = 0.092\n",
            "21977/22300 (epoch 49), train_loss = 0.712, time/batch = 0.092\n",
            "21978/22300 (epoch 49), train_loss = 0.742, time/batch = 0.092\n",
            "21979/22300 (epoch 49), train_loss = 0.732, time/batch = 0.092\n",
            "21980/22300 (epoch 49), train_loss = 0.716, time/batch = 0.092\n",
            "21981/22300 (epoch 49), train_loss = 0.730, time/batch = 0.094\n",
            "21982/22300 (epoch 49), train_loss = 0.722, time/batch = 0.092\n",
            "21983/22300 (epoch 49), train_loss = 0.730, time/batch = 0.092\n",
            "21984/22300 (epoch 49), train_loss = 0.701, time/batch = 0.105\n",
            "21985/22300 (epoch 49), train_loss = 0.711, time/batch = 0.092\n",
            "21986/22300 (epoch 49), train_loss = 0.735, time/batch = 0.094\n",
            "21987/22300 (epoch 49), train_loss = 0.755, time/batch = 0.092\n",
            "21988/22300 (epoch 49), train_loss = 0.696, time/batch = 0.093\n",
            "21989/22300 (epoch 49), train_loss = 0.739, time/batch = 0.092\n",
            "21990/22300 (epoch 49), train_loss = 0.748, time/batch = 0.092\n",
            "21991/22300 (epoch 49), train_loss = 0.753, time/batch = 0.092\n",
            "21992/22300 (epoch 49), train_loss = 0.742, time/batch = 0.093\n",
            "21993/22300 (epoch 49), train_loss = 0.753, time/batch = 0.093\n",
            "21994/22300 (epoch 49), train_loss = 0.725, time/batch = 0.092\n",
            "21995/22300 (epoch 49), train_loss = 0.749, time/batch = 0.094\n",
            "21996/22300 (epoch 49), train_loss = 0.707, time/batch = 0.092\n",
            "21997/22300 (epoch 49), train_loss = 0.716, time/batch = 0.094\n",
            "21998/22300 (epoch 49), train_loss = 0.720, time/batch = 0.097\n",
            "21999/22300 (epoch 49), train_loss = 0.763, time/batch = 0.091\n",
            "22000/22300 (epoch 49), train_loss = 0.730, time/batch = 0.092\n",
            "model saved to save/model.ckpt\n",
            "22001/22300 (epoch 49), train_loss = 0.723, time/batch = 0.085\n",
            "22002/22300 (epoch 49), train_loss = 0.747, time/batch = 0.092\n",
            "22003/22300 (epoch 49), train_loss = 0.735, time/batch = 0.092\n",
            "22004/22300 (epoch 49), train_loss = 0.742, time/batch = 0.091\n",
            "22005/22300 (epoch 49), train_loss = 0.757, time/batch = 0.092\n",
            "22006/22300 (epoch 49), train_loss = 0.745, time/batch = 0.091\n",
            "22007/22300 (epoch 49), train_loss = 0.713, time/batch = 0.092\n",
            "22008/22300 (epoch 49), train_loss = 0.731, time/batch = 0.092\n",
            "22009/22300 (epoch 49), train_loss = 0.730, time/batch = 0.092\n",
            "22010/22300 (epoch 49), train_loss = 0.734, time/batch = 0.100\n",
            "22011/22300 (epoch 49), train_loss = 0.713, time/batch = 0.099\n",
            "22012/22300 (epoch 49), train_loss = 0.746, time/batch = 0.094\n",
            "22013/22300 (epoch 49), train_loss = 0.718, time/batch = 0.097\n",
            "22014/22300 (epoch 49), train_loss = 0.693, time/batch = 0.092\n",
            "22015/22300 (epoch 49), train_loss = 0.723, time/batch = 0.092\n",
            "22016/22300 (epoch 49), train_loss = 0.725, time/batch = 0.092\n",
            "22017/22300 (epoch 49), train_loss = 0.716, time/batch = 0.092\n",
            "22018/22300 (epoch 49), train_loss = 0.712, time/batch = 0.091\n",
            "22019/22300 (epoch 49), train_loss = 0.702, time/batch = 0.092\n",
            "22020/22300 (epoch 49), train_loss = 0.683, time/batch = 0.094\n",
            "22021/22300 (epoch 49), train_loss = 0.697, time/batch = 0.095\n",
            "22022/22300 (epoch 49), train_loss = 0.706, time/batch = 0.093\n",
            "22023/22300 (epoch 49), train_loss = 0.728, time/batch = 0.095\n",
            "22024/22300 (epoch 49), train_loss = 0.712, time/batch = 0.097\n",
            "22025/22300 (epoch 49), train_loss = 0.709, time/batch = 0.092\n",
            "22026/22300 (epoch 49), train_loss = 0.698, time/batch = 0.091\n",
            "22027/22300 (epoch 49), train_loss = 0.696, time/batch = 0.092\n",
            "22028/22300 (epoch 49), train_loss = 0.691, time/batch = 0.091\n",
            "22029/22300 (epoch 49), train_loss = 0.666, time/batch = 0.091\n",
            "22030/22300 (epoch 49), train_loss = 0.650, time/batch = 0.092\n",
            "22031/22300 (epoch 49), train_loss = 0.677, time/batch = 0.093\n",
            "22032/22300 (epoch 49), train_loss = 0.628, time/batch = 0.093\n",
            "22033/22300 (epoch 49), train_loss = 0.700, time/batch = 0.094\n",
            "22034/22300 (epoch 49), train_loss = 0.670, time/batch = 0.094\n",
            "22035/22300 (epoch 49), train_loss = 0.663, time/batch = 0.093\n",
            "22036/22300 (epoch 49), train_loss = 0.703, time/batch = 0.091\n",
            "22037/22300 (epoch 49), train_loss = 0.686, time/batch = 0.092\n",
            "22038/22300 (epoch 49), train_loss = 0.685, time/batch = 0.092\n",
            "22039/22300 (epoch 49), train_loss = 0.650, time/batch = 0.091\n",
            "22040/22300 (epoch 49), train_loss = 0.707, time/batch = 0.092\n",
            "22041/22300 (epoch 49), train_loss = 0.674, time/batch = 0.093\n",
            "22042/22300 (epoch 49), train_loss = 0.702, time/batch = 0.092\n",
            "22043/22300 (epoch 49), train_loss = 0.681, time/batch = 0.093\n",
            "22044/22300 (epoch 49), train_loss = 0.700, time/batch = 0.094\n",
            "22045/22300 (epoch 49), train_loss = 0.699, time/batch = 0.092\n",
            "22046/22300 (epoch 49), train_loss = 0.695, time/batch = 0.092\n",
            "22047/22300 (epoch 49), train_loss = 0.716, time/batch = 0.092\n",
            "22048/22300 (epoch 49), train_loss = 0.701, time/batch = 0.091\n",
            "22049/22300 (epoch 49), train_loss = 0.684, time/batch = 0.092\n",
            "22050/22300 (epoch 49), train_loss = 0.703, time/batch = 0.091\n",
            "22051/22300 (epoch 49), train_loss = 0.680, time/batch = 0.092\n",
            "22052/22300 (epoch 49), train_loss = 0.725, time/batch = 0.093\n",
            "22053/22300 (epoch 49), train_loss = 0.702, time/batch = 0.093\n",
            "22054/22300 (epoch 49), train_loss = 0.734, time/batch = 0.092\n",
            "22055/22300 (epoch 49), train_loss = 0.687, time/batch = 0.093\n",
            "22056/22300 (epoch 49), train_loss = 0.670, time/batch = 0.092\n",
            "22057/22300 (epoch 49), train_loss = 0.668, time/batch = 0.092\n",
            "22058/22300 (epoch 49), train_loss = 0.686, time/batch = 0.091\n",
            "22059/22300 (epoch 49), train_loss = 0.676, time/batch = 0.092\n",
            "22060/22300 (epoch 49), train_loss = 0.731, time/batch = 0.092\n",
            "22061/22300 (epoch 49), train_loss = 0.703, time/batch = 0.092\n",
            "22062/22300 (epoch 49), train_loss = 0.741, time/batch = 0.093\n",
            "22063/22300 (epoch 49), train_loss = 0.674, time/batch = 0.092\n",
            "22064/22300 (epoch 49), train_loss = 0.695, time/batch = 0.096\n",
            "22065/22300 (epoch 49), train_loss = 0.681, time/batch = 0.093\n",
            "22066/22300 (epoch 49), train_loss = 0.699, time/batch = 0.093\n",
            "22067/22300 (epoch 49), train_loss = 0.682, time/batch = 0.096\n",
            "22068/22300 (epoch 49), train_loss = 0.662, time/batch = 0.097\n",
            "22069/22300 (epoch 49), train_loss = 0.669, time/batch = 0.092\n",
            "22070/22300 (epoch 49), train_loss = 0.676, time/batch = 0.092\n",
            "22071/22300 (epoch 49), train_loss = 0.714, time/batch = 0.091\n",
            "22072/22300 (epoch 49), train_loss = 0.690, time/batch = 0.092\n",
            "22073/22300 (epoch 49), train_loss = 0.703, time/batch = 0.092\n",
            "22074/22300 (epoch 49), train_loss = 0.744, time/batch = 0.092\n",
            "22075/22300 (epoch 49), train_loss = 0.700, time/batch = 0.096\n",
            "22076/22300 (epoch 49), train_loss = 0.712, time/batch = 0.097\n",
            "22077/22300 (epoch 49), train_loss = 0.747, time/batch = 0.092\n",
            "22078/22300 (epoch 49), train_loss = 0.678, time/batch = 0.093\n",
            "22079/22300 (epoch 49), train_loss = 0.717, time/batch = 0.092\n",
            "22080/22300 (epoch 49), train_loss = 0.741, time/batch = 0.092\n",
            "22081/22300 (epoch 49), train_loss = 0.710, time/batch = 0.092\n",
            "22082/22300 (epoch 49), train_loss = 0.726, time/batch = 0.126\n",
            "22083/22300 (epoch 49), train_loss = 0.733, time/batch = 0.087\n",
            "22084/22300 (epoch 49), train_loss = 0.711, time/batch = 0.093\n",
            "22085/22300 (epoch 49), train_loss = 0.721, time/batch = 0.091\n",
            "22086/22300 (epoch 49), train_loss = 0.717, time/batch = 0.091\n",
            "22087/22300 (epoch 49), train_loss = 0.724, time/batch = 0.091\n",
            "22088/22300 (epoch 49), train_loss = 0.759, time/batch = 0.092\n",
            "22089/22300 (epoch 49), train_loss = 0.728, time/batch = 0.091\n",
            "22090/22300 (epoch 49), train_loss = 0.724, time/batch = 0.092\n",
            "22091/22300 (epoch 49), train_loss = 0.697, time/batch = 0.092\n",
            "22092/22300 (epoch 49), train_loss = 0.724, time/batch = 0.095\n",
            "22093/22300 (epoch 49), train_loss = 0.714, time/batch = 0.096\n",
            "22094/22300 (epoch 49), train_loss = 0.729, time/batch = 0.092\n",
            "22095/22300 (epoch 49), train_loss = 0.690, time/batch = 0.092\n",
            "22096/22300 (epoch 49), train_loss = 0.724, time/batch = 0.092\n",
            "22097/22300 (epoch 49), train_loss = 0.740, time/batch = 0.092\n",
            "22098/22300 (epoch 49), train_loss = 0.703, time/batch = 0.094\n",
            "22099/22300 (epoch 49), train_loss = 0.728, time/batch = 0.092\n",
            "22100/22300 (epoch 49), train_loss = 0.745, time/batch = 0.093\n",
            "22101/22300 (epoch 49), train_loss = 0.685, time/batch = 0.091\n",
            "22102/22300 (epoch 49), train_loss = 0.750, time/batch = 0.092\n",
            "22103/22300 (epoch 49), train_loss = 0.728, time/batch = 0.094\n",
            "22104/22300 (epoch 49), train_loss = 0.736, time/batch = 0.092\n",
            "22105/22300 (epoch 49), train_loss = 0.735, time/batch = 0.093\n",
            "22106/22300 (epoch 49), train_loss = 0.746, time/batch = 0.092\n",
            "22107/22300 (epoch 49), train_loss = 0.765, time/batch = 0.092\n",
            "22108/22300 (epoch 49), train_loss = 0.740, time/batch = 0.092\n",
            "22109/22300 (epoch 49), train_loss = 0.741, time/batch = 0.092\n",
            "22110/22300 (epoch 49), train_loss = 0.709, time/batch = 0.094\n",
            "22111/22300 (epoch 49), train_loss = 0.733, time/batch = 0.099\n",
            "22112/22300 (epoch 49), train_loss = 0.729, time/batch = 0.091\n",
            "22113/22300 (epoch 49), train_loss = 0.759, time/batch = 0.093\n",
            "22114/22300 (epoch 49), train_loss = 0.714, time/batch = 0.091\n",
            "22115/22300 (epoch 49), train_loss = 0.775, time/batch = 0.094\n",
            "22116/22300 (epoch 49), train_loss = 0.711, time/batch = 0.091\n",
            "22117/22300 (epoch 49), train_loss = 0.753, time/batch = 0.092\n",
            "22118/22300 (epoch 49), train_loss = 0.720, time/batch = 0.091\n",
            "22119/22300 (epoch 49), train_loss = 0.742, time/batch = 0.092\n",
            "22120/22300 (epoch 49), train_loss = 0.717, time/batch = 0.092\n",
            "22121/22300 (epoch 49), train_loss = 0.752, time/batch = 0.093\n",
            "22122/22300 (epoch 49), train_loss = 0.723, time/batch = 0.092\n",
            "22123/22300 (epoch 49), train_loss = 0.710, time/batch = 0.092\n",
            "22124/22300 (epoch 49), train_loss = 0.750, time/batch = 0.092\n",
            "22125/22300 (epoch 49), train_loss = 0.731, time/batch = 0.092\n",
            "22126/22300 (epoch 49), train_loss = 0.761, time/batch = 0.092\n",
            "22127/22300 (epoch 49), train_loss = 0.797, time/batch = 0.092\n",
            "22128/22300 (epoch 49), train_loss = 0.798, time/batch = 0.092\n",
            "22129/22300 (epoch 49), train_loss = 0.797, time/batch = 0.100\n",
            "22130/22300 (epoch 49), train_loss = 0.820, time/batch = 0.091\n",
            "22131/22300 (epoch 49), train_loss = 0.788, time/batch = 0.093\n",
            "22132/22300 (epoch 49), train_loss = 0.755, time/batch = 0.093\n",
            "22133/22300 (epoch 49), train_loss = 0.779, time/batch = 0.092\n",
            "22134/22300 (epoch 49), train_loss = 0.760, time/batch = 0.092\n",
            "22135/22300 (epoch 49), train_loss = 0.796, time/batch = 0.092\n",
            "22136/22300 (epoch 49), train_loss = 0.758, time/batch = 0.093\n",
            "22137/22300 (epoch 49), train_loss = 0.728, time/batch = 0.092\n",
            "22138/22300 (epoch 49), train_loss = 0.722, time/batch = 0.091\n",
            "22139/22300 (epoch 49), train_loss = 0.714, time/batch = 0.094\n",
            "22140/22300 (epoch 49), train_loss = 0.754, time/batch = 0.091\n",
            "22141/22300 (epoch 49), train_loss = 0.742, time/batch = 0.092\n",
            "22142/22300 (epoch 49), train_loss = 0.725, time/batch = 0.092\n",
            "22143/22300 (epoch 49), train_loss = 0.740, time/batch = 0.092\n",
            "22144/22300 (epoch 49), train_loss = 0.710, time/batch = 0.093\n",
            "22145/22300 (epoch 49), train_loss = 0.731, time/batch = 0.092\n",
            "22146/22300 (epoch 49), train_loss = 0.759, time/batch = 0.092\n",
            "22147/22300 (epoch 49), train_loss = 0.743, time/batch = 0.092\n",
            "22148/22300 (epoch 49), train_loss = 0.765, time/batch = 0.096\n",
            "22149/22300 (epoch 49), train_loss = 0.751, time/batch = 0.093\n",
            "22150/22300 (epoch 49), train_loss = 0.728, time/batch = 0.092\n",
            "22151/22300 (epoch 49), train_loss = 0.752, time/batch = 0.098\n",
            "22152/22300 (epoch 49), train_loss = 0.724, time/batch = 0.094\n",
            "22153/22300 (epoch 49), train_loss = 0.752, time/batch = 0.100\n",
            "22154/22300 (epoch 49), train_loss = 0.705, time/batch = 0.091\n",
            "22155/22300 (epoch 49), train_loss = 0.728, time/batch = 0.093\n",
            "22156/22300 (epoch 49), train_loss = 0.691, time/batch = 0.092\n",
            "22157/22300 (epoch 49), train_loss = 0.703, time/batch = 0.092\n",
            "22158/22300 (epoch 49), train_loss = 0.707, time/batch = 0.091\n",
            "22159/22300 (epoch 49), train_loss = 0.702, time/batch = 0.092\n",
            "22160/22300 (epoch 49), train_loss = 0.685, time/batch = 0.092\n",
            "22161/22300 (epoch 49), train_loss = 0.731, time/batch = 0.092\n",
            "22162/22300 (epoch 49), train_loss = 0.728, time/batch = 0.094\n",
            "22163/22300 (epoch 49), train_loss = 0.717, time/batch = 0.094\n",
            "22164/22300 (epoch 49), train_loss = 0.699, time/batch = 0.091\n",
            "22165/22300 (epoch 49), train_loss = 0.702, time/batch = 0.092\n",
            "22166/22300 (epoch 49), train_loss = 0.727, time/batch = 0.092\n",
            "22167/22300 (epoch 49), train_loss = 0.724, time/batch = 0.093\n",
            "22168/22300 (epoch 49), train_loss = 0.719, time/batch = 0.091\n",
            "22169/22300 (epoch 49), train_loss = 0.702, time/batch = 0.091\n",
            "22170/22300 (epoch 49), train_loss = 0.698, time/batch = 0.092\n",
            "22171/22300 (epoch 49), train_loss = 0.710, time/batch = 0.092\n",
            "22172/22300 (epoch 49), train_loss = 0.691, time/batch = 0.093\n",
            "22173/22300 (epoch 49), train_loss = 0.674, time/batch = 0.093\n",
            "22174/22300 (epoch 49), train_loss = 0.681, time/batch = 0.093\n",
            "22175/22300 (epoch 49), train_loss = 0.713, time/batch = 0.093\n",
            "22176/22300 (epoch 49), train_loss = 0.708, time/batch = 0.093\n",
            "22177/22300 (epoch 49), train_loss = 0.673, time/batch = 0.099\n",
            "22178/22300 (epoch 49), train_loss = 0.684, time/batch = 0.090\n",
            "22179/22300 (epoch 49), train_loss = 0.715, time/batch = 0.092\n",
            "22180/22300 (epoch 49), train_loss = 0.727, time/batch = 0.091\n",
            "22181/22300 (epoch 49), train_loss = 0.719, time/batch = 0.092\n",
            "22182/22300 (epoch 49), train_loss = 0.741, time/batch = 0.092\n",
            "22183/22300 (epoch 49), train_loss = 0.706, time/batch = 0.092\n",
            "22184/22300 (epoch 49), train_loss = 0.688, time/batch = 0.094\n",
            "22185/22300 (epoch 49), train_loss = 0.705, time/batch = 0.091\n",
            "22186/22300 (epoch 49), train_loss = 0.688, time/batch = 0.092\n",
            "22187/22300 (epoch 49), train_loss = 0.666, time/batch = 0.094\n",
            "22188/22300 (epoch 49), train_loss = 0.698, time/batch = 0.092\n",
            "22189/22300 (epoch 49), train_loss = 0.701, time/batch = 0.093\n",
            "22190/22300 (epoch 49), train_loss = 0.714, time/batch = 0.096\n",
            "22191/22300 (epoch 49), train_loss = 0.681, time/batch = 0.092\n",
            "22192/22300 (epoch 49), train_loss = 0.692, time/batch = 0.092\n",
            "22193/22300 (epoch 49), train_loss = 0.700, time/batch = 0.092\n",
            "22194/22300 (epoch 49), train_loss = 0.718, time/batch = 0.094\n",
            "22195/22300 (epoch 49), train_loss = 0.684, time/batch = 0.092\n",
            "22196/22300 (epoch 49), train_loss = 0.684, time/batch = 0.092\n",
            "22197/22300 (epoch 49), train_loss = 0.708, time/batch = 0.091\n",
            "22198/22300 (epoch 49), train_loss = 0.707, time/batch = 0.091\n",
            "22199/22300 (epoch 49), train_loss = 0.708, time/batch = 0.099\n",
            "22200/22300 (epoch 49), train_loss = 0.683, time/batch = 0.094\n",
            "22201/22300 (epoch 49), train_loss = 0.736, time/batch = 0.092\n",
            "22202/22300 (epoch 49), train_loss = 0.673, time/batch = 0.091\n",
            "22203/22300 (epoch 49), train_loss = 0.700, time/batch = 0.094\n",
            "22204/22300 (epoch 49), train_loss = 0.684, time/batch = 0.094\n",
            "22205/22300 (epoch 49), train_loss = 0.698, time/batch = 0.092\n",
            "22206/22300 (epoch 49), train_loss = 0.672, time/batch = 0.094\n",
            "22207/22300 (epoch 49), train_loss = 0.695, time/batch = 0.092\n",
            "22208/22300 (epoch 49), train_loss = 0.695, time/batch = 0.092\n",
            "22209/22300 (epoch 49), train_loss = 0.660, time/batch = 0.094\n",
            "22210/22300 (epoch 49), train_loss = 0.706, time/batch = 0.093\n",
            "22211/22300 (epoch 49), train_loss = 0.700, time/batch = 0.092\n",
            "22212/22300 (epoch 49), train_loss = 0.682, time/batch = 0.092\n",
            "22213/22300 (epoch 49), train_loss = 0.693, time/batch = 0.092\n",
            "22214/22300 (epoch 49), train_loss = 0.687, time/batch = 0.093\n",
            "22215/22300 (epoch 49), train_loss = 0.692, time/batch = 0.103\n",
            "22216/22300 (epoch 49), train_loss = 0.702, time/batch = 0.093\n",
            "22217/22300 (epoch 49), train_loss = 0.689, time/batch = 0.093\n",
            "22218/22300 (epoch 49), train_loss = 0.722, time/batch = 0.095\n",
            "22219/22300 (epoch 49), train_loss = 0.688, time/batch = 0.093\n",
            "22220/22300 (epoch 49), train_loss = 0.683, time/batch = 0.094\n",
            "22221/22300 (epoch 49), train_loss = 0.650, time/batch = 0.093\n",
            "22222/22300 (epoch 49), train_loss = 0.658, time/batch = 0.092\n",
            "22223/22300 (epoch 49), train_loss = 0.669, time/batch = 0.097\n",
            "22224/22300 (epoch 49), train_loss = 0.690, time/batch = 0.092\n",
            "22225/22300 (epoch 49), train_loss = 0.642, time/batch = 0.093\n",
            "22226/22300 (epoch 49), train_loss = 0.700, time/batch = 0.091\n",
            "22227/22300 (epoch 49), train_loss = 0.676, time/batch = 0.098\n",
            "22228/22300 (epoch 49), train_loss = 0.707, time/batch = 0.092\n",
            "22229/22300 (epoch 49), train_loss = 0.686, time/batch = 0.096\n",
            "22230/22300 (epoch 49), train_loss = 0.684, time/batch = 0.095\n",
            "22231/22300 (epoch 49), train_loss = 0.690, time/batch = 0.092\n",
            "22232/22300 (epoch 49), train_loss = 0.677, time/batch = 0.091\n",
            "22233/22300 (epoch 49), train_loss = 0.643, time/batch = 0.092\n",
            "22234/22300 (epoch 49), train_loss = 0.659, time/batch = 0.092\n",
            "22235/22300 (epoch 49), train_loss = 0.683, time/batch = 0.093\n",
            "22236/22300 (epoch 49), train_loss = 0.653, time/batch = 0.094\n",
            "22237/22300 (epoch 49), train_loss = 0.639, time/batch = 0.092\n",
            "22238/22300 (epoch 49), train_loss = 0.669, time/batch = 0.092\n",
            "22239/22300 (epoch 49), train_loss = 0.667, time/batch = 0.094\n",
            "22240/22300 (epoch 49), train_loss = 0.670, time/batch = 0.093\n",
            "22241/22300 (epoch 49), train_loss = 0.688, time/batch = 0.092\n",
            "22242/22300 (epoch 49), train_loss = 0.648, time/batch = 0.095\n",
            "22243/22300 (epoch 49), train_loss = 0.658, time/batch = 0.095\n",
            "22244/22300 (epoch 49), train_loss = 0.650, time/batch = 0.092\n",
            "22245/22300 (epoch 49), train_loss = 0.647, time/batch = 0.091\n",
            "22246/22300 (epoch 49), train_loss = 0.671, time/batch = 0.092\n",
            "22247/22300 (epoch 49), train_loss = 0.690, time/batch = 0.091\n",
            "22248/22300 (epoch 49), train_loss = 0.661, time/batch = 0.092\n",
            "22249/22300 (epoch 49), train_loss = 0.683, time/batch = 0.093\n",
            "22250/22300 (epoch 49), train_loss = 0.676, time/batch = 0.093\n",
            "22251/22300 (epoch 49), train_loss = 0.671, time/batch = 0.093\n",
            "22252/22300 (epoch 49), train_loss = 0.679, time/batch = 0.092\n",
            "22253/22300 (epoch 49), train_loss = 0.656, time/batch = 0.092\n",
            "22254/22300 (epoch 49), train_loss = 0.677, time/batch = 0.092\n",
            "22255/22300 (epoch 49), train_loss = 0.697, time/batch = 0.092\n",
            "22256/22300 (epoch 49), train_loss = 0.694, time/batch = 0.093\n",
            "22257/22300 (epoch 49), train_loss = 0.683, time/batch = 0.092\n",
            "22258/22300 (epoch 49), train_loss = 0.687, time/batch = 0.093\n",
            "22259/22300 (epoch 49), train_loss = 0.685, time/batch = 0.093\n",
            "22260/22300 (epoch 49), train_loss = 0.691, time/batch = 0.093\n",
            "22261/22300 (epoch 49), train_loss = 0.705, time/batch = 0.091\n",
            "22262/22300 (epoch 49), train_loss = 0.685, time/batch = 0.093\n",
            "22263/22300 (epoch 49), train_loss = 0.714, time/batch = 0.093\n",
            "22264/22300 (epoch 49), train_loss = 0.720, time/batch = 0.091\n",
            "22265/22300 (epoch 49), train_loss = 0.672, time/batch = 0.094\n",
            "22266/22300 (epoch 49), train_loss = 0.702, time/batch = 0.091\n",
            "22267/22300 (epoch 49), train_loss = 0.667, time/batch = 0.092\n",
            "22268/22300 (epoch 49), train_loss = 0.701, time/batch = 0.092\n",
            "22269/22300 (epoch 49), train_loss = 0.711, time/batch = 0.092\n",
            "22270/22300 (epoch 49), train_loss = 0.712, time/batch = 0.093\n",
            "22271/22300 (epoch 49), train_loss = 0.707, time/batch = 0.092\n",
            "22272/22300 (epoch 49), train_loss = 0.705, time/batch = 0.092\n",
            "22273/22300 (epoch 49), train_loss = 0.697, time/batch = 0.092\n",
            "22274/22300 (epoch 49), train_loss = 0.701, time/batch = 0.093\n",
            "22275/22300 (epoch 49), train_loss = 0.688, time/batch = 0.093\n",
            "22276/22300 (epoch 49), train_loss = 0.694, time/batch = 0.092\n",
            "22277/22300 (epoch 49), train_loss = 0.686, time/batch = 0.093\n",
            "22278/22300 (epoch 49), train_loss = 0.701, time/batch = 0.091\n",
            "22279/22300 (epoch 49), train_loss = 0.687, time/batch = 0.093\n",
            "22280/22300 (epoch 49), train_loss = 0.722, time/batch = 0.092\n",
            "22281/22300 (epoch 49), train_loss = 0.666, time/batch = 0.093\n",
            "22282/22300 (epoch 49), train_loss = 0.691, time/batch = 0.098\n",
            "22283/22300 (epoch 49), train_loss = 0.670, time/batch = 0.091\n",
            "22284/22300 (epoch 49), train_loss = 0.660, time/batch = 0.092\n",
            "22285/22300 (epoch 49), train_loss = 0.716, time/batch = 0.092\n",
            "22286/22300 (epoch 49), train_loss = 0.685, time/batch = 0.092\n",
            "22287/22300 (epoch 49), train_loss = 0.719, time/batch = 0.093\n",
            "22288/22300 (epoch 49), train_loss = 0.684, time/batch = 0.091\n",
            "22289/22300 (epoch 49), train_loss = 0.684, time/batch = 0.092\n",
            "22290/22300 (epoch 49), train_loss = 0.665, time/batch = 0.097\n",
            "22291/22300 (epoch 49), train_loss = 0.689, time/batch = 0.093\n",
            "22292/22300 (epoch 49), train_loss = 0.764, time/batch = 0.094\n",
            "22293/22300 (epoch 49), train_loss = 0.703, time/batch = 0.091\n",
            "22294/22300 (epoch 49), train_loss = 0.702, time/batch = 0.093\n",
            "22295/22300 (epoch 49), train_loss = 0.674, time/batch = 0.092\n",
            "22296/22300 (epoch 49), train_loss = 0.695, time/batch = 0.095\n",
            "22297/22300 (epoch 49), train_loss = 0.700, time/batch = 0.093\n",
            "22298/22300 (epoch 49), train_loss = 0.683, time/batch = 0.092\n",
            "22299/22300 (epoch 49), train_loss = 0.688, time/batch = 0.103\n",
            "model saved to save/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E_9M2iLDn4T"
      },
      "source": [
        "After we've trained, we can sample from a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cd73wYTDreD",
        "outputId": "d697c93c-ff67-4438-e284-2c456ab52ae1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python sample.py -n 1000"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:30: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:36: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:39: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:46: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:47: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:86: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:92: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:98: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/char-rnn-tensorflow/model.py:100: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From sample.py:39: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-10-29 09:48:00.765219: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-10-29 09:48:00.800181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-29 09:48:00.800767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-10-29 09:48:00.801040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-10-29 09:48:00.802379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-10-29 09:48:00.803617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-10-29 09:48:00.803944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-10-29 09:48:00.805684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-10-29 09:48:00.807164: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-10-29 09:48:00.810850: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-10-29 09:48:00.810967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-29 09:48:00.811616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-29 09:48:00.812167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-10-29 09:48:00.812546: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-10-29 09:48:00.817391: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-10-29 09:48:00.817591: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x259d100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-10-29 09:48:00.817618: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-10-29 09:48:00.925585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-29 09:48:00.926335: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x259d2c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-10-29 09:48:00.926366: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-10-29 09:48:00.926571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-29 09:48:00.927139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-10-29 09:48:00.927223: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-10-29 09:48:00.927254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-10-29 09:48:00.927276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-10-29 09:48:00.927304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-10-29 09:48:00.927323: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-10-29 09:48:00.927346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-10-29 09:48:00.927370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-10-29 09:48:00.927443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-29 09:48:00.928039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-29 09:48:00.928613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-10-29 09:48:00.928688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-10-29 09:48:00.929910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-10-29 09:48:00.929957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-10-29 09:48:00.929968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-10-29 09:48:00.930098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-29 09:48:00.930724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-29 09:48:00.931288: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-10-29 09:48:00.931330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From sample.py:40: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From sample.py:41: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From sample.py:41: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "2020-10-29 09:48:02.013695: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            " love my gooing prince\n",
            "Would not like mine own popular too.\n",
            "\n",
            "Nurse:\n",
            "If I crave with peace was she that got why, forswear in\n",
            "purpose. What, is therefore? worthy liege,\n",
            "I'll tell theer.\n",
            "\n",
            "CLARENCE:\n",
            "Upon the king's ship; I know thou hast a woman's death.\n",
            "\n",
            "CAPULET:\n",
            "Welcome, my brother, when are they! no more kind\n",
            "of swift--Clarence castle, why, is Lord Angelo\n",
            "Addinglo by him.\n",
            "\n",
            "MENENIUS:\n",
            "I had rather conventably fetch me of the duke.\n",
            "I'll have a kind of language.\n",
            "\n",
            "Second Citizen:\n",
            "I am belied.\n",
            "\n",
            "First Lord:\n",
            "What willing weeds we? the infant bless him,\n",
            "And several foes the royal blood etward according blood\n",
            "And twenty orphan, the cobpiece of ours.\n",
            "\n",
            "GLOUCESTER:\n",
            "You leave to-morrow.\n",
            "\n",
            "PETER:\n",
            "Yea,' that encounter their masters in him.\n",
            "\n",
            "GLOUCESTER:\n",
            "Ay, my good lord. Come, Petruchio is so,\n",
            "for I was rise and courtesy'ly; not I: why, Buchiofh,\n",
            "Hath something spoke, or both men's state\n",
            "And let me encawp'd the provost: Abideness\n",
            "With gentle bright, slave, for some replied\n",
            "We have been in my bosom:\n",
            "Away with death and foot those and fortune\n",
            "That honourable-brothestand was forget it\n",
            "Confound, traitor, God your minds, and going to him;\n",
            "For how he, that have earth and play'd by hanging; but\n",
            "Happily forbid! no doubt, women gape,\n",
            "Of whom, in my thought I play'd to live?\n",
            "Art thou Romeo?\n",
            "\n",
            "TRANIO:\n",
            "Ay, I have:\n",
            "Wife, Grumio, my lord! why, 'tis done,\n",
            "You should pass, thyself bald, and thy father say\n",
            "Good severity of death:'\n",
            "And all my heart.\n",
            "\n",
            "First Senator:\n",
            "Didst guiltled, she came I full of it!\n",
            "\n",
            "CLIFFORD:\n",
            "No, sir, a monster, but o' the sky.\n",
            "\n",
            "ANGELO:\n",
            "What, what is't? is it not what's past, he's yielo!\n",
            "\n",
            "First Citizen:\n",
            "If they praise thy need I beseech you,\n",
            "'I way, to this done,\n",
            "Look, then, thus vice, discover:\n",
            "I'll held unto the windly king.\n",
            "\n",
            "First Gentleman:\n",
            "Who comest thou? that of a parle,\n",
            "I say 'ywarchies and in rich, if we will\n",
            "Have accidentrided to your beauty;\n",
            "Lest thou attend a babe, some two of your\n",
            "ended by thy loss. If either cheer, and trades,\n",
            "Nor how upon his face, which they put me\n",
            "Your fine father likes, to entertain,\n",
            "If any pate, in hand was describle:\n",
            "You in your service.\n",
            "\n",
            "BENVOLIO:\n",
            "Come, comes here!\n",
            "I think there's nonelehell'd with them in the law;\n",
            "For still ballad the downfall of our order!\n",
            "Ay, why should you do bite the forward heir,\n",
            "Both granted long and will, though the way\n",
            "From that scarligaloobed souls man, whiles he stoop,\n",
            "Out of his friends; know your voices since\n",
            "Again how to only upon my sharp-dread,\n",
            "And every thoughts of imprisonment--\n",
            "\n",
            "CLIFFORD:\n",
            "And here before I am a queen,\n",
            "Unless it grace mine ebg.\n",
            "\n",
            "GLOUCENSERO:\n",
            "Let me be shortail for one thing well after?\n",
            "Capul, like a fault on Lady Free, achise thee!\n",
            "\n",
            "First Citizen:\n",
            "In boldly, offer the gods that thou art solicity!\n",
            "O, might find all I dream on thee, belike; revel,\n",
            "Your harance: I must answer a mile and get like\n",
            "Ratcliff, let alsobrement I think, being\n",
            "thee, garland, that hath fathers for't, O, give you, that\n",
            "yourself a thousand Warwick hath been chief still breath\n",
            "To beg O, Tranio, in the day of a\n",
            "goodly parent, savultic, in brief,\n",
            "And treading ere we depended, and jove,\n",
            "I'll rest and rich in our conceit, and tell thee.\n",
            "\n",
            "LUCIO:\n",
            "Why, hardly so look, he spakes his royal prince,\n",
            "To part with hell soon of angry: what vows\n",
            "My use as great body's ill, beheld--\n",
            "\n",
            "CAMILLO:\n",
            "Thou liest, festial fool!\n",
            "O body! think within this love? I'll arram the blood.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Good morrow, bear.\n",
            "\n",
            "MERCUTIO:\n",
            "And then, as I bold!\n",
            "\n",
            "ARIEL:\n",
            "Not I.' 'em:\n",
            "'Tell had work, hell.\n",
            "\n",
            "CORIOLANUS:\n",
            "Alas, you\n",
            "advantage to tie someby the heart of right,\n",
            "And told him, whilst I do convey mine honour,\n",
            "That she's the world to say: I say, there is not?\n",
            "\n",
            "Servant:\n",
            "I would wish him in that; more is that behalf,\n",
            "And scatter'd, for every thing! stoles, life,\n",
            "Because his soldier, dear honour'd which\n",
            "Their course is wondrous business: To carry him.\n",
            "\n",
            "First Senator:\n",
            "All personable! if thou love glorizo's brother's cause.\n",
            "Now art thou my old words at once, infold or that\n",
            "I stay'd with making cause hit the garish hate.\n",
            "\n",
            "VINCENTIO:\n",
            "I fear the prison, of the root of Juliet,\n",
            "And wilt writ of his restries is at thine ears\n",
            "Of damned by this be catched, as the rest?\n",
            "\n",
            "Second Gentleman:\n",
            "Who's a paralley, no more! O, long-heart!\n",
            "\n",
            "DUKE OF YORK:\n",
            "O, belike, and know how on royal swift,\n",
            "The honour of her coronation!\n",
            "\n",
            "TYBALT:\n",
            "Romeo sit too lady: if thou wert not\n",
            "That patience to cheerly come, as if\n",
            "You may have son are inclined to him, and the\n",
            "people of thy hot argotrimpmithme\n",
            "More of your blood and gelded for libity;\n",
            "She must be, by my sweet Juliet, that name\n",
            "Which with divines doth hunt angry damn'd through.\n",
            "In God, proved think you before plot.\n",
            "\n",
            "Second Gentleman:\n",
            "'Tis tell thee, man, wounded the same to glad to trust;\n",
            "For Lancasterhake 'tis Captat, at a monster\n",
            "That think'st thou, Sobsch a holy good\n",
            "Can the wished ears hither than set down the phhastic. Come, I\n",
            "not to be satisfied. Let them past chopp' topn.\n",
            "The love I have.\n",
            "\n",
            "GRUMIO:\n",
            "Thou that's battle:\n",
            "Lie thee, and so than I shall\n",
            "to hear a treasure, if thou canst give you fancy\n",
            "that fellows to Pisa him: my lord, is axgradient?\n",
            "Why should he there?\n",
            "\n",
            "CAMILLO:\n",
            "Yet look is none.\n",
            "\n",
            "PETER:\n",
            "These lords at Lady Rivers, good father, bold!\n",
            "I can they are allowarity; in Angelo,\n",
            "An oath in its is deadly sworghing of God,\n",
            "Condemns to our mistress.\n",
            "\n",
            "LUCIO:\n",
            "Dallect, belike, if you'ld love, I purpose that jot:\n",
            "Away with him: he has in it for it. Arm Mars,\n",
            "After thieves welcome, or four-holds and sold,\n",
            "And Bolingbroke is flattering, Bianca, bite:\n",
            "My father judchal, that with a nature\n",
            "And snow thy blind shows before thy tongue and\n",
            "By amiss of dreadful sire, at the Lady Geo!\n",
            "\n",
            "MORTIUS:\n",
            "And now, good, forward and knowind,\n",
            "And bid thy help to serve it.\n",
            "\n",
            "SICINIUS:\n",
            "Let's talk upon him, to redite\n",
            "His own absence for the yellengs: I'll tell you hare.\n",
            "\n",
            "PETRUCHIO:\n",
            "Peace, justice,\n",
            "Than a while be not, my lord.\n",
            "\n",
            "KING RICHARD III:\n",
            "What, sir, you must believe them down.\n",
            "\n",
            "PAULINA:\n",
            "O cheerful street?\n",
            "\n",
            "JULIET:\n",
            "Ay, no farther, and that I had welcome to:\n",
            "When I am more than oft to \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8yz6SjJgFV7"
      },
      "source": [
        "## **New Data (Sherlock Holmes)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGt-QrmjCeNT"
      },
      "source": [
        "Next, let's try some different data. For example, we will use the entire Sherlock Holmes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8i2CXVECj41"
      },
      "source": [
        "%cd data\n",
        "!mkdir sherlock\n",
        "%cd sherlock\n",
        "!wget https://sherlock-holm.es/stories/plain-text/cnus.txt\n",
        "!mv cnus.txt input.txt\n",
        "%cd /content/char-rnn-tensorflow/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDWKsFAcCta7"
      },
      "source": [
        "Now, lets train on our new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNcizTtxCodu"
      },
      "source": [
        "!python train.py --data_dir data/sherlock --rnn_size 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PdXUqicfPIq"
      },
      "source": [
        "If it fails due to the data being too small, run this code block instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUq5vW5vfO0o"
      },
      "source": [
        "!python train.py --data_dir data/sherlock --rnn_size 200 --seq_length 30 --num_epochs 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_XIUL6hXGBo"
      },
      "source": [
        "Now let's sample from the new data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnEv-W8TXFcc",
        "outputId": "8029e8ec-da8f-42e9-e2b2-aead4328f825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!python sample.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:474: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:475: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
            " \n",
            "Wome just anon war?\n",
            "(I llave bicus\n",
            "Tfousm ware sne go tithby\n",
            "Wtike\n",
            "Tht wan't to wa kail\n",
            "Fan't kee vome ahe it uldod to gaure have I salgajw, thineve beigorgore, I don't nsly ngan, net taro unkt being to ke touCowl\n",
            "Bay lap) I bnech, sher anna wase thikinx hewr oh me bare\n",
            "\n",
            "I lave coke as's you'y\n",
            "\n",
            "Gyour to pcosed ar\n",
            "\n",
            "Cyou my vat lell\n",
            "I máset imy ahlt ad\n",
            "Ébebse, I loull\n",
            "Fand, I my, cu'ml say te nat, ster to loico\n",
            "Tup?\n",
            "Whis reass area love Io\n",
            "He\n",
            "ne can, I juend sicm racing if doy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgHzt7JvXOoD"
      },
      "source": [
        "To visualise the training progress, we can use Tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BNnMwLXXSmx"
      },
      "source": [
        "!tensorboard --logdir=./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haapbjaQgMwW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDJs53jWgO_M"
      },
      "source": [
        "## **Upload your own Data (WIP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tJPsn7Xgxo6"
      },
      "source": [
        "Now you can try training the AI on your own data!\n",
        "Run the cell below to upload your files to the notebook.\n",
        "\n",
        "*Currently a work in progress, this script will not work*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x9Oy2k2gWzu",
        "outputId": "b208cf0b-c215-443f-b01a-72133fdb5f76",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "%cd /content/\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "filel = list(uploaded.keys())[0]\n",
        "filell = filel.replace(\"'\", \"\")\n",
        "print(filell)\n",
        "!mv filell /char-rnn-tensorflow/data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-51e5c04c-4823-47ec-b4c6-5db5ba731ac0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-51e5c04c-4823-47ec-b4c6-5db5ba731ac0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dayglow.gif to dayglow.gif\n",
            "dayglow.gif\n",
            "/bin/bash: -c: line 0: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 0: `mv str(filell) /char-rnn-tensorflow/data/'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}